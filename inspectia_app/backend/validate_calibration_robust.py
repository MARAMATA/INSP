#!/usr/bin/env python3
"""
Script de validation approfondie de la calibration des mod√®les
V√©rifie les points critiques : split correct, sharpness, BSS, avant/apr√®s calibration
"""

import sys
import os
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.metrics import brier_score_loss, roc_auc_score
from sklearn.model_selection import train_test_split
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model_and_splits(chapter):
    """Charger le mod√®le et TOUS les splits (train, valid, test)"""
    logger.info(f"üîç Chargement complet du chapitre {chapter}...")
    
    # Chemins
    models_dir = f"/Users/macbook/Desktop/inspectia_app/backend/models/chap{chapter}"
    splits_dir = f"/Users/macbook/Desktop/inspectia_app/backend/data/ml_splits/chap{chapter}"
    
    # Charger tous les splits
    X_train = pd.read_csv(f"{splits_dir}/X_train.csv")
    X_valid = pd.read_csv(f"{splits_dir}/X_valid.csv")
    X_test = pd.read_csv(f"{splits_dir}/X_test.csv")
    
    y_train = pd.read_csv(f"{splits_dir}/y_train.csv").values.ravel()
    y_valid = pd.read_csv(f"{splits_dir}/y_valid.csv").values.ravel()
    y_test = pd.read_csv(f"{splits_dir}/y_test.csv").values.ravel()
    
    # Charger les features
    features_path = f"{models_dir}/features.pkl"
    if os.path.exists(features_path):
        features = joblib.load(features_path)
        X_train = X_train[features]
        X_valid = X_valid[features]
        X_test = X_test[features]
    
    # Identifier le meilleur mod√®le
    if chapter == 30:
        best_model_name = "XGBoost"
    elif chapter == 84:
        best_model_name = "CatBoost"
    elif chapter == 85:
        best_model_name = "XGBoost"
    else:
        raise ValueError(f"Chapitre {chapter} non support√©")
    
    # Charger le mod√®le calibr√©
    model_path = f"{models_dir}/{best_model_name.lower()}_model.pkl"
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Mod√®le non trouv√©: {model_path}")
    
    model = joblib.load(model_path)
    
    logger.info(f"   ‚úÖ Mod√®le {best_model_name} charg√©")
    logger.info(f"   ‚úÖ Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}")
    logger.info(f"   ‚úÖ Taux de fraude - Train: {y_train.mean():.3f}, Valid: {y_valid.mean():.3f}, Test: {y_test.mean():.3f}")
    
    return model, X_train, X_valid, X_test, y_train, y_valid, y_test, best_model_name

def extract_base_model(calibrated_model):
    """Extraire le mod√®le de base d'un CalibratedClassifierCV"""
    if hasattr(calibrated_model, 'estimator'):
        return calibrated_model.estimator
    else:
        return calibrated_model

def calculate_brier_skill_score(y_true, y_prob, base_rate):
    """Calculer le Brier Skill Score vs un classifieur de base"""
    brier_score = brier_score_loss(y_true, y_prob)
    brier_score_baseline = brier_score_loss(y_true, [base_rate] * len(y_true))
    bss = 1 - (brier_score / brier_score_baseline)
    return bss

def analyze_calibration_robust(model, X_train, X_valid, X_test, y_train, y_valid, y_test, model_name, chapter):
    """Analyse robuste de la calibration"""
    logger.info(f"üéØ ANALYSE ROBUSTE - {model_name} (Chapitre {chapter})")
    logger.info("=" * 60)
    
    # 1. V√©rifier si c'est un mod√®le calibr√©
    is_calibrated = isinstance(model, CalibratedClassifierCV)
    logger.info(f"üìä Mod√®le calibr√©: {is_calibrated}")
    
    if is_calibrated:
        # Extraire le mod√®le de base
        base_model = extract_base_model(model)
        logger.info(f"üìä Mod√®le de base: {type(base_model).__name__}")
        
        # Pr√©dictions du mod√®le de base (non calibr√©)
        y_prob_base_train = base_model.predict_proba(X_train)[:, 1]
        y_prob_base_valid = base_model.predict_proba(X_valid)[:, 1]
        y_prob_base_test = base_model.predict_proba(X_test)[:, 1]
        
        # Pr√©dictions du mod√®le calibr√©
        y_prob_cal_train = model.predict_proba(X_train)[:, 1]
        y_prob_cal_valid = model.predict_proba(X_valid)[:, 1]
        y_prob_cal_test = model.predict_proba(X_test)[:, 1]
        
        # Comparaison avant/apr√®s calibration
        logger.info("üìä COMPARAISON AVANT/APR√àS CALIBRATION:")
        
        # Sur validation (o√π la calibration a √©t√© ajust√©e)
        brier_base_valid = brier_score_loss(y_valid, y_prob_base_valid)
        brier_cal_valid = brier_score_loss(y_valid, y_prob_cal_valid)
        auc_base_valid = roc_auc_score(y_valid, y_prob_base_valid)
        auc_cal_valid = roc_auc_score(y_valid, y_prob_cal_valid)
        
        logger.info(f"   Validation (calibration ajust√©e ici):")
        logger.info(f"     Brier Score - Base: {brier_base_valid:.4f}, Calibr√©: {brier_cal_valid:.4f}")
        logger.info(f"     AUC - Base: {auc_base_valid:.4f}, Calibr√©: {auc_cal_valid:.4f}")
        
        # Sur test (jamais vu)
        brier_base_test = brier_score_loss(y_test, y_prob_base_test)
        brier_cal_test = brier_score_loss(y_test, y_prob_cal_test)
        auc_base_test = roc_auc_score(y_test, y_prob_base_test)
        auc_cal_test = roc_auc_score(y_test, y_prob_cal_test)
        
        logger.info(f"   Test (jamais vu):")
        logger.info(f"     Brier Score - Base: {brier_base_test:.4f}, Calibr√©: {brier_cal_test:.4f}")
        logger.info(f"     AUC - Base: {auc_base_test:.4f}, Calibr√©: {auc_cal_test:.4f}")
        
        # V√©rifier que la calibration am√©liore sur validation mais pas forc√©ment sur test
        brier_improvement_valid = brier_base_valid - brier_cal_valid
        brier_improvement_test = brier_base_test - brier_cal_test
        
        logger.info(f"   Am√©lioration Brier - Valid: {brier_improvement_valid:+.4f}, Test: {brier_improvement_test:+.4f}")
        
        if brier_improvement_valid > 0:
            logger.info("   ‚úÖ Calibration am√©liore sur validation (normal)")
        else:
            logger.warning("   ‚ö†Ô∏è Calibration n'am√©liore pas sur validation (suspect)")
        
        # Utiliser les probabilit√©s calibr√©es pour la suite
        y_prob_final = y_prob_cal_test
    else:
        # Mod√®le non calibr√©
        y_prob_final = model.predict_proba(X_test)[:, 1]
        logger.info("   ‚ö†Ô∏è Mod√®le non calibr√© d√©tect√©")
    
    # 2. Analyse de calibration avec plus de bins
    logger.info("üìä ANALYSE DE CALIBRATION D√âTAILL√âE:")
    
    # Calibration curve avec plus de bins
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_test, y_prob_final, n_bins=15, strategy='quantile'
    )
    
    # Calculer l'ECE avec plus de bins
    bin_boundaries = np.linspace(0, 1, 16)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0
    bin_counts = []
    bin_accuracies = []
    bin_confidences = []
    
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (y_prob_final > bin_lower) & (y_prob_final <= bin_upper)
        prop_in_bin = in_bin.mean()
        bin_counts.append(prop_in_bin)
        
        if prop_in_bin > 0:
            accuracy_in_bin = y_test[in_bin].mean()
            avg_confidence_in_bin = y_prob_final[in_bin].mean()
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
            
            bin_accuracies.append(accuracy_in_bin)
            bin_confidences.append(avg_confidence_in_bin)
        else:
            bin_accuracies.append(0)
            bin_confidences.append(0)
    
    brier_score = brier_score_loss(y_test, y_prob_final)
    
    logger.info(f"   Brier Score (15 bins): {brier_score:.4f}")
    logger.info(f"   ECE (15 bins): {ece:.4f}")
    
    # 3. Analyse de sharpness
    logger.info("üìä ANALYSE DE SHARPNESS:")
    
    # Histogramme des probabilit√©s
    prob_hist, prob_bins = np.histogram(y_prob_final, bins=20, range=(0, 1))
    prob_centers = (prob_bins[:-1] + prob_bins[1:]) / 2
    
    # Calculer la sharpness (√©cart-type des probabilit√©s)
    sharpness = np.std(y_prob_final)
    mean_prob = np.mean(y_prob_final)
    
    logger.info(f"   Sharpness (std): {sharpness:.4f}")
    logger.info(f"   Probabilit√© moyenne: {mean_prob:.4f}")
    logger.info(f"   Probabilit√©s > 0.8: {(y_prob_final > 0.8).mean():.3f}")
    logger.info(f"   Probabilit√©s < 0.2: {(y_prob_final < 0.2).mean():.3f}")
    
    # 4. Brier Skill Score
    base_rate = y_test.mean()
    bss = calculate_brier_skill_score(y_test, y_prob_final, base_rate)
    
    logger.info(f"üìä BRIER SKILL SCORE:")
    logger.info(f"   Taux de base: {base_rate:.3f}")
    logger.info(f"   BSS: {bss:.4f}")
    
    if bss > 0.5:
        logger.info("   ‚úÖ Excellent BSS (>0.5)")
    elif bss > 0.2:
        logger.info("   ‚úÖ Bon BSS (>0.2)")
    elif bss > 0:
        logger.info("   ‚ö†Ô∏è BSS positif mais faible")
    else:
        logger.warning("   ‚ùå BSS n√©gatif (pire que le taux de base)")
    
    # 5. Distribution des bins
    logger.info("üìä DISTRIBUTION DES BINS:")
    for i, (count, acc, conf) in enumerate(zip(bin_counts, bin_accuracies, bin_confidences)):
        if count > 0.01:  # Seulement les bins avec >1% des √©chantillons
            logger.info(f"   Bin {i+1}: {count:.3f} √©chantillons, acc={acc:.3f}, conf={conf:.3f}")
    
    return {
        'brier_score': brier_score,
        'ece': ece,
        'sharpness': sharpness,
        'bss': bss,
        'base_rate': base_rate,
        'calibration_curve': (fraction_of_positives, mean_predicted_value),
        'prob_hist': (prob_hist, prob_centers),
        'is_calibrated': is_calibrated,
        'brier_improvement_test': brier_improvement_test if is_calibrated else 0
    }

def plot_comprehensive_analysis(results):
    """Cr√©er des graphiques complets d'analyse"""
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    fig.suptitle('Analyse Compl√®te de la Calibration des Mod√®les', fontsize=16, fontweight='bold')
    
    chapters = [30, 84, 85]
    colors = ['blue', 'red', 'green']
    
    for i, (chapter, data) in enumerate(results.items()):
        if data is None:
            continue
        
        color = colors[i]
        
        # 1. Courbe de calibration
        ax = axes[0, i]
        fraction_of_positives, mean_predicted_value = data['calibration_curve']
        ax.plot(mean_predicted_value, fraction_of_positives, "s-", 
                color=color, label=f"Chapitre {chapter}", linewidth=2, markersize=6)
        ax.plot([0, 1], [0, 1], "k:", label="Parfait", linewidth=2)
        ax.set_xlabel('Probabilit√© moyenne pr√©dite')
        ax.set_ylabel('Fraction de positifs')
        ax.set_title(f'Chapitre {chapter}\nBrier: {data["brier_score"]:.4f}, ECE: {data["ece"]:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        
        # 2. Histogramme des probabilit√©s (sharpness)
        ax = axes[1, i]
        prob_hist, prob_centers = data['prob_hist']
        ax.bar(prob_centers, prob_hist, width=0.05, alpha=0.7, color=color)
        ax.axvline(data['base_rate'], color='red', linestyle='--', 
                  label=f'Taux de base: {data["base_rate"]:.3f}')
        ax.set_xlabel('Probabilit√© pr√©dite')
        ax.set_ylabel('Nombre d\'√©chantillons')
        ax.set_title(f'Sharpness: {data["sharpness"]:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. M√©triques comparatives
        ax = axes[2, i]
        metrics = ['Brier', 'ECE', 'Sharpness', 'BSS']
        values = [data['brier_score'], data['ece'], data['sharpness'], data['bss']]
        bars = ax.bar(metrics, values, color=color, alpha=0.7)
        ax.set_ylabel('Valeur')
        ax.set_title(f'Chapitre {chapter} - M√©triques')
        ax.grid(True, alpha=0.3)
        
        # Ajouter les valeurs sur les barres
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                   f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('/Users/macbook/Desktop/inspectia_app/backend/calibration_robust_analysis.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    logger.info("üìä Graphique d'analyse robuste sauvegard√©: calibration_robust_analysis.png")

def main():
    """Fonction principale"""
    logger.info("üöÄ VALIDATION ROBUSTE DE LA CALIBRATION")
    logger.info("=" * 70)
    
    results = {}
    
    # Analyser chaque chapitre
    for chapter in [30, 84, 85]:
        try:
            # Charger le mod√®le et tous les splits
            model, X_train, X_valid, X_test, y_train, y_valid, y_test, model_name = load_model_and_splits(chapter)
            
            # Analyse robuste
            analysis = analyze_calibration_robust(
                model, X_train, X_valid, X_test, y_train, y_valid, y_test, model_name, chapter
            )
            
            results[chapter] = analysis
            
            logger.info("")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chapitre {chapter}: {e}")
            results[chapter] = None
    
    # Cr√©er les graphiques
    if any(data is not None for data in results.values()):
        plot_comprehensive_analysis(results)
    
    # R√©sum√© final
    logger.info("üìã R√âSUM√â FINAL DE LA VALIDATION")
    logger.info("=" * 70)
    
    for chapter, data in results.items():
        if data is not None:
            logger.info(f"Chapitre {chapter}:")
            logger.info(f"   Brier Score: {data['brier_score']:.4f}")
            logger.info(f"   ECE: {data['ece']:.4f}")
            logger.info(f"   Sharpness: {data['sharpness']:.4f}")
            logger.info(f"   BSS: {data['bss']:.4f}")
            logger.info(f"   Mod√®le calibr√©: {data['is_calibrated']}")
            
            # √âvaluation globale
            if (data['brier_score'] < 0.1 and data['ece'] < 0.05 and 
                data['bss'] > 0.2 and data['sharpness'] > 0.1):
                logger.info(f"   ‚úÖ CALIBRATION EXCELLENTE ET ROBUSTE")
            elif (data['brier_score'] < 0.2 and data['ece'] < 0.1 and 
                  data['bss'] > 0.1):
                logger.info(f"   ‚úÖ CALIBRATION BONNE")
            else:
                logger.info(f"   ‚ö†Ô∏è CALIBRATION √Ä AM√âLIORER")
        else:
            logger.info(f"Chapitre {chapter}: ‚ùå ERREUR")
        
        logger.info("")

if __name__ == "__main__":
    main()
