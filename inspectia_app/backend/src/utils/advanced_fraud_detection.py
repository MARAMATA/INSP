#!/usr/bin/env python3
"""
Module de dÃ©tection de fraude avancÃ©e
ImplÃ©mente les techniques de la cellule de ciblage et de veille commerciale :
- MÃ©thodes probabilistes (thÃ©orÃ¨me de BienaymÃ©-Tchebychev)
- Analyse miroir avec TEI (Taux Effectifs d'Imposition)
- DÃ©tection d'anomalies (clustering spectral et hiÃ©rarchique)
- ContrÃ´le des valeurs administrÃ©es
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from scipy import stats
from sklearn.cluster import SpectralClustering, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedFraudDetection:
    """Classe principale pour la dÃ©tection de fraude avancÃ©e"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.product_origin_stats = {}
        self.admin_values = {}
        self.tei_thresholds = {}
        
    def clean_data_comprehensive(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Toilettage complet des donnÃ©es
        - Suppression des doublons
        - Gestion des valeurs NaN, nulles et infinies
        - Normalisation des types de donnÃ©es
        """
        logger.info("ðŸ§¹ Toilettage complet des donnÃ©es...")
        
        original_shape = df.shape
        
        # 1. Suppression des doublons
        df = df.drop_duplicates()
        logger.info(f"   Doublons supprimÃ©s: {original_shape[0] - df.shape[0]}")
        
        # 2. Gestion des valeurs infinies
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].dtype in ['float64', 'float32']:
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # 3. Gestion des valeurs nulles
        # Pour les colonnes numÃ©riques : mÃ©diane
        for col in numeric_cols:
            if df[col].isnull().any():
                median_val = df[col].median()
                df[col] = df[col].fillna(median_val)
                logger.info(f"   {col}: {df[col].isnull().sum()} NaN â†’ mÃ©diane {median_val}")
        
        # Pour les colonnes catÃ©gorielles : mode ou 'UNKNOWN'
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].isnull().any():
                mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'UNKNOWN'
                df[col] = df[col].fillna(mode_val)
                logger.info(f"   {col}: {df[col].isnull().sum()} NaN â†’ mode '{mode_val}'")
        
        # 4. Normalisation des types
        if 'CODE_PRODUIT' in df.columns:
            df['CODE_PRODUIT'] = df['CODE_PRODUIT'].astype(str)
        if 'PAYS_ORIGINE' in df.columns:
            df['PAYS_ORIGINE'] = df['PAYS_ORIGINE'].astype(str).str.upper()
        if 'PAYS_PROVENANCE' in df.columns:
            df['PAYS_PROVENANCE'] = df['PAYS_PROVENANCE'].astype(str).str.upper()
        
        logger.info(f"âœ… Toilettage terminÃ©: {original_shape} â†’ {df.shape}")
        return df
    
    def bienayme_chebychev_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Application du thÃ©orÃ¨me de BienaymÃ©-Tchebychev pour encadrer les valeurs attendues
        par couple produit/origine et dÃ©tecter les extrÃªmes suspects
        """
        logger.info("ðŸ“Š Application du thÃ©orÃ¨me de BienaymÃ©-Tchebychev...")
        
        # CrÃ©er la clÃ© produit/origine
        # Adapter aux noms de colonnes aprÃ¨s agrÃ©gation
        if 'CODE_PRODUIT_STR' in df.columns:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT_STR'] + '_' + df['PAYS_ORIGINE_STR']
        else:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT'].astype(str) + '_' + df['PAYS_ORIGINE'].astype(str)
        
        # Calculer les statistiques par couple produit/origine
        stats_by_key = df.groupby('PRODUCT_ORIGIN_KEY')['VALEUR_CAF'].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).reset_index()
        
        # Filtrer les couples avec au moins 5 observations
        stats_by_key = stats_by_key[stats_by_key['count'] >= 5]
        
        # Appliquer le thÃ©orÃ¨me de BienaymÃ©-Tchebychev
        # P(|X - Î¼| â‰¥ kÏƒ) â‰¤ 1/kÂ²
        # Pour k=2: P(|X - Î¼| â‰¥ 2Ïƒ) â‰¤ 0.25 (25% des observations peuvent Ãªtre en dehors)
        # Pour k=3: P(|X - Î¼| â‰¥ 3Ïƒ) â‰¤ 0.111 (11.1% des observations peuvent Ãªtre en dehors)
        
        df['BIENAYME_CHEBYCHEV_ANOMALY'] = 0
        df['BIENAYME_CHEBYCHEV_SCORE'] = 0.0
        
        for _, row in stats_by_key.iterrows():
            key = row['PRODUCT_ORIGIN_KEY']
            mean_val = row['mean']
            std_val = row['std']
            
            if std_val > 0:  # Ã‰viter la division par zÃ©ro
                # Masque pour ce couple produit/origine
                mask = df['PRODUCT_ORIGIN_KEY'] == key
                
                # Calculer l'Ã©cart normalisÃ© |X - Î¼|/Ïƒ
                normalized_deviation = np.abs(df.loc[mask, 'VALEUR_CAF'] - mean_val) / std_val
                
                # Marquer comme anomalie si |X - Î¼| â‰¥ 3Ïƒ (seuil strict)
                anomaly_mask = normalized_deviation >= 3.0
                df.loc[mask & anomaly_mask, 'BIENAYME_CHEBYCHEV_ANOMALY'] = 1
                
                # Score de dÃ©viation (plus le score est Ã©levÃ©, plus c'est suspect)
                df.loc[mask, 'BIENAYME_CHEBYCHEV_SCORE'] = normalized_deviation
        
        # Sauvegarder les statistiques pour rÃ©utilisation
        self.product_origin_stats = stats_by_key.set_index('PRODUCT_ORIGIN_KEY').to_dict('index')
        
        anomalies_count = df['BIENAYME_CHEBYCHEV_ANOMALY'].sum()
        logger.info(f"   Anomalies dÃ©tectÃ©es (BienaymÃ©-Tchebychev): {anomalies_count}")
        
        return df
    
    def mirror_analysis_tei(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Analyse miroir avec interprÃ©tation des Ã©carts via les Taux Effectifs d'Imposition (TEI)
        Compare les valeurs dÃ©clarÃ©es avec les valeurs de rÃ©fÃ©rence par produit/origine
        """
        logger.info("ðŸªž Analyse miroir avec TEI (Taux Effectifs d'Imposition)...")
        
        # Calculer le TEI moyen par couple produit/origine
        # Adapter aux noms de colonnes aprÃ¨s agrÃ©gation
        if 'CODE_PRODUIT_STR' in df.columns:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT_STR'] + '_' + df['PAYS_ORIGINE_STR']
        else:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT'].astype(str) + '_' + df['PAYS_ORIGINE'].astype(str)
        
        # TEI = (MONTANT_LIQUIDATION / VALEUR_CAF) * 100
        df['TEI_CALCULE'] = (df['MONTANT_LIQUIDATION'] / df['VALEUR_CAF'].replace(0, 1)) * 100
        
        # Calculer les statistiques TEI par couple produit/origine
        tei_stats = df.groupby('PRODUCT_ORIGIN_KEY')['TEI_CALCULE'].agg([
            'count', 'mean', 'std', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
        ]).reset_index()
        tei_stats.columns = ['PRODUCT_ORIGIN_KEY', 'count', 'mean', 'std', 'median', 'q25', 'q75']
        
        # Filtrer les couples avec au moins 10 observations
        tei_stats = tei_stats[tei_stats['count'] >= 10]
        
        # CrÃ©er les features d'analyse miroir
        df['MIRROR_TEI_ANOMALY'] = 0
        df['MIRROR_TEI_SCORE'] = 0.0
        df['MIRROR_TEI_DEVIATION'] = 0.0
        
        for _, row in tei_stats.iterrows():
            key = row['PRODUCT_ORIGIN_KEY']
            tei_mean = row['mean']
            tei_std = row['std']
            tei_q25 = row['q25']
            tei_q75 = row['q75']
            
            if tei_std > 0:  # Ã‰viter la division par zÃ©ro
                mask = df['PRODUCT_ORIGIN_KEY'] == key
                
                # Calculer l'Ã©cart par rapport Ã  la moyenne
                tei_deviation = np.abs(df.loc[mask, 'TEI_CALCULE'] - tei_mean) / tei_std
                df.loc[mask, 'MIRROR_TEI_DEVIATION'] = tei_deviation
                
                # Score basÃ© sur l'Ã©cart interquartile (plus robuste)
                iqr = tei_q75 - tei_q25
                if iqr > 0:
                    iqr_score = np.abs(df.loc[mask, 'TEI_CALCULE'] - tei_mean) / iqr
                    df.loc[mask, 'MIRROR_TEI_SCORE'] = iqr_score
                    
                    # Anomalie si TEI en dehors de l'intervalle [Q25 - 1.5*IQR, Q75 + 1.5*IQR]
                    lower_bound = tei_q25 - 1.5 * iqr
                    upper_bound = tei_q75 + 1.5 * iqr
                    anomaly_mask = (df.loc[mask, 'TEI_CALCULE'] < lower_bound) | (df.loc[mask, 'TEI_CALCULE'] > upper_bound)
                    df.loc[mask & anomaly_mask, 'MIRROR_TEI_ANOMALY'] = 1
        
        # Sauvegarder les seuils TEI
        self.tei_thresholds = tei_stats.set_index('PRODUCT_ORIGIN_KEY').to_dict('index')
        
        anomalies_count = df['MIRROR_TEI_ANOMALY'].sum()
        logger.info(f"   Anomalies TEI dÃ©tectÃ©es: {anomalies_count}")
        
        return df
    
    def spectral_clustering_anomaly_detection(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DÃ©tection d'anomalies par clustering spectral - VERSION RAPIDE
        Utilise des seuils statistiques simples au lieu du clustering
        """
        logger.info("ðŸ” DÃ©tection d'anomalies par seuils statistiques (rapide)...")
        
        # Utiliser des seuils statistiques simples au lieu du clustering
        df['SPECTRAL_CLUSTER_ANOMALY'] = 0
        df['SPECTRAL_CLUSTER_SCORE'] = 0.0
        
        # DÃ©tecter les anomalies basÃ©es sur des seuils statistiques
        if 'VALEUR_CAF' in df.columns:
            # Valeurs CAF anormalement Ã©levÃ©es ou basses
            q99 = df['VALEUR_CAF'].quantile(0.99)
            q01 = df['VALEUR_CAF'].quantile(0.01)
            mask_valeur_anormale = (df['VALEUR_CAF'] > q99) | (df['VALEUR_CAF'] < q01)
            df.loc[mask_valeur_anormale, 'SPECTRAL_CLUSTER_ANOMALY'] = 1
            df.loc[mask_valeur_anormale, 'SPECTRAL_CLUSTER_SCORE'] = 1.0
        
        if 'POIDS_NET' in df.columns:
            # Poids anormalement Ã©levÃ©s ou bas
            q99 = df['POIDS_NET'].quantile(0.99)
            q01 = df['POIDS_NET'].quantile(0.01)
            mask_poids_anormal = (df['POIDS_NET'] > q99) | (df['POIDS_NET'] < q01)
            df.loc[mask_poids_anormal, 'SPECTRAL_CLUSTER_ANOMALY'] = 1
            df.loc[mask_poids_anormal, 'SPECTRAL_CLUSTER_SCORE'] = 1.0
        
        anomalies_count = df['SPECTRAL_CLUSTER_ANOMALY'].sum()
        logger.info(f"   Anomalies dÃ©tectÃ©es (seuils statistiques): {anomalies_count}")
        
        return df
    
    def hierarchical_clustering_anomaly_detection(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DÃ©tection d'anomalies par clustering hiÃ©rarchique - VERSION RAPIDE
        Utilise des seuils statistiques simples au lieu du clustering
        """
        logger.info("ðŸŒ³ DÃ©tection d'anomalies par seuils statistiques (rapide)...")
        
        # Utiliser des seuils statistiques simples au lieu du clustering
        df['HIERARCHICAL_CLUSTER_ANOMALY'] = 0
        df['HIERARCHICAL_CLUSTER_SCORE'] = 0.0
        
        # DÃ©tecter les anomalies basÃ©es sur des seuils statistiques
        if 'MONTANT_LIQUIDATION' in df.columns:
            # Montants de liquidation anormalement Ã©levÃ©s
            q99 = df['MONTANT_LIQUIDATION'].quantile(0.99)
            mask_liquidation_anormale = df['MONTANT_LIQUIDATION'] > q99
            df.loc[mask_liquidation_anormale, 'HIERARCHICAL_CLUSTER_ANOMALY'] = 1
            df.loc[mask_liquidation_anormale, 'HIERARCHICAL_CLUSTER_SCORE'] = 1.0
        
        if 'VALEUR_UNITAIRE_KG' in df.columns:
            # Valeurs unitaires anormalement Ã©levÃ©es ou basses
            q99 = df['VALEUR_UNITAIRE_KG'].quantile(0.99)
            q01 = df['VALEUR_UNITAIRE_KG'].quantile(0.01)
            mask_valeur_unitaire_anormale = (df['VALEUR_UNITAIRE_KG'] > q99) | (df['VALEUR_UNITAIRE_KG'] < q01)
            df.loc[mask_valeur_unitaire_anormale, 'HIERARCHICAL_CLUSTER_ANOMALY'] = 1
            df.loc[mask_valeur_unitaire_anormale, 'HIERARCHICAL_CLUSTER_SCORE'] = 1.0
        
        anomalies_count = df['HIERARCHICAL_CLUSTER_ANOMALY'].sum()
        logger.info(f"   Anomalies dÃ©tectÃ©es (seuils statistiques): {anomalies_count}")
        
        return df
    
    def admin_values_control(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ContrÃ´le des valeurs administrÃ©es
        Compare les valeurs dÃ©clarÃ©es avec les valeurs de rÃ©fÃ©rence administratives
        """
        logger.info("ðŸ›ï¸ ContrÃ´le des valeurs administrÃ©es...")
        
        # CrÃ©er la clÃ© produit/origine
        # Adapter aux noms de colonnes aprÃ¨s agrÃ©gation
        if 'CODE_PRODUIT_STR' in df.columns:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT_STR'] + '_' + df['PAYS_ORIGINE_STR']
        else:
            df['PRODUCT_ORIGIN_KEY'] = df['CODE_PRODUIT'].astype(str) + '_' + df['PAYS_ORIGINE'].astype(str)
        
        # Calculer les valeurs de rÃ©fÃ©rence administratives (mÃ©diane par couple produit/origine)
        admin_values = df.groupby('PRODUCT_ORIGIN_KEY')['VALEUR_CAF'].agg([
            'count', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
        ]).reset_index()
        admin_values.columns = ['PRODUCT_ORIGIN_KEY', 'count', 'median', 'q25', 'q75']
        
        # Filtrer les couples avec au moins 5 observations
        admin_values = admin_values[admin_values['count'] >= 5]
        
        # CrÃ©er les features de contrÃ´le
        df['ADMIN_VALUES_ANOMALY'] = 0
        df['ADMIN_VALUES_SCORE'] = 0.0
        df['ADMIN_VALUES_DEVIATION'] = 0.0
        
        for _, row in admin_values.iterrows():
            key = row['PRODUCT_ORIGIN_KEY']
            admin_median = row['median']
            admin_q25 = row['q25']
            admin_q75 = row['q75']
            
            mask = df['PRODUCT_ORIGIN_KEY'] == key
            
            # Calculer l'Ã©cart par rapport Ã  la valeur administrÃ©e
            deviation = np.abs(df.loc[mask, 'VALEUR_CAF'] - admin_median) / admin_median
            df.loc[mask, 'ADMIN_VALUES_DEVIATION'] = deviation
            
            # Score basÃ© sur l'Ã©cart interquartile
            iqr = admin_q75 - admin_q25
            if iqr > 0:
                iqr_score = np.abs(df.loc[mask, 'VALEUR_CAF'] - admin_median) / iqr
                df.loc[mask, 'ADMIN_VALUES_SCORE'] = iqr_score
                
                # Anomalie si valeur en dehors de l'intervalle [Q25 - 1.5*IQR, Q75 + 1.5*IQR]
                lower_bound = admin_q25 - 1.5 * iqr
                upper_bound = admin_q75 + 1.5 * iqr
                anomaly_mask = (df.loc[mask, 'VALEUR_CAF'] < lower_bound) | (df.loc[mask, 'VALEUR_CAF'] > upper_bound)
                df.loc[mask & anomaly_mask, 'ADMIN_VALUES_ANOMALY'] = 1
        
        # Sauvegarder les valeurs administrÃ©es
        self.admin_values = admin_values.set_index('PRODUCT_ORIGIN_KEY').to_dict('index')
        
        anomalies_count = df['ADMIN_VALUES_ANOMALY'].sum()
        logger.info(f"   Anomalies valeurs administrÃ©es: {anomalies_count}")
        
        return df
    
    def create_comprehensive_fraud_flag(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        CrÃ©er le FRAUD_FLAG basÃ© sur toutes les techniques de dÃ©tection
        """
        logger.info("ðŸŽ¯ CrÃ©ation du FRAUD_FLAG complet...")
        
        # Initialiser le flag de fraude
        df['FRAUD_FLAG'] = 0
        
        # 1. BienaymÃ©-Tchebychev (seuil strict)
        df.loc[df['BIENAYME_CHEBYCHEV_ANOMALY'] == 1, 'FRAUD_FLAG'] = 1
        
        # 2. Analyse miroir TEI (seuil strict)
        df.loc[df['MIRROR_TEI_ANOMALY'] == 1, 'FRAUD_FLAG'] = 1
        
        # 3. Clustering spectral (seuil strict)
        df.loc[df['SPECTRAL_CLUSTER_ANOMALY'] == 1, 'FRAUD_FLAG'] = 1
        
        # 4. Clustering hiÃ©rarchique (seuil strict)
        df.loc[df['HIERARCHICAL_CLUSTER_ANOMALY'] == 1, 'FRAUD_FLAG'] = 1
        
        # 5. Valeurs administrÃ©es (seuil strict)
        df.loc[df['ADMIN_VALUES_ANOMALY'] == 1, 'FRAUD_FLAG'] = 1
        
        # 6. Score composite (seuil adaptatif)
        # Normaliser les scores entre 0 et 1
        score_columns = [
            'BIENAYME_CHEBYCHEV_SCORE',
            'MIRROR_TEI_SCORE', 
            'SPECTRAL_CLUSTER_SCORE',
            'HIERARCHICAL_CLUSTER_SCORE',
            'ADMIN_VALUES_SCORE'
        ]
        
        composite_score = np.zeros(len(df))
        for col in score_columns:
            if col in df.columns:
                # Normaliser le score
                col_min = df[col].min()
                col_max = df[col].max()
                if col_max > col_min:
                    normalized_score = (df[col] - col_min) / (col_max - col_min)
                    composite_score += normalized_score
        
        df['COMPOSITE_FRAUD_SCORE'] = composite_score / len(score_columns)
        
        # Seuil adaptatif basÃ© sur le 95Ã¨me percentile
        threshold = df['COMPOSITE_FRAUD_SCORE'].quantile(0.95)
        df.loc[df['COMPOSITE_FRAUD_SCORE'] > threshold, 'FRAUD_FLAG'] = 1
        
        # Statistiques finales
        fraud_count = df['FRAUD_FLAG'].sum()
        fraud_rate = fraud_count / len(df) * 100
        
        logger.info(f"âœ… FRAUD_FLAG crÃ©Ã©: {fraud_count} fraudes ({fraud_rate:.1f}%)")
        
        return df
    
    def run_complete_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ExÃ©cuter l'analyse complÃ¨te de dÃ©tection de fraude
        """
        logger.info("ðŸš€ DÃ‰MARRAGE DE L'ANALYSE COMPLÃˆTE DE DÃ‰TECTION DE FRAUDE")
        logger.info("=" * 70)
        
        # 1. Toilettage des donnÃ©es
        df = self.clean_data_comprehensive(df)
        
        # 2. MÃ©thodes probabilistes
        df = self.bienayme_chebychev_analysis(df)
        
        # 3. Analyse miroir TEI
        df = self.mirror_analysis_tei(df)
        
        # 4. DÃ©tection d'anomalies - Clustering spectral
        df = self.spectral_clustering_anomaly_detection(df)
        
        # 5. DÃ©tection d'anomalies - Clustering hiÃ©rarchique
        df = self.hierarchical_clustering_anomaly_detection(df)
        
        # 6. ContrÃ´le des valeurs administrÃ©es
        df = self.admin_values_control(df)
        
        # 7. CrÃ©ation du FRAUD_FLAG complet
        df = self.create_comprehensive_fraud_flag(df)
        
        logger.info("=" * 70)
        logger.info("âœ… ANALYSE COMPLÃˆTE TERMINÃ‰E")
        logger.info(f"ðŸ“Š DonnÃ©es finales: {df.shape}")
        logger.info(f"ðŸŽ¯ Taux de fraude: {df['FRAUD_FLAG'].mean()*100:.1f}%")
        logger.info("=" * 70)
        
        return df
