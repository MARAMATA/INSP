#!/usr/bin/env python3
"""
SystÃ¨me de retraining automatique des modÃ¨les ML avec les feedbacks
"""

import os
import sys
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import joblib
import psycopg2
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import threading
import time

# Ajouter le chemin du backend
backend_root = Path(__file__).resolve().parents[2]
sys.path.append(str(backend_root))

from src.chapters.chap30.ml_model_advanced import Chap30MLAdvanced
from src.chapters.chap84.ml_model_advanced import Chap84MLAdvanced
from src.chapters.chap85.ml_model_advanced import Chap85MLAdvanced

logger = logging.getLogger(__name__)

class MLRetrainingSystem:
    """
    SystÃ¨me de retraining automatique des modÃ¨les ML basÃ© sur les feedbacks
    """
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.retraining_lock = threading.Lock()
        self.last_retraining = {}
        self.retraining_interval = 1 * 60 * 60  # 1 heure en secondes (pour les tests)
        self.min_feedbacks_for_retraining = 10  # Seuil rÃ©duit pour les tests
        self.models_dir = backend_root / "results"
        
        # Configuration des modÃ¨les par chapitre
        self.chapter_models = {
            "chap30": Chap30MLAdvanced,
            "chap84": Chap84MLAdvanced,
            "chap85": Chap85MLAdvanced
        }
        
        logger.info("ğŸš€ SystÃ¨me de retraining ML initialisÃ©")
    
    def should_retrain(self, chapter: str) -> bool:
        """DÃ©termine si un modÃ¨le doit Ãªtre retrainÃ©"""
        try:
            # VÃ©rifier l'intervalle de temps
            last_time = self.last_retraining.get(chapter, 0)
            current_time = time.time()
            
            if current_time - last_time < self.retraining_interval:
                logger.info(f"â° Retraining trop rÃ©cent pour {chapter}")
                return False
            
            # VÃ©rifier le nombre de feedbacks
            feedback_count = self._get_feedback_count(chapter)
            if feedback_count < self.min_feedbacks_for_retraining:
                logger.info(f"ğŸ“Š Pas assez de feedbacks pour {chapter}: {feedback_count}")
                return False
            
            # VÃ©rifier la qualitÃ© des feedbacks
            feedback_quality = self._get_feedback_quality(chapter)
            if feedback_quality < 0.3:  # Seuil de qualitÃ© trÃ¨s rÃ©duit pour les tests
                logger.info(f"ğŸ“‰ QualitÃ© des feedbacks insuffisante pour {chapter}: {feedback_quality}")
                return False
            
            logger.info(f"âœ… Conditions de retraining remplies pour {chapter}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur vÃ©rification retraining {chapter}: {e}")
            return False
    
    def retrain_model(self, chapter: str) -> Dict[str, Any]:
        """Retrain un modÃ¨le ML avec les nouveaux feedbacks"""
        with self.retraining_lock:
            try:
                logger.info(f"ğŸ”„ DÃ©but du retraining pour {chapter}")
                
                # 1. RÃ©cupÃ©rer les donnÃ©es d'entraÃ®nement existantes
                existing_data = self._load_existing_training_data(chapter)
                if existing_data is None:
                    return {"success": False, "error": "DonnÃ©es d'entraÃ®nement non trouvÃ©es"}
                
                # 2. RÃ©cupÃ©rer les nouveaux feedbacks
                new_feedbacks = self._get_new_feedbacks(chapter)
                if not new_feedbacks:
                    return {"success": False, "error": "Aucun nouveau feedback"}
                
                # 3. PrÃ©parer les nouvelles donnÃ©es
                new_data = self._prepare_feedback_data(new_feedbacks, chapter)
                if new_data is None:
                    return {"success": False, "error": "Erreur prÃ©paration des donnÃ©es"}
                
                # 4. Combiner les donnÃ©es
                combined_data = self._combine_training_data(existing_data, new_data)
                
                # 5. EntraÃ®ner le nouveau modÃ¨le
                model_class = self.chapter_models[chapter]
                ml_system = model_class()
                
                # Diviser les donnÃ©es
                X = combined_data.drop('FRAUD_FLAG', axis=1)
                y = combined_data['FRAUD_FLAG']
                
                X_train, X_temp, y_train, y_temp = train_test_split(
                    X, y, test_size=0.3, random_state=42, stratify=y
                )
                X_val, X_test, y_val, y_test = train_test_split(
                    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
                )
                
                splits = {
                    'X_train': X_train, 'y_train': y_train,
                    'X_val': X_val, 'y_val': y_val,
                    'X_test': X_test, 'y_test': y_test
                }
                
                # CrÃ©er le pipeline de prÃ©processing
                preprocessor = ml_system.create_preprocessing_pipeline(X_train)
                
                # EntraÃ®ner les modÃ¨les
                results, trained_models, validation_results = ml_system.train_models(splits, preprocessor)
                
                # Trouver le meilleur modÃ¨le
                best_model_name = ml_system.find_best_model(validation_results)
                best_model = trained_models[best_model_name]
                
                # 6. Ã‰valuer le nouveau modÃ¨le
                evaluation = self._evaluate_model(best_model, X_test, y_test, chapter)
                
                # 7. Sauvegarder le nouveau modÃ¨le si meilleur
                if evaluation["improvement"]:
                    self._save_retrained_model(best_model, best_model_name, chapter, evaluation)
                    self.last_retraining[chapter] = time.time()
                    
                    logger.info(f"âœ… Retraining rÃ©ussi pour {chapter} - AmÃ©lioration: {evaluation['improvement_score']:.4f}")
                    
                    return {
                        "success": True,
                        "chapter": chapter,
                        "best_model": best_model_name,
                        "improvement": evaluation["improvement_score"],
                        "new_metrics": evaluation.get("metrics", {}),
                        "feedbacks_used": len(new_feedbacks),
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    logger.info(f"âš ï¸ Retraining pour {chapter} - Pas d'amÃ©lioration")
                    return {
                        "success": False,
                        "chapter": chapter,
                        "reason": "Pas d'amÃ©lioration",
                        "evaluation": evaluation
                    }
                
            except Exception as e:
                logger.error(f"âŒ Erreur retraining {chapter}: {e}")
                return {"success": False, "error": str(e)}
    
    def _get_feedback_count(self, chapter: str) -> int:
        """RÃ©cupÃ¨re le nombre de feedbacks pour un chapitre"""
        try:
            conn = psycopg2.connect(self.database_url)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT COUNT(*) FROM advanced_feedbacks 
                WHERE chapter_id = %s AND created_at > %s
            """, (chapter, datetime.now() - timedelta(days=7)))
            
            count = cursor.fetchone()[0]
            cursor.close()
            conn.close()
            
            return count
            
        except Exception as e:
            logger.error(f"Erreur rÃ©cupÃ©ration feedbacks {chapter}: {e}")
            return 0
    
    def _get_feedback_quality(self, chapter: str) -> float:
        """Calcule la qualitÃ© moyenne des feedbacks"""
        try:
            conn = psycopg2.connect(self.database_url)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT AVG(feedback_quality_score) FROM advanced_feedbacks 
                WHERE chapter_id = %s AND created_at > %s
            """, (chapter, datetime.now() - timedelta(days=7)))
            
            quality = cursor.fetchone()[0]
            cursor.close()
            conn.close()
            
            return float(quality) if quality else 0.0
            
        except Exception as e:
            logger.error(f"Erreur qualitÃ© feedbacks {chapter}: {e}")
            return 0.0
    
    def _load_existing_training_data(self, chapter: str) -> Optional[pd.DataFrame]:
        """Charge les donnÃ©es d'entraÃ®nement existantes"""
        try:
            data_path = backend_root / "data" / "processed" / f"{chapter.upper()}_PROCESSED_ADVANCED.csv"
            if data_path.exists():
                data = pd.read_csv(data_path)
                logger.info(f"ğŸ“Š DonnÃ©es existantes chargÃ©es pour {chapter}: {len(data)} lignes")
                return data
            else:
                logger.warning(f"âš ï¸ Fichier de donnÃ©es non trouvÃ©: {data_path}")
                return None
                
        except Exception as e:
            logger.error(f"Erreur chargement donnÃ©es {chapter}: {e}")
            return None
    
    def _get_new_feedbacks(self, chapter: str) -> List[Dict[str, Any]]:
        """RÃ©cupÃ¨re les nouveaux feedbacks depuis la base de donnÃ©es"""
        try:
            conn = psycopg2.connect(self.database_url)
            cursor = conn.cursor()
            
            # RÃ©cupÃ©rer les feedbacks des 7 derniers jours
            cursor.execute("""
                SELECT declaration_id, inspector_decision, inspector_confidence,
                       predicted_fraud, predicted_probability, context_json,
                       feedback_quality_score, inspector_expertise_level
                FROM advanced_feedbacks 
                WHERE chapter_id = %s 
                AND created_at > %s
                AND feedback_quality_score > 0.6
                ORDER BY created_at DESC
            """, (chapter, datetime.now() - timedelta(days=7)))
            
            feedbacks = []
            for row in cursor.fetchall():
                feedbacks.append({
                    "declaration_id": row[0],
                    "inspector_decision": bool(row[1]),
                    "inspector_confidence": float(row[2]),
                    "predicted_fraud": bool(row[3]),
                    "predicted_probability": float(row[4]),
                    "context": json.loads(row[5]) if row[5] else {},
                    "quality_score": float(row[6]),
                    "expertise_level": row[7]
                })
            
            cursor.close()
            conn.close()
            
            logger.info(f"ğŸ“¥ {len(feedbacks)} nouveaux feedbacks rÃ©cupÃ©rÃ©s pour {chapter}")
            return feedbacks
            
        except Exception as e:
            logger.error(f"Erreur rÃ©cupÃ©ration feedbacks {chapter}: {e}")
            return []
    
    def _prepare_feedback_data(self, feedbacks: List[Dict[str, Any]], chapter: str) -> Optional[pd.DataFrame]:
        """PrÃ©pare les donnÃ©es de feedback pour l'entraÃ®nement"""
        try:
            if not feedbacks:
                return None
            
            # Convertir les feedbacks en DataFrame
            data_rows = []
            
            for feedback in feedbacks:
                context = feedback["context"]
                
                # CrÃ©er une ligne de donnÃ©es basÃ©e sur le contexte
                row = {
                    "ANNEE": context.get("ANNEE", 2024),
                    "BUREAU": context.get("BUREAU", "UNKNOWN"),
                    "NUMERO": context.get("NUMERO", "000"),
                    "VALEUR": context.get("VALEUR", 0.0),
                    "POIDS": context.get("POIDS", 0.0),
                    "PAYS_ORIGINE": context.get("PAYS_ORIGINE", "UNKNOWN"),
                    "CODE_SH": context.get("CODE_SH", "00000000"),
                    "FRAUD_FLAG": int(feedback["inspector_decision"]),  # Utiliser la dÃ©cision de l'inspecteur
                    "feedback_quality": feedback["quality_score"],
                    "inspector_confidence": feedback["inspector_confidence"],
                    "expertise_level": feedback["expertise_level"]
                }
                
                # Ajouter les features business spÃ©cifiques au chapitre
                if chapter == "chap30":
                    row.update({
                        "BUSINESS_GLISSEMENT_COSMETIQUE": context.get("BUSINESS_GLISSEMENT_COSMETIQUE", 0),
                        "BUSINESS_IS_MEDICAMENT": context.get("BUSINESS_IS_MEDICAMENT", 0),
                        "BUSINESS_IS_ANTIPALUDEEN": context.get("BUSINESS_IS_ANTIPALUDEEN", 0)
                    })
                elif chapter == "chap84":
                    row.update({
                        "BUSINESS_GLISSEMENT_MACHINE": context.get("BUSINESS_GLISSEMENT_MACHINE", 0),
                        "BUSINESS_IS_MACHINE": context.get("BUSINESS_IS_MACHINE", 0),
                        "BUSINESS_IS_ELECTRONIQUE": context.get("BUSINESS_IS_ELECTRONIQUE", 0)
                    })
                elif chapter == "chap85":
                    row.update({
                        "BUSINESS_GLISSEMENT_ELECTRONIQUE": context.get("BUSINESS_GLISSEMENT_ELECTRONIQUE", 0),
                        "BUSINESS_IS_ELECTRONIQUE": context.get("BUSINESS_IS_ELECTRONIQUE", 0),
                        "BUSINESS_IS_TELEPHONE": context.get("BUSINESS_IS_TELEPHONE", 0)
                    })
                
                data_rows.append(row)
            
            df = pd.DataFrame(data_rows)
            logger.info(f"ğŸ“Š DonnÃ©es de feedback prÃ©parÃ©es pour {chapter}: {len(df)} lignes")
            
            return df
            
        except Exception as e:
            logger.error(f"Erreur prÃ©paration donnÃ©es feedback {chapter}: {e}")
            return None
    
    def _combine_training_data(self, existing_data: pd.DataFrame, new_data: pd.DataFrame) -> pd.DataFrame:
        """Combine les donnÃ©es existantes avec les nouvelles donnÃ©es de feedback"""
        try:
            # S'assurer que les colonnes sont compatibles
            common_columns = list(set(existing_data.columns) & set(new_data.columns))
            
            existing_subset = existing_data[common_columns]
            new_subset = new_data[common_columns]
            
            # Combiner les donnÃ©es
            combined = pd.concat([existing_subset, new_subset], ignore_index=True)
            
            # Supprimer les doublons basÃ©s sur les colonnes clÃ©s
            key_columns = ["ANNEE", "BUREAU", "NUMERO"]
            if all(col in combined.columns for col in key_columns):
                combined = combined.drop_duplicates(subset=key_columns, keep='last')
            
            logger.info(f"ğŸ“Š DonnÃ©es combinÃ©es: {len(existing_data)} + {len(new_data)} = {len(combined)}")
            
            return combined
            
        except Exception as e:
            logger.error(f"Erreur combinaison donnÃ©es: {e}")
            return existing_data
    
    def _evaluate_model(self, model, X_test: pd.DataFrame, y_test: pd.Series, chapter: str) -> Dict[str, Any]:
        """Ã‰value le nouveau modÃ¨le et compare avec l'ancien"""
        try:
            # PrÃ©dictions du nouveau modÃ¨le
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # MÃ©triques du nouveau modÃ¨le
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            
            new_metrics = {
                "accuracy": accuracy_score(y_test, y_pred),
                "precision": precision_score(y_test, y_pred, zero_division=0),
                "recall": recall_score(y_test, y_pred, zero_division=0),
                "f1": f1_score(y_test, y_pred, zero_division=0),
                "auc": roc_auc_score(y_test, y_pred_proba)
            }
            
            # Charger les mÃ©triques de l'ancien modÃ¨le
            old_metrics = self._load_old_model_metrics(chapter)
            
            # Calculer l'amÃ©lioration
            improvement_score = 0.0
            if old_metrics:
                for metric in ["f1", "auc", "precision", "recall"]:
                    if metric in new_metrics and metric in old_metrics:
                        improvement_score += (new_metrics[metric] - old_metrics[metric])
                
                improvement_score /= 4  # Moyenne des amÃ©liorations
            
            improvement = improvement_score > 0.01  # Seuil d'amÃ©lioration de 1%
            
            return {
                "metrics": new_metrics,
                "old_metrics": old_metrics,
                "improvement": improvement,
                "improvement_score": improvement_score
            }
            
        except Exception as e:
            logger.error(f"Erreur Ã©valuation modÃ¨le {chapter}: {e}")
            return {
                "metrics": {},
                "old_metrics": {},
                "improvement": False,
                "improvement_score": 0.0
            }
    
    def _load_old_model_metrics(self, chapter: str) -> Optional[Dict[str, float]]:
        """Charge les mÃ©triques de l'ancien modÃ¨le"""
        try:
            # Utiliser les mÃ©triques de la configuration par dÃ©faut
            from src.shared.ocr_pipeline import CHAPTER_CONFIGS
            
            config = CHAPTER_CONFIGS.get(chapter, {})
            if config:
                return {
                    "f1": config.get("f1_score", 0.0),
                    "auc": config.get("auc_score", 0.0),
                    "precision": config.get("precision", 0.0),
                    "recall": config.get("recall", 0.0),
                    "accuracy": config.get("accuracy", 0.0)
                }
            
            # Fallback vers le fichier YAML si disponible
            metrics_file = self.models_dir / chapter / "ml_supervised_report.yaml"
            if metrics_file.exists():
                import yaml
                with open(metrics_file, 'r') as f:
                    data = yaml.safe_load(f)
                    return data.get("test_metrics", {})
            
            return None
            
        except Exception as e:
            logger.error(f"Erreur chargement mÃ©triques ancien modÃ¨le {chapter}: {e}")
            return None
    
    def _save_retrained_model(self, model, model_name: str, chapter: str, evaluation: Dict[str, Any]):
        """Sauvegarde le nouveau modÃ¨le retrainÃ©"""
        try:
            # CrÃ©er le rÃ©pertoire si nÃ©cessaire
            chapter_dir = self.models_dir / chapter / "models"
            chapter_dir.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder le modÃ¨le
            model_path = chapter_dir / f"{model_name}_model.pkl"
            joblib.dump(model, model_path)
            
            # Sauvegarder les mÃ©triques
            metrics_path = chapter_dir / f"{model_name}_retraining_metrics.json"
            with open(metrics_path, 'w') as f:
                json.dump({
                    "model_name": model_name,
                    "chapter": chapter,
                    "retraining_timestamp": datetime.now().isoformat(),
                    "metrics": evaluation["metrics"],
                    "improvement_score": evaluation["improvement_score"],
                    "old_metrics": evaluation["old_metrics"]
                }, f, indent=2)
            
            # Mettre Ã  jour le fichier optimal_thresholds.json
            self._update_optimal_thresholds(chapter, model_name, evaluation["metrics"])
            
            logger.info(f"ğŸ’¾ ModÃ¨le retrainÃ© sauvegardÃ©: {model_path}")
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde modÃ¨le retrainÃ© {chapter}: {e}")
    
    def _update_optimal_thresholds(self, chapter: str, model_name: str, metrics: Dict[str, float]):
        """Met Ã  jour les seuils optimaux aprÃ¨s retraining"""
        try:
            thresholds_file = self.models_dir / chapter / "optimal_thresholds.json"
            
            # Calculer le nouveau seuil optimal basÃ© sur la prÃ©cision
            precision = metrics.get("precision", 0.5)
            optimal_threshold = 1.0 - precision  # Seuil inverse de la prÃ©cision
            
            # Charger les seuils existants
            if thresholds_file.exists():
                with open(thresholds_file, 'r') as f:
                    thresholds = json.load(f)
            else:
                thresholds = {}
            
            # Mettre Ã  jour
            thresholds[model_name] = {
                "optimal_threshold": optimal_threshold,
                "precision": precision,
                "f1": metrics.get("f1", 0.0),
                "auc": metrics.get("auc", 0.0),
                "last_updated": datetime.now().isoformat()
            }
            
            # Sauvegarder
            with open(thresholds_file, 'w') as f:
                json.dump(thresholds, f, indent=2)
            
            logger.info(f"ğŸ“Š Seuils optimaux mis Ã  jour pour {chapter}: {optimal_threshold:.4f}")
            
        except Exception as e:
            logger.error(f"Erreur mise Ã  jour seuils {chapter}: {e}")
    
    def check_and_retrain_all(self) -> Dict[str, Any]:
        """VÃ©rifie et retrain tous les modÃ¨les si nÃ©cessaire"""
        results = {}
        
        for chapter in ["chap30", "chap84", "chap85"]:
            try:
                if self.should_retrain(chapter):
                    logger.info(f"ğŸ”„ Lancement du retraining pour {chapter}")
                    result = self.retrain_model(chapter)
                    results[chapter] = result
                else:
                    results[chapter] = {
                        "success": False,
                        "reason": "Conditions non remplies",
                        "chapter": chapter
                    }
                    
            except Exception as e:
                logger.error(f"Erreur retraining {chapter}: {e}")
                results[chapter] = {
                    "success": False,
                    "error": str(e),
                    "chapter": chapter
                }
        
        return results

# Instance globale du systÃ¨me de retraining
_retraining_system = None

def get_retraining_system(database_url: str = None) -> MLRetrainingSystem:
    """RÃ©cupÃ¨re l'instance globale du systÃ¨me de retraining"""
    global _retraining_system
    
    if _retraining_system is None:
        if database_url is None:
            # Configuration par dÃ©faut
            database_url = "postgresql://maramata:maramata@localhost:5432/INSPECT_IA"
        
        _retraining_system = MLRetrainingSystem(database_url)
    
    return _retraining_system

def trigger_retraining(chapter: str) -> Dict[str, Any]:
    """DÃ©clenche le retraining pour un chapitre spÃ©cifique"""
    system = get_retraining_system()
    return system.retrain_model(chapter)

def check_retraining_status() -> Dict[str, Any]:
    """VÃ©rifie le statut de retraining de tous les chapitres"""
    system = get_retraining_system()
    return system.check_and_retrain_all()


















