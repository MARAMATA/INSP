# backend/src/shared/ocr_pipeline.py
"""
Pipeline OCR COMPL√àTEMENT ADAPT√â aux r√©sultats finaux des 3 chapitres
MOD√àLES S√âLECTIONN√âS SUR F1-SCORE DE VALIDATION (Convention train/val/test respect√©e)

- Chapitre 30: Produits pharmaceutiques (XGBoost)
  * Validation F1: 0.9815 ‚≠ê (crit√®re de s√©lection)
  * Test F1: 0.9796, Test AUC: 0.9995
  * Seuil optimal: 0.35
  * Donn√©es: 25,334 √©chantillons (Fraude: 19.44%)

- Chapitre 84: Machines et appareils m√©caniques (XGBoost)
  * Validation F1: 0.9891 ‚≠ê (crit√®re de s√©lection)
  * Test F1: 0.9887, Test AUC: 0.9997
  * Seuil optimal: 0.25
  * Donn√©es: 264,494 √©chantillons (Fraude: 26.80%)

- Chapitre 85: Appareils √©lectriques (XGBoost)
  * Validation F1: 0.9808 ‚≠ê (crit√®re de s√©lection)
  * Test F1: 0.9808, Test AUC: 0.9993
  * Seuil optimal: 0.20
  * Donn√©es: 197,402 √©chantillons (Fraude: 21.32%)

- Int√©gration RL-ML avanc√©e avec mod√®les ML avanc√©s (sans calibration)
- Features business optimis√©es par corr√©lation pour chaque chapitre
- Protection robuste contre data leakage et overfitting
- Hyperparam√®tres optimis√©s avec validation compl√®te de production
- Mod√®les ML avanc√©s dans results/{chapter}/models/ avec optimal_thresholds.json
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import pandas as pd
import re
import hashlib
import logging
import threading
import json
import sys
import os

def stable_hash_float(value: str, mod: int = 1000) -> float:
    """Hash stable et d√©terministe pour l'encodage des features cat√©gorielles"""
    if value is None:
        return 0.0
    s = str(value).strip()
    if s == "" or s.lower() in {"nan", "none"}:
        return 0.0
    return float(int(hashlib.md5(s.encode("utf-8")).hexdigest(), 16) % mod)

# Configuration du logger
logger = logging.getLogger(__name__)

# Cache global des mod√®les ML pour optimiser les performances
_MODEL_CACHE = {}
_CACHE_LOCK = threading.Lock()

# Cache global des mod√®les RL pour optimiser les performances
_RL_CACHE = {}
_RL_CACHE_LOCK = threading.Lock()

# -------------------------------
# CONFIGURATIONS DES CHAPITRES
# -------------------------------

CHAPTER_CONFIGS = {
    "chap30": {
        "name": "Produits pharmaceutiques",
        "best_model": "xgboost",  # XGBoost - S√©lectionn√© sur F1 Validation: 0.9815
        "features_count": 41,  # Nombre r√©el de colonnes (features optimis√©es)
        "f1_score": 0.9796,  # Test F1 XGBoost (Validation F1: 0.9815)
        "auc_score": 0.9995,  # Test AUC XGBoost
        "precision": 0.9889,  # Test Precision XGBoost
        "recall": 0.9705,  # Test Recall XGBoost
        "accuracy": 0.9796,  # Test Accuracy
        "validation_f1": 0.9815,  # F1 de validation (crit√®re de s√©lection)
        "fraud_rate": 19.44,  # Taux r√©el de fraude dans les donn√©es
        "data_size": 25334,  # Taille r√©elle des donn√©es
        "leakage_risk": "LOW",
        "validation_status": "ROBUST",
        "optimal_threshold": 0.35,  # Seuil optimal recalcul√© scientifiquement (maximise F1)
        "model_performance": {
            "train_samples": 16213,  # Train set
            "valid_samples": 4054,   # Validation set
            "test_samples": 5067,    # Test set
            "base_rate": 0.1944,     # Taux r√©el de fraude
            "auc_score": 0.9995,
            "f1_score": 0.9796,
            "precision": 0.9889,
            "recall": 0.9705
        }
    },
    "chap84": {
        "name": "Machines et appareils m√©caniques",
        "best_model": "xgboost",  # XGBoost - S√©lectionn√© sur F1 Validation: 0.9891
        "features_count": 43,  # Nombre r√©el de colonnes dans CHAP84_PROCESSED_ADVANCED.csv
        "f1_score": 0.9887,  # Test F1 XGBoost (Validation F1: 0.9891)
        "auc_score": 0.9997,  # Test AUC XGBoost
        "precision": 0.9942,  # Test Precision XGBoost
        "recall": 0.9833,  # Test Recall XGBoost
        "accuracy": 0.9887,  # Test Accuracy
        "validation_f1": 0.9891,  # F1 de validation (crit√®re de s√©lection)
        "fraud_rate": 26.80,  # Taux r√©el de fraude dans les donn√©es
        "data_size": 264494,  # Taille r√©elle des donn√©es
        "leakage_risk": "LOW",
        "validation_status": "ROBUST",
        "optimal_threshold": 0.20,  # Seuil optimal recalcul√© scientifiquement (maximise F1)
        "model_performance": {
            "train_samples": 169276,  # Train set
            "valid_samples": 42319,   # Validation set
            "test_samples": 52899,    # Test set
            "base_rate": 0.2680,      # Taux r√©el de fraude
            "auc_score": 0.9997,      # AUC XGBoost
            "f1_score": 0.9887,       # F1 XGBoost
            "precision": 0.9942,      # Precision XGBoost
            "recall": 0.9833          # Recall XGBoost
        }
    },
    "chap85": {
        "name": "Appareils √©lectriques",
        "best_model": "xgboost",  # XGBoost - S√©lectionn√© sur F1 Validation: 0.9808
        "features_count": 43,  # Nombre r√©el de colonnes dans CHAP85_PROCESSED_ADVANCED.csv
        "f1_score": 0.9808,  # Test F1 XGBoost (Validation F1: 0.9808)
        "auc_score": 0.9993,  # Test AUC XGBoost
        "precision": 0.9894,  # Test Precision XGBoost
        "recall": 0.9723,  # Test Recall XGBoost
        "accuracy": 0.9808,  # Test Accuracy
        "validation_f1": 0.9808,  # F1 de validation (crit√®re de s√©lection)
        "fraud_rate": 21.32,  # Taux r√©el de fraude dans les donn√©es
        "data_size": 197402,  # Taille r√©elle des donn√©es
        "leakage_risk": "LOW",
        "validation_status": "ROBUST",
        "optimal_threshold": 0.20,  # Seuil optimal recalcul√© scientifiquement (maximise F1)
        "model_performance": {
            "train_samples": 126336,  # Train set
            "valid_samples": 31585,   # Validation set
            "test_samples": 39481,    # Test set
            "base_rate": 0.2132,      # Taux r√©el de fraude
            "auc_score": 0.9993
        }
    }
}

# -------------------------------
# FONCTIONS DE SEUILS DE D√âCISION
# -------------------------------

def load_decision_thresholds(chapter: str) -> Dict[str, float]:
    """Charger les seuils de d√©cision optimaux pour un chapitre bas√©s sur les performances r√©elles des mod√®les"""
    try:
        from pathlib import Path
        import json
        
        # Charger les seuils optimaux calcul√©s depuis les performances ML
        thresholds_file = Path(__file__).resolve().parents[2] / "results" / chapter / "optimal_thresholds.json"
        if thresholds_file.exists():
            with open(thresholds_file, 'r') as f:
                thresholds = json.load(f)
            logger.info(f"‚úÖ Seuils optimaux charg√©s pour {chapter}: seuil={thresholds.get('optimal_threshold', 'N/A')}")
            return thresholds
        else:
            logger.warning(f"Fichier de seuils non trouv√© pour {chapter}: {thresholds_file}")
            # Utiliser les seuils optimaux des configurations mises √† jour
            config = CHAPTER_CONFIGS.get(chapter, {})
            optimal_threshold = config.get("optimal_threshold", 0.5)
            
            return {
                "conforme": optimal_threshold - 0.1,  # Zone conforme
                "fraude": optimal_threshold + 0.1,    # Zone fraude
                "optimal_threshold": optimal_threshold,
                "model_performance": config.get("model_performance", {})
            }
    except Exception as e:
        logger.error(f"Erreur chargement seuils pour {chapter}: {e}")
        return {
            "conforme": 0.3,  # < 30% : CONFORME
            "fraude": 0.7,    # > 70% : FRAUDE
            "optimal_threshold": 0.5
        }

# Fonction de calibration supprim√©e - les nouveaux mod√®les ML avanc√©s n'ont pas de calibration


def determine_decision(probability: float, chapter: str) -> str:
    """D√©terminer la d√©cision bas√©e sur la probabilit√© et les seuils du chapitre"""
    # Pas de calibration pour les nouveaux mod√®les ML avanc√©s
    if probability is None:
        return "conforme"
    
    thresholds = load_decision_thresholds(chapter)
    
    # ‚úÖ LOGIQUE DE D√âCISION AVEC ZONE GRISE ‚úÖ
    if probability < thresholds["conforme"]:
        return "conforme"
    elif probability > thresholds["fraude"]:
        return "fraude"
    else:
        # Zone grise : utiliser le seuil optimal
        return "fraude" if probability > thresholds["optimal_threshold"] else "conforme"

def debug_single_prediction(data: Dict, chapter: str):
    """Fonction de debug pour tracer une pr√©diction compl√®te"""
    print(f"=== DEBUG PREDICTION {chapter} ===")
    
    # 1. Features d'entr√©e
    print(f"Donn√©es d'entr√©e: {data}")
    
    # 2. Features calcul√©es
    context = create_advanced_context_from_ocr_data(data, chapter)
    print(f"Features calcul√©es: {len(context)}")
    
    # 3. Features business activ√©es
    business_features = {k: v for k, v in context.items() if k.startswith('BUSINESS_') and v == 1}
    print(f"Features business activ√©es: {business_features}")
    
    # 4. Pr√©diction brute
    try:
        model_data = load_ml_model(chapter)
        if model_data and 'model' in model_data:
            import pandas as pd
            df = pd.DataFrame([context])
            raw_prob = model_data['model'].predict_proba(df)[0][1]
            print(f"Probabilit√© brute: {raw_prob}")
            
            # 5. D√©cision finale
            decision = determine_decision(raw_prob, chapter)
            print(f"D√©cision finale: {decision}")
        else:
            print("Mod√®le non trouv√©")
    except Exception as e:
        print(f"Erreur pr√©diction: {e}")


# D√©pendances OCR
try:
    import pytesseract  # type: ignore
    from PIL import Image  # type: ignore
    OCR_AVAILABLE = True
except ImportError:
    pytesseract = None  # type: ignore
    Image = None  # type: ignore
    OCR_AVAILABLE = False

# ML Libraries
import joblib
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import TfidfVectorizer

# Classes n√©cessaires pour charger les mod√®les
class TextColumnTransformer(BaseEstimator, TransformerMixin):
    """Transformateur personnalis√© pour extraire et traiter les colonnes textuelles avec TF-IDF"""
    
    def __init__(self, tfidf_params=None):
        self.tfidf_params = tfidf_params or {}
        self.tfidf = TfidfVectorizer(**self.tfidf_params)
    
    def fit(self, X, y=None):
        text_data = X.iloc[:, 0].astype(str)
        self.tfidf.fit(text_data)
        return self
    
    def transform(self, X):
        text_data = X.iloc[:, 0].astype(str)
        return self.tfidf.transform(text_data).toarray()

class SelectedFeatureWeighter(BaseEstimator, TransformerMixin):
    """Transformateur pour appliquer les poids m√©tier aux features s√©lectionn√©es"""
    
    def __init__(self, selected_features, feature_weights):
        self.selected_features = selected_features
        self.feature_weights = feature_weights
        self.feature_indices = None
    
    def fit(self, X, y=None):
        self.feature_indices = {}
        for i, feature_name in enumerate(self.selected_features):
            if feature_name in self.feature_weights:
                self.feature_indices[i] = self.feature_weights[feature_name]
        return self
    
    def transform(self, X):
        X_weighted = X.copy()
        for idx, weight in self.feature_indices.items():
            X_weighted[:, idx] *= weight
        return X_weighted

# Ingestion (PDF -> images + manifest)
try:
    from .ocr_ingest import ingest, DATA, MANIFEST
except ImportError:
    ingest = None
    DATA = None
    MANIFEST = None

# -------------------------------
# IMPORT DU MAPPING DEPUIS OCR_INGEST
# -------------------------------

# Utiliser le mapping depuis OCR_INGEST pour √©viter la redondance
from .ocr_ingest import FIELD_MAPPING, create_advanced_context_from_ocr_data, OCRDataContract

# -------------------------------
# CONFIGURATION DES CHAPITRES AVEC ANALYSES POURS√âES
# -------------------------------

# CONFIGURATION OBSOL√àTE SUPPRIM√âE - Utiliser celle en haut du fichier

# -------------------------------
# UTILITAIRES
# -------------------------------

def safe_mode(x, fallback="UNKNOWN"):
    """Mode robuste avec fallback"""
    try:
        m = x.mode(dropna=True)
        if m.empty:
            return fallback
        return m.iloc[0]
    except Exception:
        return fallback

def safe_first(x, fallback=None):
    """Premi√®re valeur non-nulle avec fallback"""
    try:
        for v in x:
            if pd.notna(v):
                return v
        return fallback
    except Exception:
        return fallback

def extract_chapter_from_code_sh(code_sh: str) -> str:
    """Extraire le chapitre √† partir du code SH"""
    if not code_sh or pd.isna(code_sh):
        return "unknown"
    
    code_sh_str = str(code_sh).strip()
    if len(code_sh_str) >= 2:
        chapter = code_sh_str[:2]
        if chapter == "30":
            return "chap30"
        elif chapter == "84":
            return "chap84"
        elif chapter == "85":
            return "chap85"
    
    return "unknown"

def get_chap30_selected_features():
    """Retourner exactement les m√™mes features utilis√©es dans ml_model_advanced.py du chapitre 30"""
    return [
        # Features num√©riques (9)
        'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET',
        'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
        'NUMERO_ARTICLE', 'PRECISION_UEMOA',
        # Features de d√©tection de fraude (10)
        'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE', 
        'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 
        'HIERARCHICAL_CLUSTER_SCORE', 'ADMIN_VALUES_SCORE', 
        'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR',
        # Features business (18) - CHAPITRE 30 (Produits pharmaceutiques)
        'BUSINESS_GLISSEMENT_TARIFAIRE', 'BUSINESS_GLISSEMENT_DESCRIPTION',
        'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
        'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
        'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE',
        'BUSINESS_VALEUR_EXCEPTIONNELLE', 'BUSINESS_POIDS_ELEVE',
        'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
        'BUSINESS_RATIO_DOUANE_CAF',
        'BUSINESS_IS_ANTIPALUDEEN',
        'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI',
        'BUSINESS_VALEUR_UNITAIRE_SUSPECTE',
        # Features cat√©gorielles (6)
        'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR',
        'BUREAU', 'REGIME_FISCAL', 'NUMERO_DPI'
    ]

def get_chap84_selected_features():
    """Retourner exactement les m√™mes features utilis√©es dans ml_model_advanced.py du chapitre 84"""
    return [
        # Features num√©riques (9)
        'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET',
        'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
        'NUMERO_ARTICLE', 'PRECISION_UEMOA',
        # Features de d√©tection de fraude (10)
        'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE', 
        'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 
        'HIERARCHICAL_CLUSTER_SCORE', 'ADMIN_VALUES_SCORE', 
        'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR',
        # Features business (18) - CHAPITRE 84 (Machines et √©quipements)
        'BUSINESS_GLISSEMENT_MACHINE', 'BUSINESS_GLISSEMENT_PAYS_MACHINES',
        'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
        'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
        'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE',
        'BUSINESS_VALEUR_EXCEPTIONNELLE', 'BUSINESS_POIDS_ELEVE',
        'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
        'BUSINESS_RATIO_DOUANE_CAF', 'BUSINESS_IS_MACHINE',
        'BUSINESS_IS_ELECTRONIQUE', 'BUSINESS_IS_PRECISION_UEMOA',
        'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI',
        # Features cat√©gorielles (6)
        'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR',
        'BUREAU', 'REGIME_FISCAL', 'NUMERO_DPI'
    ]

def get_chap85_selected_features():
    """Retourner exactement les m√™mes features utilis√©es dans ml_model_advanced.py du chapitre 85"""
    return [
        # Features num√©riques (9)
        'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET',
        'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
        'NUMERO_ARTICLE', 'PRECISION_UEMOA',
        # Features de d√©tection de fraude (10)
        'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE', 
        'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 
        'HIERARCHICAL_CLUSTER_SCORE', 'ADMIN_VALUES_SCORE', 
        'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR',
        # Features business (18) - CHAPITRE 85 (Appareils √©lectriques)
        'BUSINESS_GLISSEMENT_ELECTRONIQUE', 'BUSINESS_GLISSEMENT_PAYS_ELECTRONIQUES',
        'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
        'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
        'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE',
        'BUSINESS_VALEUR_EXCEPTIONNELLE', 'BUSINESS_POIDS_FAIBLE',
        'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
        'BUSINESS_RATIO_DOUANE_CAF', 'BUSINESS_IS_ELECTRONIQUE',
        'BUSINESS_IS_TELEPHONE', 'BUSINESS_IS_PRECISION_UEMOA',
        'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI',
        # Features cat√©gorielles (6)
        'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR',
        'BUREAU', 'REGIME_FISCAL', 'NUMERO_DPI'
    ]

# FONCTION SUPPRIM√âE - Utiliser create_advanced_context_from_ocr_data depuis OCR_INGEST
# Cette fonction √©tait redondante avec celle d'OCR_INGEST
    
    # G√©rer le DECLARATION_ID correctement avec agr√©gation par ANNEE/BUREAU/NUMERO
# Fonction obsol√®te supprim√©e - utilise maintenant calculate_business_features qui d√©l√®gue √† ocr_ingest.py

# Fonction obsol√®te supprim√©e - utilise maintenant _create_chapter_specific_business_features d'ocr_ingest.py

def calculate_dynamic_thresholds(chapter: str) -> Dict[str, float]:
    """
    Charger les seuils optimaux depuis results/{chapter}/optimal_thresholds.json
    CES FICHIERS SONT OBLIGATOIRES - PAS DE FALLBACK
    """
    try:
        # Charger les seuils optimaux depuis le fichier JSON sp√©cifique au chapitre
        backend_root = Path(__file__).resolve().parents[2]
        thresholds_file = backend_root / "results" / chapter / "optimal_thresholds.json"
        
        if not thresholds_file.exists():
            logger.error(f"‚ùå ERREUR CRITIQUE: Fichier optimal_thresholds.json introuvable pour {chapter}")
            logger.error(f"‚ùå Chemin recherch√©: {thresholds_file}")
            raise FileNotFoundError(f"Fichier optimal_thresholds.json introuvable pour {chapter}. Chemin: {thresholds_file}")
        
        with open(thresholds_file, 'r') as f:
            chapter_thresholds = json.load(f)
        
        logger.info(f"‚úÖ Seuils optimaux charg√©s pour {chapter} depuis {thresholds_file}")
        logger.info(f"   - Seuil optimal: {chapter_thresholds.get('optimal_threshold', 'N/A')}")
        logger.info(f"   - Zone conforme: < {chapter_thresholds.get('conforme', 'N/A')}")
        logger.info(f"   - Zone fraude: > {chapter_thresholds.get('fraude', 'N/A')}")
        
        return chapter_thresholds
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: Impossible de charger les seuils pour {chapter}: {e}")
        raise ValueError(f"Seuils optimaux introuvables pour {chapter}. Fichier optimal_thresholds.json manquant.")

# FONCTION SUPPRIM√âE - Utiliser UNIQUEMENT optimal_thresholds.json

# FONCTION SUPPRIM√âE - Pas de seuils par d√©faut, OBLIGATOIRE d'avoir optimal_thresholds.json


def calculate_business_features(context: Dict[str, Any], chapter: str, ocr_data: Dict = None) -> Dict[str, Any]:
    """
    Calculer les features business en utilisant la fonction d'ocr_ingest.py
    Cette fonction d√©l√®gue le calcul des features business √† ocr_ingest.py qui contient
    les vraies features utilis√©es par les mod√®les ML.
    """
    try:
        # Utiliser la fonction d'ocr_ingest.py qui contient les vraies features
        from .ocr_ingest import _create_chapter_specific_business_features
        business_features = _create_chapter_specific_business_features(context, chapter)
        
        # Ajouter les features business au contexte
        context.update(business_features)
        
        return context
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: Impossible de calculer les features business pour {chapter}: {e}")
        # PAS DE FALLBACK! Les features business sont OBLIGATOIRES pour le ML
        raise ValueError(f"Calcul des features business √©chou√© pour {chapter}. Pr√©diction impossible.")

def analyze_fraud_risk_patterns(context: Dict[str, Any], chapter: str) -> Dict[str, Any]:
    """Analyser les patterns de risque de fraude bas√©s sur les analyses pouss√©es"""
    config = CHAPTER_CONFIGS.get(chapter, CHAPTER_CONFIGS["chap30"])
    fraud_patterns = config.get("fraud_patterns", {})
    
    risk_analysis = {
        "risk_score": 0.0,
        "risk_factors": [],
        "suspicious_indicators": [],
        "confidence_level": "low"
    }
    
    # V√©rifier les codes SH suspects
    code_sh = str(context.get('CODE_SH_COMPLET', ''))
    suspicious_sh_codes = fraud_patterns.get("suspicious_sh_codes", [])
    if code_sh in suspicious_sh_codes:
        risk_analysis["risk_score"] += 0.3
        risk_analysis["risk_factors"].append(f"Code SH suspect: {code_sh}")
        risk_analysis["suspicious_indicators"].append("suspicious_sh_code")
    
    # V√©rifier les pays √† risque
    pays_origine = str(context.get('CODE_PAYS_ORIGINE', '')).upper()
    risky_countries = fraud_patterns.get("risky_countries", [])
    if pays_origine in risky_countries:
        risk_analysis["risk_score"] += 0.2
        risk_analysis["risk_factors"].append(f"Pays √† risque: {pays_origine}")
        risk_analysis["suspicious_indicators"].append("risky_country")
    
    # V√©rifier les valeurs anormales
    valeur_caf = context.get('VALEUR_CAF', 0)
    poids_net = context.get('POIDS_NET_KG', 0)
    
    if poids_net > 0 and valeur_caf > 0:
        valeur_unitaire = valeur_caf / poids_net
        seuil_min = fraud_patterns.get("min_unit_value", 50)
        seuil_max = fraud_patterns.get("max_unit_value", 5000)
        
        if valeur_unitaire < seuil_min:
            risk_analysis["risk_score"] += 0.4
            risk_analysis["risk_factors"].append(f"Valeur unitaire tr√®s faible: {valeur_unitaire:.2f} CFA/kg")
            risk_analysis["suspicious_indicators"].append("low_unit_value")
        elif valeur_unitaire > seuil_max:
            risk_analysis["risk_score"] += 0.3
            risk_analysis["risk_factors"].append(f"Valeur unitaire tr√®s √©lev√©e: {valeur_unitaire:.2f} CFA/kg")
            risk_analysis["suspicious_indicators"].append("high_unit_value")
    
    # D√©terminer le niveau de confiance
    if risk_analysis["risk_score"] >= 0.7:
        risk_analysis["confidence_level"] = "high"
    elif risk_analysis["risk_score"] >= 0.4:
        risk_analysis["confidence_level"] = "medium"
    
    return risk_analysis

# -------------------------------

def load_ml_model(chapter: str) -> Optional[Any]:
    """Charger le meilleur mod√®le ML calibr√© pour un chapitre avec scalers et encoders"""
    # FORCER LE RECHARGEMENT POUR TOUS LES CHAPITRES
    with _CACHE_LOCK:
        if chapter in _MODEL_CACHE:
            del _MODEL_CACHE[chapter]
            logger.info(f"Cache vid√© pour {chapter} - rechargement forc√©")
    
    with _CACHE_LOCK:
        if chapter in _MODEL_CACHE:
            return _MODEL_CACHE[chapter]
    
    try:
        config = CHAPTER_CONFIGS.get(chapter)
        if not config:
            logger.error(f"‚ùå ERREUR CRITIQUE: Chapitre {chapter} non configur√©")
            raise ValueError(f"Chapitre {chapter} non configur√©. Chapitres valides: {list(CHAPTER_CONFIGS.keys())}")
        
        best_model = config["best_model"]
        
        # Utiliser UNIQUEMENT les nouveaux mod√®les ML avanc√©s depuis results/{chapter}/models
        models_dir = Path(__file__).resolve().parents[2] / "results" / chapter / "models"
        model_path = models_dir / f"{best_model}_model.pkl"
        
        # PAS DE FALLBACK vers l'ancien r√©pertoire! Seuls les mod√®les avanc√©s sont accept√©s
        logger.info(f"Chargement du mod√®le ML avanc√© pour {chapter}: {model_path}")
        
        if not model_path.exists():
            logger.error(f"‚ùå ERREUR CRITIQUE: Mod√®le ML introuvable pour {chapter}: {model_path}")
            raise FileNotFoundError(f"Mod√®le ML introuvable pour {chapter}. Fichier: {model_path}")
        
        # Charger le mod√®le ML avanc√©
        model = joblib.load(model_path)
        
        # V√©rifier le type de mod√®le
        model_type = str(type(model))
        logger.info(f"Type de mod√®le pour {chapter}: {model_type}")
        
        # Charger les scalers et encoders si disponibles
        scalers_path = models_dir / "scalers.pkl"
        encoders_path = models_dir / "encoders.pkl"
        features_path = models_dir / "features.pkl"
        preprocessing_path = models_dir / "preprocessing_pipeline.pkl"
        
        model_data = {
            'model': model,
            'scalers': None,
            'encoders': None,
            'features': None,
            'preprocessing_pipeline': None,
            'model_performance': config.get('model_performance', {}),
            'chapter_info': {
                'name': config.get('name', ''),
                'best_model': best_model,
                'f1_score': config.get('f1_score', 0.0),
                'auc_score': config.get('auc_score', 0.0),
                'fraud_rate': config.get('fraud_rate', 0.0),
                'data_size': config.get('data_size', 0)
            }
        }
        
        if scalers_path.exists():
            model_data['scalers'] = joblib.load(scalers_path)
            logger.info(f"   ‚úÖ Scalers charg√©s pour {chapter}")
        
        if encoders_path.exists():
            model_data['encoders'] = joblib.load(encoders_path)
            logger.info(f"   ‚úÖ Encoders charg√©s pour {chapter}")
        
        if features_path.exists():
            model_data['features'] = joblib.load(features_path)
            model_data['feature_names'] = model_data['features']  # Alias pour compatibilit√©
            logger.info(f"   ‚úÖ Features charg√©es pour {chapter}")
        
        if preprocessing_path.exists():
            model_data['preprocessing_pipeline'] = joblib.load(preprocessing_path)
            logger.info(f"   ‚úÖ Pipeline de preprocessing charg√© pour {chapter}")
        
        with _CACHE_LOCK:
            _MODEL_CACHE[chapter] = model_data
        
        logger.info(f"‚úÖ Mod√®le ML avanc√© charg√© pour {chapter}: {best_model} ({model_type})")
        logger.info(f"   üéØ Performance: F1={config.get('f1_score', 0.0):.3f}, AUC={config.get('auc_score', 0.0):.3f}")
        logger.info(f"   üìä Seuil optimal: {config.get('optimal_threshold', 0.5):.3f}")
        return model_data
            
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: Impossible de charger le mod√®le ML pour {chapter}: {e}")
        raise ValueError(f"Chargement du mod√®le ML √©chou√© pour {chapter}: {e}")

# -------------------------------
# CHARGEMENT DES MOD√àLES RL
# -------------------------------

def load_rl_manager(chapter: str, level: str = "basic") -> Optional[Any]:
    """Charger le manager RL pour un chapitre et niveau"""
    cache_key = f"{chapter}_{level}"
    
    with _RL_CACHE_LOCK:
        if cache_key in _RL_CACHE:
            return _RL_CACHE[cache_key]
    
    try:
        # Import dynamique des managers RL
        import importlib.util
        
        if chapter == "chap30":
            spec = importlib.util.spec_from_file_location(
                "rl_integration", 
                Path(__file__).resolve().parents[1] / "chapters" / "chap30" / "rl_integration.py"
            )
        elif chapter == "chap84":
            spec = importlib.util.spec_from_file_location(
                "rl_integration", 
                Path(__file__).resolve().parents[1] / "chapters" / "chap84" / "rl_integration.py"
            )
        elif chapter == "chap85":
            spec = importlib.util.spec_from_file_location(
                "rl_integration", 
                Path(__file__).resolve().parents[1] / "chapters" / "chap85" / "rl_integration.py"
            )
        else:
            logger.error(f"‚ùå ERREUR CRITIQUE: Chapitre {chapter} non support√© pour RL")
            raise ValueError(f"Chapitre {chapter} non support√©. Chapitres valides: chap30, chap84, chap85")
        
        if spec is None:
            logger.error(f"‚ùå ERREUR CRITIQUE: Module RL non trouv√© pour {chapter}")
            raise FileNotFoundError(f"Module RL non trouv√© pour {chapter}")
            
        rl_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(rl_module)
        get_manager = rl_module.get_manager
        
        manager = get_manager(level)
        
        with _RL_CACHE_LOCK:
            _RL_CACHE[cache_key] = manager
        
        logger.info(f"‚úÖ Manager RL charg√© pour {chapter} niveau {level}")
        return manager
                
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: Impossible de charger le manager RL pour {chapter}: {e}")
        raise ValueError(f"Chargement du manager RL √©chou√© pour {chapter}: {e}")

# -------------------------------
# PIPELINE OCR PRINCIPAL
# -------------------------------

class AdvancedOCRPipeline:
    """Pipeline OCR avanc√© avec int√©gration ML-RL pour les 3 chapitres"""
    
    def __init__(self):
        self.backend_root = Path(__file__).resolve().parents[2]
        self.logger = logging.getLogger(__name__)
        
    # M√âTHODES SUPPRIM√âES - Utiliser process_image_declaration depuis OCR_INGEST
    # Ces m√©thodes √©taient redondantes avec celles d'OCR_INGEST
    def predict_fraud(self, ocr_data: Dict[str, Any], chapter: str = None, level: str = "basic") -> Dict[str, Any]:
        """Pr√©dire la fraude avec le syst√®me ML-RL int√©gr√©"""
        try:
            # D√©terminer le chapitre si non fourni
            if not chapter:
                chapter = extract_chapter_from_code_sh(ocr_data.get('code_sh_complet', ''))
            
            if chapter == "unknown":
                self.logger.warning("Chapitre inconnu, utilisation du chapitre 30 par d√©faut")
                chapter = "chap30"
            
            # Cr√©er le contexte complet
            context = create_advanced_context_from_ocr_data(ocr_data, chapter)
            
            # Charger le mod√®le ML
            ml_model_data = load_ml_model(chapter)
            
            # Charger le manager RL - OBLIGATOIRE, pas de fallback
            rl_manager = load_rl_manager(chapter, level)
            
            # load_rl_manager() l√®ve maintenant une exception si √©chec - pas de v√©rification None n√©cessaire
            
            # Utiliser directement le meilleur mod√®le ML d√©j√† entra√Æn√© (pas de recalcul)
            ml_probability = None
            decision = "unknown"  # Initialiser la d√©cision par d√©faut
            
            self.logger.info(f"üîç ml_model_data: {ml_model_data is not None}")
            if ml_model_data:
                self.logger.info(f"üîç Mod√®le ML disponible: {ml_model_data.get('model') is not None}")
            else:
                self.logger.warning(f"‚ö†Ô∏è ml_model_data est None pour {chapter}")
            
            if ml_model_data:
                # Utiliser directement ml_model_data (pas besoin d'extraire)
                scalers = ml_model_data.get('scalers')
                encoders = ml_model_data.get('encoders')
                features = ml_model_data.get('features')
                # Supprimer les warnings de dtype pour les mod√®les LightGBM/CatBoost
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message="The dtype of the filling value")
                    warnings.filterwarnings("ignore", message="X does not have valid feature names")
                    warnings.filterwarnings("ignore", category=UserWarning)
                    try:
                        # SOLUTION SIMPLIFI√âE: Utiliser directement le pipeline scikit-learn
                        if not ml_model_data or not ml_model_data.get('model'):
                            self.logger.error(f"‚ùå ERREUR CRITIQUE: Impossible de charger le mod√®le ML pour {chapter}")
                            raise ValueError(f"Mod√®le ML non disponible pour {chapter}. V√©rifier load_ml_model().")
                        
                        # Le pipeline scikit-learn g√®re automatiquement l'encodage et la normalisation
                        pipeline = ml_model_data.get('model')
                        
                        self.logger.info(f"‚úÖ Pipeline scikit-learn charg√© pour {chapter}")
                        self.logger.info(f"üîç Type de pipeline: {type(pipeline)}")
                        
                        # Cr√©er le DataFrame avec TOUTES les features du contexte
                        # Le pipeline scikit-learn g√®re automatiquement le preprocessing
                        df = pd.DataFrame([context])
                        self.logger.info(f"üîç DataFrame cr√©√©: {df.shape}")
                        self.logger.info(f"üîç Colonnes disponibles: {len(df.columns)}")
                        
                        # Le pipeline scikit-learn g√®re automatiquement tout le preprocessing
                        self.logger.info(f"‚úÖ Preprocessing g√©r√© automatiquement par scikit-learn")
                        
                        # Pr√©diction avec le pipeline scikit-learn complet
                        try:
                            # Debug: afficher quelques features importantes
                            self.logger.info(f"üîç Features importantes:")
                            self.logger.info(f"   VALEUR_CAF: {context.get('VALEUR_CAF', 0)}")
                            self.logger.info(f"   POIDS_NET_KG: {context.get('POIDS_NET_KG', 0)}")
                            self.logger.info(f"   PAYS_ORIGINE_STR: {context.get('PAYS_ORIGINE_STR', '')}")
                            self.logger.info(f"   CODE_PRODUIT_STR: {context.get('CODE_PRODUIT_STR', '')}")
                            
                            # Compter les features business actives
                            business_features = [k for k in context.keys() if k.startswith('BUSINESS_')]
                            active_business_features = sum(1 for k in business_features if context.get(k, 0) > 0)
                            self.logger.info(f"   Features business actives: {active_business_features}/{len(business_features)}")
                            
                            # Compter les fraud detection features
                            fraud_features = [k for k in context.keys() if 'SCORE' in k or k in ['MIRROR_TEI_SCORE', 'ADMIN_VALUES_SCORE', 'SPECTRAL_CLUSTER_SCORE', 'HIERARCHICAL_CLUSTER_SCORE', 'BIENAYME_CHEBYCHEV_SCORE', 'COMPOSITE_FRAUD_SCORE']]
                            active_fraud_features = sum(1 for k in fraud_features if context.get(k, 0) != 0.0)
                            self.logger.info(f"   Fraud detection features actives: {active_fraud_features}/{len(fraud_features)}")
                            
                            # Afficher les valeurs des fraud features
                            for fraud_feature in fraud_features:
                                value = context.get(fraud_feature, 0.0)
                                if value != 0.0:
                                    self.logger.info(f"   {fraud_feature}: {value:.6f}")
                            
                            ml_probability = float(pipeline.predict_proba(df)[0][1])
                            self.logger.info(f"‚úÖ Pr√©diction ML R√âELLE r√©ussie: {ml_probability:.3f}")
                            self.logger.info(f"‚úÖ Mod√®le utilis√©: {ml_model_data.get('best_model', 'Inconnu')}")
                        except Exception as e:
                            self.logger.error(f"‚ùå ERREUR CRITIQUE: Le mod√®le ML n'a pas pu pr√©dire: {e}")
                            self.logger.error(f"‚ùå D√©tails: {str(e)}")
                            # PLUS DE FALLBACK BIDON! Si le ML √©choue, on retourne une erreur
                            raise ValueError(f"Le mod√®le ML pour {chapter} a √©chou√©. V√©rifier le mod√®le et les features.")
                        
                        # V√©rifier que la probabilit√© est dans une plage raisonnable
                        if ml_probability < 0.0 or ml_probability > 1.0:
                            self.logger.warning(f"Probabilit√© ML anormale pour {chapter}: {ml_probability}")
                            ml_probability = max(0.0, min(1.0, ml_probability))
                        
                        self.logger.info(f"Pr√©diction ML avanc√©e pour {chapter}: {ml_probability:.3f}")
                        
                        # D√©terminer la d√©cision bas√©e sur les seuils
                        decision = determine_decision(ml_probability, chapter)
                        self.logger.info(f"D√©cision pour {chapter}: {decision} (probabilit√©: {ml_probability:.3f})")
                    except Exception as e:
                        self.logger.warning(f"Erreur pr√©diction ML pour {chapter}: {e}")
                        # Pas de recalcul, on continue avec RL seul
            
            # Pr√©diction RL avec int√©gration ML
            rl_result = rl_manager.predict(
                context,
                ml_probability=ml_probability,
                threshold=0.5
            )
            
            # Analyser les patterns de risque de fraude
            risk_analysis = analyze_fraud_risk_patterns(context, chapter)
            
            # Enrichir le r√©sultat avec les analyses pouss√©es
            # Pas de calibration pour les nouveaux mod√®les ML avanc√©s
            if ml_probability is None:
                logger.error("‚ùå ERREUR CRITIQUE: ml_probability est None - Le mod√®le ML n'a pas pu pr√©dire")
                raise ValueError(f"La pr√©diction ML a √©chou√© pour {chapter}. ml_probability est None.")
            final_probability = ml_probability
            
            # R√©cup√©rer les m√©triques de performance (sans calibration)
            model_performance = ml_model_data.get('model_performance', {}) if ml_model_data else {}
            chapter_info = ml_model_data.get('chapter_info', {}) if ml_model_data else {}
            
            result = {
                "chapter": chapter,
                "level": level,
                "declaration_id": context.get('DECLARATION_ID', ''),
                "predicted_fraud": decision == "fraude",
                "fraud_probability": final_probability,  # Probabilit√© directe du ML (sans calibration)
                "ml_probability_raw": ml_probability,  # Probabilit√© brute du ML
                "confidence_score": rl_result.get("confidence_score", 0.0),
                "exploration_used": rl_result.get("exploration_used", False),
                "decision_source": rl_result.get("decision_source", "unknown"),
                "model_used": chapter_info.get('best_model', 'unknown'),
                "optimal_threshold_used": load_decision_thresholds(chapter).get('optimal_threshold', 0.5) if load_decision_thresholds(chapter) else 0.5,
                "ml_probability": final_probability,  # Pour compatibilit√©
                "decision": decision,
                "ml_integration_used": rl_result.get("ml_integration_used", False),
                "context_complexity": rl_result.get("context_complexity", 1),
                "seasonal_factor": rl_result.get("seasonal_factor", 1.0),
                "bureau_risk_score": rl_result.get("bureau_risk_score", 0.0),
                "strategy_info": rl_result.get("strategy_info", {}),
                # Analyses pouss√©es
                "risk_analysis": risk_analysis,
                "context": context,  # Inclure le contexte complet
                # M√©triques de performance (sans calibration)
                "performance_metrics": {
                    "validation_status": "ROBUST",
                    "leakage_risk": "LOW"
                },
                "model_performance": {
                    "train_samples": model_performance.get('train_samples', 0),
                    "valid_samples": model_performance.get('valid_samples', 0),
                    "test_samples": model_performance.get('test_samples', 0),
                    "base_rate": model_performance.get('base_rate', 0.0),
                    "auc_score": model_performance.get('auc_score', 0.0),
                    "auc": chapter_info.get('auc_score', 0.0),
                    "f1": chapter_info.get('f1_score', 0.0),
                    "precision": model_performance.get('precision', 0.0),
                    "recall": model_performance.get('recall', 0.0)
                },
                "chapter_info": {
                    "name": chapter_info.get('name', CHAPTER_CONFIGS[chapter]["name"]),
                    "best_model": chapter_info.get('best_model', CHAPTER_CONFIGS[chapter]["best_model"]),
                    "f1_score": chapter_info.get('f1_score', CHAPTER_CONFIGS[chapter].get("f1_score", 0.0)),
                    "auc_score": chapter_info.get('auc_score', CHAPTER_CONFIGS[chapter].get("auc_score", 0.0)),
                    "fraud_rate": chapter_info.get('fraud_rate', CHAPTER_CONFIGS[chapter].get("fraud_rate", 0)),
                    "data_size": chapter_info.get('data_size', CHAPTER_CONFIGS[chapter].get("data_size", 0))
                },
                "timestamp": datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erreur pr√©diction fraude: {e}")
            return {
                "error": str(e),
                "chapter": chapter or "unknown",
                "level": level
            }
    
    def process_document(self, image_path: str, chapter: str = None, level: str = "basic") -> Dict[str, Any]:
        """Traiter un document complet avec OCR et pr√©diction de fraude"""
        try:
            # 1. Extraction OCR
            text = self.extract_text_from_image(image_path)
            if not text:
                return {
                    "error": "Aucun texte extrait de l'image",
                    "image_path": image_path
                }
            
            # 2. Parsing des donn√©es
            ocr_data = self.parse_ocr_text(text)
            
            # 3. Pr√©diction de fraude
            prediction = self.predict_fraud(ocr_data, chapter, level)
            
            # 4. R√©sultat complet
            result = {
                "image_path": image_path,
                "ocr_text": text,
                "parsed_data": ocr_data,
                "prediction": prediction,
                "processing_time": datetime.now().isoformat()
            }
            
            return result
                    
        except Exception as e:
            self.logger.error(f"Erreur traitement document: {e}")
            return {
                "error": str(e),
                "image_path": image_path
            }
    
    def _load_features_from_ml_model(self, chapter: str) -> List[str]:
        """Charger les features directement depuis les classes ML_MODEL"""
        try:
            if chapter == "chap30":
                from ..chapters.chap30.ml_model_advanced import Chap30MLAdvanced
                ml_model = Chap30MLAdvanced()
                feature_dict = ml_model._get_feature_columns()
            elif chapter == "chap84":
                from ..chapters.chap84.ml_model_advanced import Chap84MLAdvanced
                ml_model = Chap84MLAdvanced()
                feature_dict = ml_model._get_feature_columns()
            elif chapter == "chap85":
                from ..chapters.chap85.ml_model_advanced import Chap85MLAdvanced
                ml_model = Chap85MLAdvanced()
                feature_dict = ml_model._get_feature_columns()
            else:
                return []
            
            # Convertir le dictionnaire en liste plate de toutes les features
            all_features = []
            for category, features in feature_dict.items():
                all_features.extend(features)
            
            return all_features
                
        except Exception as e:
            self.logger.warning(f"Erreur chargement features ML_MODEL pour {chapter}: {e}")
            return []

    def _load_features_from_shap_report(self, chapter: str) -> List[str]:
        """Charger les vraies features depuis le rapport SHAP et les mapper aux indices num√©riques"""
        try:
            import json
            from pathlib import Path
            
            shap_report_path = Path(__file__).resolve().parents[2] / "results" / chapter / "shap_analysis" / "comprehensive_shap_report_real_names.json"
            
            if shap_report_path.exists():
                with open(shap_report_path, 'r') as f:
                    shap_data = json.load(f)
                
                if 'feature_names' in shap_data:
                    all_features = shap_data['feature_names']
                    
                    # Exclure seulement les champs vraiment non-num√©riques (les features cat√©gorielles sont converties en num√©riques)
                    excluded_fields = {
                        'DECLARATION_ID', 'ANNEE', 'NUMERO', 'NUMERO_DPI', 'DESIGNATION_COMMERCIALE', 'CATEGORIE_PRODUIT'
                    }
                    
                    # Cr√©er la liste des features filtr√©es dans le m√™me ordre que les indices num√©riques
                    filtered_features = []
                    for feature in all_features:
                        if feature not in excluded_fields:
                            filtered_features.append(feature)
                    
                    self.logger.info(f"Features SHAP filtr√©es pour {chapter}: {len(filtered_features)} features")
                    return filtered_features
            
            return []
            
        except Exception as e:
            self.logger.warning(f"Erreur chargement features SHAP pour {chapter}: {e}")
            return []

    def process_csv_with_aggregation(self, csv_data: List[Dict[str, Any]], chapter: str = None, level: str = "basic") -> Dict[str, Any]:
        """Traiter des donn√©es CSV avec agr√©gation par DECLARATION_ID"""
        try:
            if not csv_data:
                return {"error": "Aucune donn√©e CSV fournie"}
            
            # Agr√©gation par DECLARATION_ID
            aggregated_data = {}
            
            for row in csv_data:
                # Construire le DECLARATION_ID pour chaque ligne
                declaration_id = row.get('DECLARATION_ID', '')
                if not declaration_id:
                    # Construire √† partir des composants
                    annee = row.get('ANNEE', '')
                    bureau = row.get('BUREAU', '')
                    numero = row.get('NUMERO', '')
                    
                    if numero:
                        if '/' in str(numero):
                            declaration_id = str(numero)
                        elif annee and bureau:
                            declaration_id = f"{annee}/{bureau}/{numero}"
                        else:
                            declaration_id = str(numero)
                    elif annee and bureau:
                        declaration_id = f"{annee}/{bureau}"
                    elif annee:
                        declaration_id = str(annee)
                    else:
                        declaration_id = f"ROW_{len(aggregated_data)}"
                
                # Agr√©gation des donn√©es num√©riques
                if declaration_id not in aggregated_data:
                    aggregated_data[declaration_id] = {
                        'DECLARATION_ID': declaration_id,
                        'ANNEE': row.get('ANNEE', ''),
                        'BUREAU': row.get('BUREAU', ''),
                        'NUMERO': row.get('NUMERO', ''),
                        'POIDS_NET_KG': 0.0,
                        'NOMBRE_COLIS': 0,
                        'QUANTITE_COMPLEMENT': 0.0,
                        'VALEUR_CAF': 0.0,
                        'VALEUR_DOUANE': 0.0,
                        'MONTANT_LIQUIDATION': 0.0,
                        'TAUX_DROITS_PERCENT': 0.0,
                        'count': 0
                    }
                
                # Somme des valeurs num√©riques
                agg_row = aggregated_data[declaration_id]
                agg_row['POIDS_NET_KG'] += float(row.get('POIDS_NET', 0))  # CORRECTION: POIDS_NET au lieu de POIDS_NET_KG
                agg_row['NOMBRE_COLIS'] += int(row.get('NOMBRE_COLIS', 0))
                agg_row['QUANTITE_COMPLEMENT'] += float(row.get('QTTE_COMPLEMENTAIRE', 0))  # CORRECTION: QTTE_COMPLEMENTAIRE
                agg_row['VALEUR_CAF'] += float(row.get('VALEUR_CAF', 0))
                agg_row['VALEUR_DOUANE'] += float(row.get('VALEUR_DOUANE', 0))
                agg_row['MONTANT_LIQUIDATION'] += float(row.get('MONTANT_LIQUIDATION', 0))
                agg_row['count'] += 1
                
                # Prendre la premi√®re valeur non-num√©rique pour les autres champs
                # CORRECTION: Utiliser les bonnes cl√©s du CSV
                field_mapping = {
                    'CODE_SH_COMPLET': 'NOMENCLATURE_COMPLETE',
                    'CODE_PAYS_ORIGINE': 'PAYS_ORIGINE', 
                    'CODE_PAYS_PROVENANCE': 'PAYS_PROVENANCE',
                    'REGIME_COMPLET': 'REGIME',
                    'STATUT_BAE': 'STATUT_BAE',
                    'TYPE_REGIME': 'TYPE_REGIME',
                    'REGIME_DOUANIER': 'REGIME_DOUANIER',
                    'REGIME_FISCAL': 'REGIME_FISCAL',
                    'DESIGNATION_COMMERCIALE': 'DESIGNATION_COMMERCIALE'
                }
                
                for field, csv_key in field_mapping.items():
                    if field not in agg_row or not agg_row[field]:
                        agg_row[field] = row.get(csv_key, '')
            
            # Prendre la premi√®re d√©claration agr√©g√©e pour la pr√©diction
            first_declaration_id = list(aggregated_data.keys())[0]
            first_declaration = aggregated_data[first_declaration_id]
            
            # Cr√©er le contexte pour la pr√©diction
            context = create_advanced_context_from_ocr_data(first_declaration, chapter)
            
            # Pr√©diction de fraude
            prediction = self.predict_fraud(first_declaration, chapter, level)
            
            # R√©sultat complet
            result = {
                "csv_data": csv_data,
                "aggregated_data": aggregated_data,
                "total_declarations": len(csv_data),
                "unique_declaration_ids": len(aggregated_data),
                "processed_declaration": first_declaration,
                "context": context,
                "prediction": prediction,
                "processing_time": datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erreur traitement CSV avec agr√©gation: {e}")
            return {
                "error": str(e),
                "csv_data": csv_data
            }

# -------------------------------
# FONCTIONS D'INTERFACE
# -------------------------------

def process_ocr_document(image_path: str, chapter: str = None, level: str = "basic") -> Dict[str, Any]:
    """Interface principale pour traiter un document OCR"""
    pipeline = AdvancedOCRPipeline()
    return pipeline.process_document(image_path, chapter, level)

def predict_fraud_from_ocr_data(ocr_data: Dict[str, Any], chapter: str = None, level: str = "basic") -> Dict[str, Any]:
    """Interface pour pr√©dire la fraude √† partir de donn√©es OCR"""
    pipeline = AdvancedOCRPipeline()
    return pipeline.predict_fraud(ocr_data, chapter, level)

def test_aggregation_and_features(chapter: str = "chap30") -> Dict[str, Any]:
    """Tester l'agr√©gation par DECLARATION_ID et l'alignement des features"""
    try:
        # Donn√©es de test avec diff√©rents formats de DECLARATION_ID
        test_data = [
            {
                'DECLARATION_ID': '2023/ABJ/001',
                'ANNEE': '2023',
                'BUREAU': 'ABJ',
                'NUMERO': '001',
                'POIDS_NET_KG': 100.0,
                'VALEUR_CAF': 500000.0,
                'CODE_SH_COMPLET': '3003.90.00.00',
                'CODE_PAYS_ORIGINE': 'CN'
            },
            {
                'DECLARATION_ID': '',  # Vide - doit √™tre construit
                'ANNEE': '2023',
                'BUREAU': 'ABJ',
                'NUMERO': '002',
                'POIDS_NET_KG': 200.0,
                'VALEUR_CAF': 1000000.0,
                'CODE_SH_COMPLET': '3003.90.00.00',
                'CODE_PAYS_ORIGINE': 'CN'
            },
            {
                'DECLARATION_ID': '',  # Vide - doit √™tre construit
                'ANNEE': '2023',
                'BUREAU': 'ABJ',
                'NUMERO': '2023/ABJ/003',  # NUMERO contient d√©j√† le DECLARATION_ID complet
                'POIDS_NET_KG': 150.0,
                'VALEUR_CAF': 750000.0,
                'CODE_SH_COMPLET': '3003.90.00.00',
                'CODE_PAYS_ORIGINE': 'CN'
            },
            {
                'DECLARATION_ID': '',  # Vide - doit √™tre construit
                'ANNEE': '2023',
                'BUREAU': 'ABJ',
                'NUMERO': '004',  # Juste un num√©ro
                'POIDS_NET_KG': 300.0,
                'VALEUR_CAF': 1500000.0,
                'CODE_SH_COMPLET': '3003.90.00.00',
                'CODE_PAYS_ORIGINE': 'CN'
            }
        ]
        
        # Tester l'agr√©gation
        pipeline = AdvancedOCRPipeline()
        result = pipeline.process_csv_with_aggregation(test_data, chapter, "basic")
        
        # V√©rifier les r√©sultats
        if "error" in result:
            return {
                "error": result["error"],
                "test_data": test_data
            }
        
        # Analyser l'agr√©gation
        aggregated_data = result.get("aggregated_data", {})
        unique_ids = result.get("unique_declaration_ids", 0)
        
        # V√©rifier que chaque DECLARATION_ID est unique et correctement form√©
        declaration_ids = list(aggregated_data.keys())
        
        return {
            "success": True,
            "chapter": chapter,
            "test_data_count": len(test_data),
            "unique_declaration_ids": unique_ids,
            "declaration_ids": declaration_ids,
            "aggregated_data": aggregated_data,
            "processed_declaration": result.get("processed_declaration", {}),
            "prediction": result.get("prediction", {}),
            "context": result.get("context", {}),
            "processing_time": result.get("processing_time")
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "chapter": chapter,
            "test_data": test_data if 'test_data' in locals() else []
        }

def run_auto_predict(chapter: str, uploads: List[str] = None, declarations: List[Dict] = None, pdfs: List[str] = None) -> Dict[str, Any]:
    """Fonction d'interface pour la pr√©diction automatique (compatible avec l'ancien syst√®me)"""
    try:
        pipeline = AdvancedOCRPipeline()
        results = []
        
        # Traiter les uploads (images)
        if uploads:
            for upload_path in uploads:
                result = pipeline.process_document(upload_path, chapter, "basic")
                results.append(result)
        
        # Traiter les PDFs
        if pdfs:
            for pdf_path in pdfs:
                # Pour les PDFs, on simule l'extraction OCR avec donn√©es plus r√©alistes
                if chapter == 'chap84':
                    ocr_data = {
                        'declaration_id': f'PDF_{Path(pdf_path).stem}',
                        'valeur_caf': 500000,  # Valeur plus r√©aliste pour machines
                        'poids_net': 2500,     # Poids plus √©lev√© pour machines
                        'quantite_complement': 1,  # Quantit√© unitaire pour machines
                        'taux_droits_percent': 15.0,  # Taux plus √©lev√©
                        'code_sh_complet': '8471309000',  # Code r√©ellement utilis√© dans les donn√©es
                        'pays_origine': 'CN',
                        'pays_provenance': 'CN',
                        'regime_complet': 'S110',  # R√©gime suspensif plus fr√©quent
                        'statut_bae': 'SANS_BAE',
                        'type_regime': 'IMPORTATION',
                        'regime_douanier': 'SUSPENSIF',
                        'regime_fiscal': 'NORMAL'
                    }
                else:
                    # Donn√©es par d√©faut pour autres chapitres
                    ocr_data = {
                        'declaration_id': f'PDF_{Path(pdf_path).stem}',
                        'valeur_caf': 1000000,
                        'poids_net': 100,
                        'quantite_complement': 10,
                        'taux_droits_percent': 5.0,
                        'code_sh_complet': '3003.90.00.00' if chapter == 'chap30' else '8517.12.00.00',
                        'pays_origine': 'CN',
                        'pays_provenance': 'CN',
                        'regime_complet': 'C111',
                        'statut_bae': 'AVEC_BAE',
                        'type_regime': 'CONSOMMATION',
                        'regime_douanier': 'CONSOMMATION',
                        'regime_fiscal': 'NORMAL'
                    }
                result = pipeline.predict_fraud(ocr_data, chapter, "basic")
                result['pdf_path'] = pdf_path
                results.append(result)
        
        # Traiter les d√©clarations directes
        if declarations:
            for declaration in declarations:
                result = pipeline.predict_fraud(declaration, chapter, "basic")
                results.append(result)

        return {
            "chapter": chapter,
            "results": results,
            "total_processed": len(results),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Erreur run_auto_predict pour {chapter}: {e}")
        return {
            "error": str(e),
            "chapter": chapter,
            "results": []
        }

def get_chapter_config(chapter: str) -> Dict[str, Any]:
    """Obtenir la configuration d'un chapitre"""
    return CHAPTER_CONFIGS.get(chapter, {})

def list_available_chapters() -> List[str]:
    """Lister les chapitres disponibles"""
    return list(CHAPTER_CONFIGS.keys())

def get_best_model_for_chapter(chapter: str) -> str:
    """Obtenir le meilleur mod√®le pour un chapitre"""
    config = CHAPTER_CONFIGS.get(chapter, {})
    return config.get("best_model", "unknown")


# -------------------------------
# FONCTIONS DE MAINTENANCE
# -------------------------------

def clear_model_cache():
    """Vider le cache des mod√®les"""
    with _CACHE_LOCK:
        _MODEL_CACHE.clear()
    with _RL_CACHE_LOCK:
        _RL_CACHE.clear()
    logger.info("Cache des mod√®les vid√©")

def get_cache_status() -> Dict[str, Any]:
    """Obtenir le statut du cache"""
    return {
        "ml_models_cached": len(_MODEL_CACHE),
        "rl_managers_cached": len(_RL_CACHE),
        "cached_chapters": list(_MODEL_CACHE.keys())
    }

# -------------------------------
# FONCTIONS DE TEST
# -------------------------------

def test_pipeline():
    """Tester le pipeline OCR"""
    logger.info("üß™ Test du pipeline OCR avanc√©")
    
    # Test des configurations
    for chapter in CHAPTER_CONFIGS.keys():
        config = get_chapter_config(chapter)
        best_model = get_best_model_for_chapter(chapter)
        logger.info(f"‚úÖ {chapter}: {config['name']} - Mod√®le: {best_model}")
    
    # Test de cr√©ation de contexte
    test_ocr_data = {
        'declaration_id': 'TEST_001',
        'valeur_caf': 1000000,
        'poids_net': 100,
        'quantite_complement': 10,
        'taux_droits_percent': 5.0,
        'code_sh_complet': '8517.12.00.00',
        'pays_origine': 'CN',
        'pays_provenance': 'CN',
        'regime_complet': 'C111',
        'statut_bae': 'AVEC_BAE',
        'type_regime': 'CONSOMMATION',
        'regime_douanier': 'CONSOMMATION',
        'regime_fiscal': 'NORMAL'
    }
    
    for chapter in CHAPTER_CONFIGS.keys():
        context = create_advanced_context_from_ocr_data(test_ocr_data, chapter)
        logger.info(f"‚úÖ Contexte cr√©√© pour {chapter}: {len(context)} features")
    
    logger.info("üéØ Pipeline OCR test√© avec succ√®s!")


def predict_fraud_risk(data: Dict[str, Any], chapter: str) -> Dict[str, Any]:
    """
    Fonction simple pour pr√©dire le risque de fraude √† partir de donn√©es structur√©es
    
    Args:
        data: Dictionnaire contenant les features de base
        chapter: Chapitre ('chap30', 'chap84', 'chap85')
    
    Returns:
        Dictionnaire avec la d√©cision et la probabilit√©
    """
    try:
        # Charger le mod√®le
        model_data = load_ml_model(chapter)
        if not model_data or 'model' not in model_data:
            return {
                "error": f"Mod√®le non trouv√© pour {chapter}",
                "decision": "ERREUR",
                "probability": 0.0,
                "final_probability": 0.0,
                "confidence": "LOW"
            }
        
        model = model_data['model']
        
        # Utiliser les fichiers g√©n√©r√©s au lieu de recr√©er les features
        import pandas as pd
        
        # Cr√©er un DataFrame avec les donn√©es d'entr√©e
        df = pd.DataFrame([data])
        
        # Appliquer les scalers et encoders g√©n√©r√©s si disponibles
        if 'scalers' in model_data and model_data['scalers']:
            scalers = model_data['scalers']
            # Appliquer les scalers aux features num√©riques
            for feature, scaler in scalers.items():
                if feature in df.columns:
                    df[feature] = scaler.transform(df[[feature]])
        
        if 'encoders' in model_data and model_data['encoders']:
            encoders = model_data['encoders']
            # Appliquer les encoders aux features cat√©gorielles
            for feature, encoder in encoders.items():
                if feature in df.columns:
                    encoded = encoder.transform(df[[feature]])
                    # Cr√©er les colonnes encod√©es
                    feature_names = encoder.get_feature_names_out([feature])
                    for i, col_name in enumerate(feature_names):
                        df[col_name] = encoded[:, i]
                    # Supprimer la colonne originale
                    df.drop(columns=[feature], inplace=True)
        
        # Filtrer avec seulement les features attendues par le mod√®le
        if 'feature_names' in model_data:
            expected_features = model_data['feature_names']
            # Garder seulement les colonnes qui existent dans le DataFrame
            available_features = [f for f in expected_features if f in df.columns]
            df = df[available_features]
            
            # Ajouter les features manquantes avec des valeurs par d√©faut
            missing_features = set(expected_features) - set(available_features)
            for feature in missing_features:
                df[feature] = 0.0  # Valeur par d√©faut
        
        # Faire la pr√©diction - LE MOD√àLE DOIT AVOIR predict_proba
        if hasattr(model, 'predict_proba'):
            proba = model.predict_proba(df)[0][1]  # Probabilit√© de classe 1 (fraude)
        else:
            # PAS DE FALLBACK! Les mod√®les ML DOIVENT avoir predict_proba
            logger.error(f"‚ùå ERREUR CRITIQUE: Le mod√®le pour {chapter} n'a pas de m√©thode predict_proba")
            raise ValueError(f"Le mod√®le ML pour {chapter} ne supporte pas predict_proba. Mod√®le invalide.")
        
        # Pas de calibration pour les nouveaux mod√®les ML avanc√©s
        final_proba = proba
        
        # D√©terminer la d√©cision
        decision = determine_decision(final_proba, chapter)
        
        # D√©terminer la confiance
        if final_proba > 0.8 or final_proba < 0.2:
            confidence = "HIGH"
        elif final_proba > 0.6 or final_proba < 0.4:
            confidence = "MEDIUM"
        else:
            confidence = "LOW"
        
        return {
            "decision": decision,
            "probability": float(proba),
            "final_probability": float(final_proba),
            "confidence": confidence,
            "model_used": CHAPTER_CONFIGS[chapter]["best_model"],
            "chapter": chapter,
            "features_used": len(df.columns)
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "decision": "ERREUR",
            "probability": 0.0,
            "final_probability": 0.0,
            "confidence": "LOW"
        }

def test_chap84_specifically():
    """Test sp√©cifique pour le chapitre 84 avec donn√©es r√©alistes"""
    logger.info("üß™ Test sp√©cifique du chapitre 84")
    
    # Donn√©es r√©alistes pour chapitre 84
    test_ocr_data = {
        'declaration_id': 'TEST_CHAP84_001',
        'valeur_caf': 750000,
        'poids_net': 1200,
        'quantite_complement': 2,
        'taux_droits_percent': 18.0,
        'code_sh_complet': '8471309000',  # Machine de bureau
        'pays_origine': 'CN',
        'pays_provenance': 'CN',
        'regime_complet': 'S110',
        'statut_bae': 'SANS_BAE',
        'type_regime': 'IMPORTATION',
        'regime_douanier': 'SUSPENSIF',
        'regime_fiscal': 'NORMAL'
    }
    
    pipeline = AdvancedOCRPipeline()
    result = pipeline.predict_fraud(test_ocr_data, "chap84", "basic")
    
    logger.info(f"üéØ Test chapitre 84 - ML Probability: {result.get('ml_probability', 'N/A')}")
    logger.info(f"üéØ Test chapitre 84 - Fraud Probability: {result.get('fraud_probability', 'N/A')}")
    
    return result

def display_performance_summary():
    """Afficher un r√©sum√© complet des m√©triques de performance des mod√®les ML avanc√©s"""
    logger.info("üìä R√âSUM√â COMPLET DES M√âTRIQUES DE PERFORMANCE DES MOD√àLES ML AVANC√âS")
    logger.info("=" * 80)
    
    for chapter, config in CHAPTER_CONFIGS.items():
        logger.info(f"\nüèÜ {chapter.upper()} - {config['name']}")
        logger.info(f"   ü§ñ Meilleur mod√®le: {config['best_model']}")
        logger.info(f"   üìä Performance:")
        logger.info(f"      - F1-Score: {config['f1_score']:.3f}")
        logger.info(f"      - AUC: {config['auc_score']:.3f}")
        logger.info(f"      - Precision: {config['precision']:.3f}")
        logger.info(f"      - Recall: {config['recall']:.3f}")
        logger.info(f"      - Accuracy: {config['accuracy']:.3f}")
        logger.info(f"   üéØ Seuils optimaux:")
        logger.info(f"      - Seuil optimal: {config['optimal_threshold']:.3f}")
        logger.info(f"      - Zone conforme: {config['optimal_threshold'] - 0.1:.3f}")
        logger.info(f"      - Zone fraude: {config['optimal_threshold'] + 0.1:.3f}")
        logger.info(f"   üìà Donn√©es:")
        logger.info(f"      - Taille: {config['data_size']:,} √©chantillons")
        logger.info(f"      - Taux de fraude: {config['fraud_rate']:.1f}%")
        logger.info(f"      - Features: {config['features_count']}")
        logger.info(f"   üõ°Ô∏è Validation:")
        logger.info(f"      - Statut: {config['validation_status']}")
        logger.info(f"      - Risque de fuite: {config['leakage_risk']}")
        
        # Afficher les m√©triques de performance d√©taill√©es
        if 'model_performance' in config:
            perf = config['model_performance']
            logger.info(f"   üî¨ Performance d√©taill√©e:")
            logger.info(f"      - Train: {perf.get('train_samples', 0):,} √©chantillons")
            logger.info(f"      - Validation: {perf.get('valid_samples', 0):,} √©chantillons")
            logger.info(f"      - Test: {perf.get('test_samples', 0):,} √©chantillons")
            logger.info(f"      - Taux de base: {perf.get('base_rate', 0.0):.3f}")
            logger.info(f"      - AUC: {perf.get('auc_score', 0.0):.4f}")
    
    logger.info("\n" + "=" * 80)
    logger.info("üéØ R√âSUM√â GLOBAL:")
    logger.info("   ‚úÖ Tous les mod√®les ML avanc√©s sont optimaux")
    logger.info("   ‚úÖ Validation robuste compl√®te effectu√©e")
    logger.info("   ‚úÖ Protection contre data leakage en place")
    logger.info("   ‚úÖ Seuils optimaux calcul√©s depuis les performances")
    logger.info("   ‚úÖ Performance de pr√©diction optimale")
    logger.info("=" * 80)

if __name__ == "__main__":
    # Afficher le r√©sum√© complet des m√©triques de performance
    display_performance_summary()
    
    # Tests du pipeline
    test_pipeline()
    test_chap84_specifically()  # Test sp√©cifique


def process_file_with_ml_prediction(file_path: str, chapter: str = None, level: str = "basic") -> Dict[str, Any]:
    """
    Workflow complet: Fichier ‚Üí Donn√©es ‚Üí Pr√©diction ML-RL
    
    Cette fonction orchestre le workflow complet en utilisant:
    - OCR_INGEST pour l'extraction de donn√©es depuis les fichiers
    - OCR_PIPELINE pour les pr√©dictions ML-RL
    
    Args:
        file_path: Chemin vers le fichier (PDF, CSV, Image)
        chapter: Chapitre cible (optionnel)
        level: Niveau RL (basic, advanced, expert)
    
    Returns:
        Dictionnaire avec donn√©es extraites ET pr√©diction ML-RL
    """
    try:
        # √âTAPE 1: OCR_INGEST - Extraire les donn√©es du fichier avec contexte avanc√©
        from .ocr_ingest import process_declaration_file
        extraction_result = process_declaration_file(file_path, chapter)
        
        # Valider le contrat de communication
        if not OCRDataContract.validate_ingest_result(extraction_result):
            logger.error("R√©sultat OCR_INGEST invalide - contrat de communication non respect√©")
            return {
                "validation_status": "error",
                "error": "Contrat de communication OCR_INGEST invalide",
                "ml_ready": False,
                "fraud_detection_enabled": False
            }
        
        if extraction_result.get("validation_status") != "success":
            return extraction_result
        
        # √âTAPE 2: OCR_PIPELINE - Extraire les donn√©es selon le contrat
        try:
            # Utiliser le contrat de communication pour extraire les donn√©es
            pipeline_input = OCRDataContract.extract_pipeline_input(extraction_result)
            context = pipeline_input["context"]
            chapter = extraction_result["metadata"]["chapter"]
            
            logger.info(f"Communication OCR_INGEST ‚Üí OCR_PIPELINE: {pipeline_input['features_count']} features, ML ready: {pipeline_input['ml_ready']}")
            
            # Faire la pr√©diction avec le contexte standardis√©
            prediction_result = predict_fraud_from_ocr_data(context, chapter, level)
            
            # Enrichir avec les informations du contexte avanc√©
            if context:
                prediction_result["advanced_context"] = context
                prediction_result["advanced_features_count"] = len([k for k in context.keys() if k.startswith('BUSINESS_') or 'SCORE' in k])
            
            # Combiner les r√©sultats
            result = {
                **extraction_result,  # Donn√©es d'extraction
                "ml_prediction": {
                    "predicted_fraud": prediction_result.get("predicted_fraud", False),
                    "fraud_probability": prediction_result.get("fraud_probability", 0.0),
                    "confidence_score": prediction_result.get("confidence_score", 0.0),
                    "decision_source": prediction_result.get("decision_source", "unknown"),
                    "model_used": prediction_result.get("model_used", "unknown"),
                    "performance_metrics": prediction_result.get("performance_metrics", {}),
                    "model_performance": prediction_result.get("model_performance", {}),
                    "advanced_features_used": prediction_result.get("advanced_features_count", 0),
                    "rl_level": level
                }
            }
            
            logger.info(f"Workflow complet: {file_path} -> Pr√©diction ML-RL r√©ussie avec {prediction_result.get('advanced_features_count', 0)} features avanc√©es")
            return result
            
        except Exception as e:
            logger.warning(f"Erreur pr√©diction ML-RL: {e}")
            return {
                **extraction_result,
                "ml_prediction": {
                    "predicted_fraud": False,
                    "fraud_probability": 0.0,
                    "confidence_score": 0.0,
                    "decision_source": "error",
                    "model_used": "unknown",
                    "error": str(e),
                    "rl_level": level
                }
            }
        
    except Exception as e:
        logger.error(f"Erreur workflow complet: {e}")
        return {
            "file_path": file_path,
            "chapter": chapter or "unknown",
            "error": str(e),
            "processing_timestamp": datetime.now().isoformat(),
            "validation_status": "error"
        }

def process_multiple_declarations_with_advanced_fraud_detection(declarations_data: List[Dict[str, Any]], chapter: str) -> Dict[str, Any]:
    """
    Traiter plusieurs d√©clarations avec la d√©tection de fraude avanc√©e compl√®te
    
    Args:
        declarations_data: Liste de dictionnaires contenant les donn√©es des d√©clarations
        chapter: Chapitre cible
    
    Returns:
        Dictionnaire avec les d√©clarations enrichies et les scores de fraude
    """
    try:
        import pandas as pd
        from ..utils.advanced_fraud_detection import AdvancedFraudDetection
        
        logger.info(f"üîç Traitement de {len(declarations_data)} d√©clarations avec d√©tection de fraude avanc√©e")
        
        # Convertir en DataFrame
        df = pd.DataFrame(declarations_data)
        
        # Cr√©er le d√©tecteur de fraude avanc√©e
        fraud_detector = AdvancedFraudDetection()
        
        # Ex√©cuter l'analyse compl√®te
        df_analyzed = fraud_detector.run_complete_analysis(df)
        
        # Convertir le r√©sultat en liste de dictionnaires
        enriched_declarations = df_analyzed.to_dict('records')
        
        # Calculer les statistiques
        fraud_rate = df_analyzed['FRAUD_FLAG'].mean() * 100
        avg_composite_score = df_analyzed['COMPOSITE_FRAUD_SCORE'].mean()
        
        result = {
            "chapter": chapter,
            "total_declarations": len(enriched_declarations),
            "enriched_declarations": enriched_declarations,
            "fraud_statistics": {
                "fraud_rate_percent": fraud_rate,
                "avg_composite_fraud_score": avg_composite_score,
                "high_risk_count": (df_analyzed['COMPOSITE_FRAUD_SCORE'] > 0.7).sum(),
                "medium_risk_count": ((df_analyzed['COMPOSITE_FRAUD_SCORE'] > 0.3) & (df_analyzed['COMPOSITE_FRAUD_SCORE'] <= 0.7)).sum(),
                "low_risk_count": (df_analyzed['COMPOSITE_FRAUD_SCORE'] <= 0.3).sum()
            },
            "processing_timestamp": datetime.now().isoformat(),
            "analysis_method": "advanced_fraud_detection_complete"
        }
        
        logger.info(f"‚úÖ Analyse termin√©e: {fraud_rate:.1f}% de fraude d√©tect√©e, score moyen: {avg_composite_score:.3f}")
        return result
        
    except Exception as e:
        logger.error(f"Erreur traitement multiple d√©clarations: {e}")
        return {
            "chapter": chapter,
            "error": str(e),
            "processing_timestamp": datetime.now().isoformat(),
            "analysis_method": "advanced_fraud_detection_complete"
        }

def get_advanced_features_summary(chapter: str = None) -> Dict[str, Any]:
    """
    Obtenir un r√©sum√© des features avanc√©es disponibles pour un chapitre
    """
    try:
        if chapter:
            config = CHAPTER_CONFIGS.get(chapter, {})
            return {
                "chapter": chapter,
                "name": config.get("name", "Inconnu"),
                "best_model": config.get("best_model", "unknown"),
                "features_count": config.get("features_count", 0),
                "f1_score": config.get("f1_score", 0.0),
                "auc_score": config.get("auc_score", 0.0),
                "optimal_threshold": config.get("optimal_threshold", 0.5),
                "validation_status": config.get("validation_status", "ROBUST"),
                "available_features": [
                    "Features de base (POIDS_NET_KG, VALEUR_CAF, etc.)",
                    "Features string (CODE_PRODUIT_STR, PAYS_ORIGINE_STR, etc.)",
                    "Features business sp√©cifiques au chapitre",
                    "Scores de d√©tection de fraude avanc√©e (Bienaym√©-Tchebychev, TEI, Clustering, etc.)",
                    "Contexte enrichi pour ML-RL",
                    "Analyse compl√®te avec AdvancedFraudDetection"
                ],
                "fraud_detection_methods": [
                    "Th√©or√®me de Bienaym√©-Tchebychev",
                    "Analyse miroir avec TEI (Taux Effectifs d'Imposition)",
                    "Clustering spectral pour d√©tection d'anomalies",
                    "Clustering hi√©rarchique pour d√©tection d'anomalies",
                    "Contr√¥le des valeurs administr√©es",
                    "Score composite de fraude"
                ]
            }
        else:
            # R√©sum√© pour tous les chapitres
            summary = {}
            for chap, config in CHAPTER_CONFIGS.items():
                summary[chap] = {
                    "name": config.get("name", "Inconnu"),
                    "best_model": config.get("best_model", "unknown"),
                    "f1_score": config.get("f1_score", 0.0),
                    "auc_score": config.get("auc_score", 0.0),
                    "optimal_threshold": config.get("optimal_threshold", 0.5)
                }
            return summary
            
    except Exception as e:
        logger.error(f"Erreur r√©sum√© features avanc√©es: {e}")
        return {"error": str(e)}
