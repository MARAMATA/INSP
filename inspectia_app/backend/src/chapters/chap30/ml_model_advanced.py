#!/usr/bin/env python3
"""
Mod√®le ML Supervis√© pour la D√©tection de Fraude - Chapitre 30 AVANC√â
Utilise les nouvelles donn√©es pr√©process√©es avec techniques avanc√©es de d√©tection de fraude
"""

import pandas as pd
import numpy as np
import logging
import joblib
from pathlib import Path
from datetime import datetime
import warnings
import yaml
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, roc_curve, f1_score, precision_score, recall_score,
    average_precision_score, auc
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV

# Gradient Boosting
import lightgbm as lgb
import xgboost as xgb
import catboost as cb

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# SHAP for feature importance
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("‚ö†Ô∏è SHAP non disponible - l'analyse SHAP sera ignor√©e")

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Chap30MLAdvanced:
    """Mod√®le ML Supervis√© pour la D√©tection de Fraude - Chapitre 30 AVANC√â"""
    
    def __init__(self):
        # Chemins des donn√©es
        self.backend_root = Path(__file__).resolve().parents[3]
        self.data_path = self.backend_root / "data/processed/CHAP30_PROCESSED_ADVANCED.csv"
        self.splits_path = self.backend_root / "data/ml_splits/chap30"
        self.results_path = self.backend_root / "results/chap30"
        self.models_path = self.results_path / "models"
        
        # Cr√©er les dossiers n√©cessaires
        self.splits_path.mkdir(parents=True, exist_ok=True)
        self.results_path.mkdir(parents=True, exist_ok=True)
        self.models_path.mkdir(parents=True, exist_ok=True)
        
        # Configuration des mod√®les
        self.models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100, 
                max_depth=10, 
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                random_state=42, 
                max_iter=1000,
                class_weight='balanced'
            ),
            'LightGBM': lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss'
            ),
            'CatBoost': cb.CatBoostClassifier(
                iterations=100,
                depth=6,
                learning_rate=0.1,
                random_state=42,
                verbose=False
            )
        }
        
        # Colonnes √† utiliser pour l'entra√Ænement
        self.feature_columns = self._get_feature_columns()
        
        logger.info("‚úÖ Mod√®le ML Chapitre 30 AVANC√â initialis√©")

    def _get_feature_columns(self):
        """D√©finir les colonnes √† utiliser comme features"""
        # Colonnes num√©riques de base
        numeric_features = [
            'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET',
            'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
            'NUMERO_ARTICLE', 'PRECISION_UEMOA'
        ]
        
        # Colonnes des techniques avanc√©es de d√©tection de fraude
        fraud_detection_features = [
            'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE', 
            'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 
            'HIERARCHICAL_CLUSTER_SCORE', 'ADMIN_VALUES_SCORE', 
            'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR'
        ]
        
        # Colonnes business (toutes les features BUSINESS_)
        business_features = [
            'BUSINESS_GLISSEMENT_COSMETIQUE', 'BUSINESS_GLISSEMENT_PAYS_COSMETIQUES',
            'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
            'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
            'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE',
            'BUSINESS_VALEUR_EXCEPTIONNELLE', 'BUSINESS_POIDS_ELEVE',
            'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
            'BUSINESS_RATIO_DOUANE_CAF', 'BUSINESS_IS_MEDICAMENT',
            'BUSINESS_IS_ANTIPALUDEEN', 'BUSINESS_IS_PRECISION_UEMOA',
            'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI'
        ]
        
        # Colonnes cat√©gorielles
        categorical_features = [
            'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR',
            'BUREAU', 'REGIME_FISCAL', 'NUMERO_DPI'
        ]
        
        return {
            'numeric': numeric_features,
            'fraud_detection': fraud_detection_features,
            'business': business_features,
            'categorical': categorical_features
        }

    def load_data(self):
        """Charger les donn√©es pr√©process√©es"""
        logger.info("üìä Chargement des donn√©es pr√©process√©es...")
        
        df = pd.read_csv(self.data_path)
        logger.info(f"‚úÖ Donn√©es charg√©es: {df.shape}")
        
        # V√©rifier la pr√©sence du target
        if 'FRAUD_FLAG' not in df.columns:
            raise ValueError("FRAUD_FLAG non trouv√© dans les donn√©es")
        
        # Afficher la distribution du target
        fraud_distribution = df['FRAUD_FLAG'].value_counts()
        logger.info(f"üìä Distribution FRAUD_FLAG: {fraud_distribution.to_dict()}")
        logger.info(f"üìä Taux de fraude: {df['FRAUD_FLAG'].mean()*100:.2f}%")
        
        return df

    def prepare_features(self, df):
        """Pr√©parer les features pour l'entra√Ænement"""
        logger.info("üîß Pr√©paration des features...")
        
        # Toutes les colonnes de features
        all_features = []
        for feature_type, features in self.feature_columns.items():
            all_features.extend(features)
        
        # V√©rifier que toutes les colonnes existent
        missing_cols = [col for col in all_features if col not in df.columns]
        if missing_cols:
            logger.warning(f"‚ö†Ô∏è Colonnes manquantes: {missing_cols}")
            all_features = [col for col in all_features if col in df.columns]
        
        # S√©lectionner les features et le target
        X = df[all_features].copy()
        y = df['FRAUD_FLAG'].copy()
        
        logger.info(f"‚úÖ Features pr√©par√©es: {X.shape[1]} features, {X.shape[0]} √©chantillons")
        logger.info(f"‚úÖ Target pr√©par√©: {y.shape[0]} √©chantillons")
        
        return X, y

    def create_train_test_splits(self, X, y):
        """Cr√©er les splits train/test/validation"""
        logger.info("üìä Cr√©ation des splits train/test/validation...")
        
        # Split initial: 80% train, 20% test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Split du train: 80% train, 20% validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # Sauvegarder les splits
        splits = {
            'X_train': X_train,
            'X_val': X_val,
            'X_test': X_test,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test
        }
        
        for name, data in splits.items():
            data.to_csv(self.splits_path / f"{name}.csv", index=False)
        
        logger.info(f"‚úÖ Splits cr√©√©s et sauvegard√©s dans {self.splits_path}")
        logger.info(f"   Train: {X_train.shape[0]} √©chantillons")
        logger.info(f"   Validation: {X_val.shape[0]} √©chantillons")
        logger.info(f"   Test: {X_test.shape[0]} √©chantillons")
        
        return splits

    def create_preprocessing_pipeline(self, X):
        """Cr√©er le pipeline de preprocessing avec TOUTES les features importantes"""
        logger.info("üîß Cr√©ation du pipeline de preprocessing...")
        
        # Toutes les features num√©riques importantes (inclure toutes les features disponibles)
        numeric_features = [
            # Features num√©riques de base
            'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET', 'POIDS_NET_KG',
            'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
            'NUMERO_ARTICLE', 'PRECISION_UEMOA', 'NOMBRE_COLIS', 'QUANTITE_COMPLEMENT',
            'VALEUR_UNITAIRE_PAR_KG', 'VALEUR_FOB', 'VALEUR_PAR_COLIS', 'POIDS_BRUT',
            'ASSURANCE', 'FRET', 'TAUX', 'MONTANT', 'BASE_TAXABLE', 'NOMBRE_CONTENEUR',
            
            # Features de d√©tection de fraude avanc√©e
            'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE',
            'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 'HIERARCHICAL_CLUSTER_SCORE',
            'ADMIN_VALUES_SCORE', 'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR',
            
            # Features business (toutes les features BUSINESS_)
            'BUSINESS_GLISSEMENT_COSMETIQUE', 'BUSINESS_GLISSEMENT_PAYS_COSMETIQUES',
            'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
            'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
            'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE', 'BUSINESS_VALEUR_EXCEPTIONNELLE',
            'BUSINESS_POIDS_ELEVE', 'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
            'BUSINESS_RATIO_DOUANE_CAF', 'BUSINESS_IS_MEDICAMENT', 'BUSINESS_IS_ANTIPALUDEEN',
            'BUSINESS_IS_PRECISION_UEMOA', 'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI'
        ]
        
        # Features cat√©gorielles importantes
        categorical_features = [
            'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR', 'BUREAU',
            'REGIME_FISCAL', 'NUMERO_ARTICLE_STR', 'PRECISION_UEMOA_STR', 'DATE_DECLARATION_STR',
            'CODE_SH', 'LIBELLE_TARIF', 'DESCRIPTION_COMMERCIALE', 'CATEGORIE_PRODUIT',
            'ALERTE_MOTS_CLES', 'DESTINATION', 'BUREAU_FRONTIERE', 'TYPE_REGIME',
            'REGIME_DOUANIER', 'REGIME_FISCAL_CODE', 'STATUT_BAE', 'CODE_TAXE',
            'LIBELLE_TAXE', 'NOM_NAVIRE', 'DATE_ARRIVEE', 'DATE_EMBARQUEMENT'
        ]
        
        # Filtrer les features qui existent dans les donn√©es
        numeric_features = [col for col in numeric_features if col in X.columns]
        categorical_features = [col for col in categorical_features if col in X.columns]
        
        # Pipeline de preprocessing
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='UNKNOWN')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ]
        )
        
        logger.info(f"‚úÖ Pipeline cr√©√©: {len(numeric_features)} num√©riques, {len(categorical_features)} cat√©gorielles")
        logger.info(f"   Features num√©riques: {numeric_features[:10]}..." if len(numeric_features) > 10 else f"   Features num√©riques: {numeric_features}")
        logger.info(f"   Features cat√©gorielles: {categorical_features[:5]}..." if len(categorical_features) > 5 else f"   Features cat√©gorielles: {categorical_features}")
        
        return preprocessor

    def train_models(self, splits, preprocessor):
        """Entra√Æner tous les mod√®les en respectant la convention train/val/test"""
        logger.info("üöÄ D√©but de l'entra√Ænement des mod√®les...")
        logger.info("üìã Convention respect√©e: fit sur train, val pour s√©lection, test pour √©valuation finale")
        
        results = {}
        trained_models = {}
        validation_results = {}
        
        for name, model in self.models.items():
            logger.info(f"üìä Entra√Ænement {name}...")
            
            # Cr√©er le pipeline complet
            pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('classifier', model)
            ])
            
            # √âTAPE 1: Fit uniquement sur X_train (convention respect√©e)
            logger.info(f"   üîß Fit sur X_train uniquement...")
            pipeline.fit(splits['X_train'], splits['y_train'])
            
            # √âTAPE 2: √âvaluation sur X_val pour la s√©lection d'hyperparam√®tres
            logger.info(f"   üìä √âvaluation sur X_val pour s√©lection...")
            y_val_pred = pipeline.predict(splits['X_val'])
            y_val_pred_proba = pipeline.predict_proba(splits['X_val'])[:, 1]
            
            # M√©triques de validation
            val_metrics = self._calculate_metrics(splits['y_val'], y_val_pred, y_val_pred_proba)
            validation_results[name] = val_metrics
            
            logger.info(f"   üìà {name} - Validation AUC: {val_metrics['auc']:.4f}, F1: {val_metrics['f1']:.4f}")
            
            # √âTAPE 3: Test final sur X_test (une seule fois, apr√®s s√©lection)
            logger.info(f"   üéØ Test final sur X_test...")
            y_test_pred = pipeline.predict(splits['X_test'])
            y_test_pred_proba = pipeline.predict_proba(splits['X_test'])[:, 1]
            
            # M√©triques de test
            test_metrics = self._calculate_metrics(splits['y_test'], y_test_pred, y_test_pred_proba)
            results[name] = test_metrics
            
            logger.info(f"   üèÜ {name} - Test AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}")
            
            trained_models[name] = pipeline
        
        # Sauvegarder les mod√®les
        for name, model in trained_models.items():
            joblib.dump(model, self.models_path / f"{name.lower()}_model.pkl")
        
        logger.info(f"‚úÖ Tous les mod√®les entra√Æn√©s et sauvegard√©s dans {self.models_path}")
        logger.info(f"üìä R√©sultats de validation: {len(validation_results)} mod√®les")
        logger.info(f"üìä R√©sultats de test: {len(results)} mod√®les")
        
        return results, trained_models, validation_results

    def _calculate_metrics(self, y_true, y_pred, y_pred_proba):
        """Calculer les m√©triques de performance"""
        return {
            'accuracy': (y_pred == y_true).mean(),
            'precision': precision_score(y_true, y_pred),
            'recall': recall_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred),
            'auc': roc_auc_score(y_true, y_pred_proba),
            'avg_precision': average_precision_score(y_true, y_pred_proba)
        }

    def find_best_model(self, validation_results):
        """Trouver le meilleur mod√®le bas√© sur le F1-Score de validation (convention respect√©e)"""
        best_model = max(validation_results.items(), key=lambda x: x[1]['f1'])
        logger.info(f"üèÜ Meilleur mod√®le s√©lectionn√© sur validation: {best_model[0]} (Validation F1: {best_model[1]['f1']:.4f}, AUC: {best_model[1]['auc']:.4f})")
        return best_model[0]

    def generate_comprehensive_results(self, results, trained_models, splits, best_model_name):
        """G√©n√©rer tous les r√©sultats et graphiques"""
        logger.info("üìä G√©n√©ration des r√©sultats complets...")
        
        # 1. M√©triques de comparaison
        self._plot_metrics_comparison(results, best_model_name)
        
        # 2. Matrices de confusion
        self._plot_confusion_matrices(trained_models, splits, best_model_name)
        
        # 3. Courbes ROC
        self._plot_roc_curves(trained_models, splits, best_model_name)
        
        # 4. Courbes Precision-Recall
        self._plot_precision_recall_curves(trained_models, splits, best_model_name)
        
        # 5. SHAP pour le meilleur mod√®le
        self._generate_shap_analysis(trained_models[best_model_name], splits)
        
        # 6. Rapport YAML
        self._generate_yaml_report(results, best_model_name)
        
        logger.info("‚úÖ Tous les r√©sultats g√©n√©r√©s avec succ√®s")

    def _plot_metrics_comparison(self, results, best_model_name):
        """G√©n√©rer les graphiques de comparaison des m√©triques"""
        logger.info("üìä G√©n√©ration des graphiques de m√©triques...")
        
        # Pr√©parer les donn√©es
        metrics_df = pd.DataFrame(results).T
        
        # Graphique de comparaison des m√©triques - AM√âLIOR√â
        fig, axes = plt.subplots(2, 3, figsize=(20, 14))
        fig.suptitle('Comparaison des M√©triques - Chapitre 30 (Produits Pharmaceutiques)', 
                     fontsize=18, fontweight='bold', y=0.98)
        
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'avg_precision']
        titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Average Precision']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
        
        for i, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
            ax = axes[i//3, i%3]
            
            # Cr√©er les barres avec des couleurs diff√©rentes
            bars = ax.bar(range(len(metrics_df)), metrics_df[metric], 
                         color=color, alpha=0.8, edgecolor='black', linewidth=0.5)
            
            ax.set_title(title, fontweight='bold', fontsize=14, pad=20)
            ax.set_ylabel('Score', fontsize=12, fontweight='bold')
            ax.set_ylim(0, 1.05)
            
            # Personnaliser les labels des axes
            ax.set_xticks(range(len(metrics_df)))
            ax.set_xticklabels(metrics_df.index, rotation=45, ha='right', fontsize=10)
            
            # Ajouter les valeurs sur les barres avec un meilleur positionnement
            for j, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                       f'{height:.3f}', ha='center', va='bottom', 
                       fontweight='bold', fontsize=9)
            
            # Am√©liorer la grille
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.set_axisbelow(True)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(self.results_path / 'metrics_comparison_all_algorithms.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # Graphique du meilleur mod√®le - AM√âLIOR√â
        best_model = metrics_df.loc[best_model_name]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Cr√©er un graphique en barres horizontales pour une meilleure lisibilit√©
        y_pos = np.arange(len(metrics))
        bars = ax.barh(y_pos, [best_model[metric] for metric in metrics], 
                      color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
        
        ax.set_yticks(y_pos)
        ax.set_yticklabels(titles, fontsize=12, fontweight='bold')
        ax.set_xlabel('Score', fontsize=14, fontweight='bold')
        ax.set_xlim(0, 1.05)
        ax.set_title(f'M√©triques du Meilleur Mod√®le - {best_model_name}\nChapitre 30 (Produits Pharmaceutiques)', 
                    fontweight='bold', fontsize=16, pad=20)
        
        # Ajouter les valeurs sur les barres
        for i, (bar, metric) in enumerate(zip(bars, metrics)):
            width = bar.get_width()
            ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,
                   f'{width:.3f}', ha='left', va='center', 
                   fontweight='bold', fontsize=11)
        
        # Am√©liorer la grille
        ax.grid(True, alpha=0.3, linestyle='--', axis='x')
        ax.set_axisbelow(True)
        
        # Inverser l'ordre des barres pour avoir le meilleur score en haut
        ax.invert_yaxis()
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'metrics_best.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        logger.info("‚úÖ Graphiques de m√©triques g√©n√©r√©s")

    def _plot_confusion_matrices(self, trained_models, splits, best_model_name):
        """G√©n√©rer les matrices de confusion"""
        logger.info("üìä G√©n√©ration des matrices de confusion...")
        
        # Toutes les matrices de confusion - AM√âLIOR√â
        n_models = len(trained_models)
        fig, axes = plt.subplots(2, 3, figsize=(20, 14))
        fig.suptitle('Matrices de Confusion - Chapitre 30 (Produits Pharmaceutiques)', 
                     fontsize=18, fontweight='bold', y=0.98)
        
        # Couleurs pour chaque mod√®le
        model_colors = ['Blues', 'Greens', 'Oranges', 'Reds', 'Purples']
        
        for i, (name, model) in enumerate(trained_models.items()):
            ax = axes[i//3, i%3]
            
            y_pred = model.predict(splits['X_test'])
            cm = confusion_matrix(splits['y_test'], y_pred)
            
            # Calculer les pourcentages
            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
            
            # Cr√©er l'annotation avec les valeurs et pourcentages
            annotations = []
            for j in range(cm.shape[0]):
                row = []
                for k in range(cm.shape[1]):
                    row.append(f'{cm[j, k]}\n({cm_percent[j, k]:.1f}%)')
                annotations.append(row)
            
            sns.heatmap(cm, annot=annotations, fmt='', cmap=model_colors[i % len(model_colors)], 
                       ax=ax, cbar_kws={'shrink': 0.8})
            ax.set_title(f'{name}', fontweight='bold', fontsize=14, pad=15)
            ax.set_xlabel('Pr√©diction', fontsize=12, fontweight='bold')
            ax.set_ylabel('Vraie Valeur', fontsize=12, fontweight='bold')
            
            # Personnaliser les labels
            ax.set_xticklabels(['Conforme', 'Fraude'], fontsize=10)
            ax.set_yticklabels(['Conforme', 'Fraude'], fontsize=10)
        
        # Masquer le dernier subplot s'il n'est pas utilis√©
        if n_models < 6:
            axes[1, 2].set_visible(False)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(self.results_path / 'confusion_matrices_all.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # Matrice de confusion du meilleur mod√®le (bas√© sur F1 validation)
        best_model = trained_models[best_model_name]
        
        y_pred = best_model.predict(splits['X_test'])
        cm = confusion_matrix(splits['y_test'], y_pred)
        
        # Calculer les m√©triques d√©taill√©es
        tn, fp, fn, tp = cm.ravel()
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        plt.figure(figsize=(10, 8))
        
        # Cr√©er l'annotation avec les m√©triques
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        annotations = []
        for j in range(cm.shape[0]):
            row = []
            for k in range(cm.shape[1]):
                row.append(f'{cm[j, k]}\n({cm_percent[j, k]:.1f}%)')
            annotations.append(row)
        
        sns.heatmap(cm, annot=annotations, fmt='', cmap='RdYlBu_r', 
                   cbar_kws={'shrink': 0.8})
        
        plt.title(f'Matrice de Confusion - {best_model_name} (Meilleur Mod√®le)\n'
                 f'Chapitre 30 (Produits Pharmaceutiques)\n'
                 f'Accuracy: {accuracy:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}', 
                 fontweight='bold', fontsize=14, pad=20)
        plt.xlabel('Pr√©diction', fontsize=12, fontweight='bold')
        plt.ylabel('Vraie Valeur', fontsize=12, fontweight='bold')
        
        # Personnaliser les labels
        plt.xticks([0.5, 1.5], ['Conforme', 'Fraude'], fontsize=11)
        plt.yticks([0.5, 1.5], ['Conforme', 'Fraude'], fontsize=11)
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'confusion_matrix_best_algorithm.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        logger.info("‚úÖ Matrices de confusion g√©n√©r√©es")

    def _plot_roc_curves(self, trained_models, splits, best_model_name):
        """G√©n√©rer les courbes ROC"""
        logger.info("üìä G√©n√©ration des courbes ROC...")
        
        # Toutes les courbes ROC - AM√âLIOR√â
        plt.figure(figsize=(14, 10))
        
        # Couleurs et styles pour chaque mod√®le
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
        linestyles = ['-', '--', '-.', ':', '-']
        
        for i, (name, model) in enumerate(trained_models.items()):
            y_pred_proba = model.predict_proba(splits['X_test'])[:, 1]
            fpr, tpr, _ = roc_curve(splits['y_test'], y_pred_proba)
            auc_score = roc_auc_score(splits['y_test'], y_pred_proba)
            
            plt.plot(fpr, tpr, 
                    color=colors[i % len(colors)], 
                    linestyle=linestyles[i % len(linestyles)],
                    linewidth=3, 
                    label=f'{name} (AUC = {auc_score:.4f})',
                    alpha=0.8)
        
        # Ligne de r√©f√©rence (classificateur al√©atoire)
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, label='Classificateur Al√©atoire')
        
        plt.xlabel('Taux de Faux Positifs (1 - Sp√©cificit√©)', fontsize=14, fontweight='bold')
        plt.ylabel('Taux de Vrais Positifs (Sensibilit√©)', fontsize=14, fontweight='bold')
        plt.title('Courbes ROC - Chapitre 30 (Produits Pharmaceutiques)', 
                 fontsize=16, fontweight='bold', pad=20)
        
        # Am√©liorer la l√©gende
        plt.legend(loc='lower right', fontsize=12, framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # Personnaliser les axes
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'roc_curves_all.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # Courbe ROC du meilleur mod√®le (bas√© sur F1 validation)
        best_model = trained_models[best_model_name]
        
        y_pred_proba = best_model.predict_proba(splits['X_test'])[:, 1]
        fpr, tpr, _ = roc_curve(splits['y_test'], y_pred_proba)
        auc_score = roc_auc_score(splits['y_test'], y_pred_proba)
        
        plt.figure(figsize=(10, 8))
        
        # Courbe principale
        plt.plot(fpr, tpr, color='#FF6B6B', linewidth=4, 
                label=f'{best_model_name} (AUC = {auc_score:.4f})', alpha=0.8)
        
        # Ligne de r√©f√©rence
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, 
                label='Classificateur Al√©atoire')
        
        # Zone sous la courbe
        plt.fill_between(fpr, tpr, alpha=0.2, color='#FF6B6B')
        
        plt.xlabel('Taux de Faux Positifs (1 - Sp√©cificit√©)', fontsize=14, fontweight='bold')
        plt.ylabel('Taux de Vrais Positifs (Sensibilit√©)', fontsize=14, fontweight='bold')
        plt.title(f'Courbe ROC - {best_model_name} (Meilleur Mod√®le)\n'
                 f'Chapitre 30 (Produits Pharmaceutiques)', 
                 fontsize=16, fontweight='bold', pad=20)
        
        # Am√©liorer la l√©gende
        plt.legend(loc='lower right', fontsize=14, framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # Personnaliser les axes
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        
        # Ajouter des annotations
        plt.text(0.6, 0.2, f'AUC = {auc_score:.4f}', 
                fontsize=16, fontweight='bold', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'roc_curve_best_algorithm.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        logger.info("‚úÖ Courbes ROC g√©n√©r√©es")

    def _plot_precision_recall_curves(self, trained_models, splits, best_model_name):
        """G√©n√©rer les courbes Precision-Recall"""
        logger.info("üìä G√©n√©ration des courbes Precision-Recall...")
        
        # Toutes les courbes Precision-Recall - AM√âLIOR√â
        plt.figure(figsize=(14, 10))
        
        # Couleurs et styles pour chaque mod√®le
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
        linestyles = ['-', '--', '-.', ':', '-']
        
        for i, (name, model) in enumerate(trained_models.items()):
            y_pred_proba = model.predict_proba(splits['X_test'])[:, 1]
            precision, recall, _ = precision_recall_curve(splits['y_test'], y_pred_proba)
            avg_precision = average_precision_score(splits['y_test'], y_pred_proba)
            
            plt.plot(recall, precision, 
                    color=colors[i % len(colors)], 
                    linestyle=linestyles[i % len(linestyles)],
                    linewidth=3, 
                    label=f'{name} (AP = {avg_precision:.4f})',
                    alpha=0.8)
        
        # Ligne de r√©f√©rence (baseline)
        baseline = splits['y_test'].mean()
        plt.axhline(y=baseline, color='k', linestyle='--', linewidth=2, alpha=0.6, 
                   label=f'Baseline (AP = {baseline:.4f})')
        
        plt.xlabel('Recall (Sensibilit√©)', fontsize=14, fontweight='bold')
        plt.ylabel('Precision (Pr√©cision)', fontsize=14, fontweight='bold')
        plt.title('Courbes Precision-Recall - Chapitre 30 (Produits Pharmaceutiques)', 
                 fontsize=16, fontweight='bold', pad=20)
        
        # Am√©liorer la l√©gende
        plt.legend(loc='lower left', fontsize=12, framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # Personnaliser les axes
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'precision_recall_curves_all.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # Courbe Precision-Recall du meilleur mod√®le (bas√© sur F1 validation)
        best_model = trained_models[best_model_name]
        
        y_pred_proba = best_model.predict_proba(splits['X_test'])[:, 1]
        precision, recall, _ = precision_recall_curve(splits['y_test'], y_pred_proba)
        avg_precision = average_precision_score(splits['y_test'], y_pred_proba)
        
        plt.figure(figsize=(10, 8))
        
        # Courbe principale
        plt.plot(recall, precision, color='#FF6B6B', linewidth=4, 
                label=f'{best_model_name} (AP = {avg_precision:.4f})', alpha=0.8)
        
        # Ligne de r√©f√©rence
        baseline = splits['y_test'].mean()
        plt.axhline(y=baseline, color='k', linestyle='--', linewidth=2, alpha=0.6, 
                   label=f'Baseline (AP = {baseline:.4f})')
        
        # Zone sous la courbe
        plt.fill_between(recall, precision, alpha=0.2, color='#FF6B6B')
        
        plt.xlabel('Recall (Sensibilit√©)', fontsize=14, fontweight='bold')
        plt.ylabel('Precision (Pr√©cision)', fontsize=14, fontweight='bold')
        plt.title(f'Courbe Precision-Recall - {best_model_name} (Meilleur Mod√®le)\n'
                 f'Chapitre 30 (Produits Pharmaceutiques)', 
                 fontsize=16, fontweight='bold', pad=20)
        
        # Am√©liorer la l√©gende
        plt.legend(loc='lower left', fontsize=14, framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # Personnaliser les axes
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        
        # Ajouter des annotations
        plt.text(0.6, 0.3, f'AP = {avg_precision:.4f}', 
                fontsize=16, fontweight='bold', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.results_path / 'precision_recall_curve_best_algorithm.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        logger.info("‚úÖ Courbes Precision-Recall g√©n√©r√©es")

    def _get_feature_names_after_preprocessing(self, model, X_sample):
        """Obtenir les noms des features apr√®s preprocessing"""
        preprocessor = model.named_steps['preprocessor']
        
        # Toutes les features num√©riques importantes (m√™me liste que dans create_preprocessing_pipeline)
        numeric_features = [
            # Features num√©riques de base
            'VALEUR_CAF', 'VALEUR_DOUANE', 'MONTANT_LIQUIDATION', 'POIDS_NET', 'POIDS_NET_KG',
            'VALEUR_UNITAIRE_KG', 'TAUX_DROITS_PERCENT', 'RATIO_DOUANE_CAF',
            'NUMERO_ARTICLE', 'PRECISION_UEMOA', 'NOMBRE_COLIS', 'QUANTITE_COMPLEMENT',
            'VALEUR_UNITAIRE_PAR_KG', 'VALEUR_FOB', 'VALEUR_PAR_COLIS', 'POIDS_BRUT',
            'ASSURANCE', 'FRET', 'TAUX', 'MONTANT', 'BASE_TAXABLE', 'NOMBRE_CONTENEUR',
            
            # Features de d√©tection de fraude avanc√©e
            'BIENAYME_CHEBYCHEV_SCORE', 'TEI_CALCULE', 'MIRROR_TEI_SCORE',
            'MIRROR_TEI_DEVIATION', 'SPECTRAL_CLUSTER_SCORE', 'HIERARCHICAL_CLUSTER_SCORE',
            'ADMIN_VALUES_SCORE', 'ADMIN_VALUES_DEVIATION', 'COMPOSITE_FRAUD_SCORE', 'RATIO_POIDS_VALEUR',
            
            # Features business (toutes les features BUSINESS_)
            'BUSINESS_GLISSEMENT_COSMETIQUE', 'BUSINESS_GLISSEMENT_PAYS_COSMETIQUES',
            'BUSINESS_GLISSEMENT_RATIO_SUSPECT', 'BUSINESS_RISK_PAYS_HIGH',
            'BUSINESS_ORIGINE_DIFF_PROVENANCE', 'BUSINESS_REGIME_PREFERENTIEL',
            'BUSINESS_REGIME_NORMAL', 'BUSINESS_VALEUR_ELEVEE', 'BUSINESS_VALEUR_EXCEPTIONNELLE',
            'BUSINESS_POIDS_ELEVE', 'BUSINESS_DROITS_ELEVES', 'BUSINESS_RATIO_LIQUIDATION_CAF',
            'BUSINESS_RATIO_DOUANE_CAF', 'BUSINESS_IS_MEDICAMENT', 'BUSINESS_IS_ANTIPALUDEEN',
            'BUSINESS_IS_PRECISION_UEMOA', 'BUSINESS_ARTICLES_MULTIPLES', 'BUSINESS_AVEC_DPI'
        ]
        
        # Features cat√©gorielles importantes
        categorical_features = [
            'CODE_PRODUIT_STR', 'PAYS_ORIGINE_STR', 'PAYS_PROVENANCE_STR', 'BUREAU',
            'REGIME_FISCAL', 'NUMERO_ARTICLE_STR', 'PRECISION_UEMOA_STR', 'DATE_DECLARATION_STR',
            'CODE_SH', 'LIBELLE_TARIF', 'DESCRIPTION_COMMERCIALE', 'CATEGORIE_PRODUIT',
            'ALERTE_MOTS_CLES', 'DESTINATION', 'BUREAU_FRONTIERE', 'TYPE_REGIME',
            'REGIME_DOUANIER', 'REGIME_FISCAL_CODE', 'STATUT_BAE', 'CODE_TAXE',
            'LIBELLE_TAXE', 'NOM_NAVIRE', 'DATE_ARRIVEE', 'DATE_EMBARQUEMENT'
        ]
        
        # Filtrer les features qui existent dans les donn√©es
        numeric_features = [col for col in numeric_features if col in X_sample.columns]
        categorical_features = [col for col in categorical_features if col in X_sample.columns]
        
        # Obtenir les noms des features cat√©gorielles apr√®s OneHotEncoder
        categorical_transformer = preprocessor.named_transformers_['cat']
        if hasattr(categorical_transformer.named_steps['onehot'], 'get_feature_names_out'):
            cat_feature_names = categorical_transformer.named_steps['onehot'].get_feature_names_out(categorical_features)
        else:
            # Fallback si get_feature_names_out n'est pas disponible
            cat_feature_names = [f"{col}_{i}" for col in categorical_features for i in range(10)]  # Approximation
        
        # Combiner tous les noms de features
        all_feature_names = numeric_features + list(cat_feature_names)
        
        return all_feature_names

    def _generate_shap_analysis(self, best_model, splits):
        """G√©n√©rer l'analyse SHAP pour le meilleur mod√®le"""
        logger.info("üìä G√©n√©ration de l'analyse SHAP...")
        
        if not SHAP_AVAILABLE:
            logger.warning("‚ö†Ô∏è SHAP non disponible - analyse SHAP ignor√©e")
            return
        
        try:
            # Pr√©parer les donn√©es pour SHAP
            X_test_processed = best_model.named_steps['preprocessor'].transform(splits['X_test'])
            
            # Obtenir les noms des features apr√®s preprocessing
            feature_names = self._get_feature_names_after_preprocessing(best_model, splits['X_test'])
            
            # Cr√©er un explainer SHAP
            explainer = shap.TreeExplainer(best_model.named_steps['classifier'])
            shap_values = explainer.shap_values(X_test_processed)
            
            # Si le mod√®le est binaire, prendre les valeurs pour la classe positive
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            
            # Cr√©er un DataFrame avec les noms des features
            X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)
            
            # Graphique d'importance des features
            plt.figure(figsize=(12, 10))
            shap.summary_plot(shap_values, X_test_df, max_display=20, show=False)
            plt.title('SHAP Feature Importance - Top 20 Features (Chapitre 30)', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(self.results_path / 'shap_summary_plot_20.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # Graphique des valeurs SHAP
            plt.figure(figsize=(12, 10))
            shap.summary_plot(shap_values, X_test_df, plot_type="bar", max_display=20, show=False)
            plt.title('SHAP Feature Values - Top 20 Features (Chapitre 30)', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(self.results_path / 'shap_feature_importance_20.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("‚úÖ Analyse SHAP g√©n√©r√©e")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration SHAP: {e}")

    def _generate_yaml_report(self, results, best_model_name):
        """G√©n√©rer le rapport YAML"""
        logger.info("üìä G√©n√©ration du rapport YAML...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'chapter': '30',
            'data_source': str(self.data_path),
            'best_model': best_model_name,
            'models_performance': results,
            'summary': {
                'total_models': len(results),
                'best_auc': max(results[model]['auc'] for model in results),
                'best_f1': max(results[model]['f1'] for model in results),
                'best_accuracy': max(results[model]['accuracy'] for model in results)
            }
        }
        
        with open(self.results_path / 'ml_supervised_report.yaml', 'w') as f:
            yaml.dump(report, f, default_flow_style=False)
        
        logger.info("‚úÖ Rapport YAML g√©n√©r√©")

    def run_complete_ml_pipeline(self):
        """Ex√©cuter le pipeline ML complet"""
        logger.info("üöÄ D√âMARRAGE DU PIPELINE ML COMPLET - CHAPITRE 30")
        logger.info("=" * 60)
        
        try:
            # 1. Charger les donn√©es
            df = self.load_data()
            
            # 2. Pr√©parer les features
            X, y = self.prepare_features(df)
            
            # 3. Cr√©er les splits
            splits = self.create_train_test_splits(X, y)
            
            # 4. Cr√©er le pipeline de preprocessing
            preprocessor = self.create_preprocessing_pipeline(X)
            
            # 5. Entra√Æner les mod√®les (convention train/val/test respect√©e)
            results, trained_models, validation_results = self.train_models(splits, preprocessor)
            
            # 6. Trouver le meilleur mod√®le bas√© sur la validation
            best_model_name = self.find_best_model(validation_results)
            
            # 7. G√©n√©rer tous les r√©sultats
            self.generate_comprehensive_results(results, trained_models, splits, best_model_name)
            
            logger.info("=" * 60)
            logger.info("‚úÖ PIPELINE ML COMPLET TERMIN√â AVEC SUCC√àS")
            logger.info(f"üèÜ Meilleur mod√®le: {best_model_name}")
            logger.info(f"üìÅ R√©sultats sauvegard√©s dans: {self.results_path}")
            logger.info(f"üìÅ Mod√®les sauvegard√©s dans: {self.models_path}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du pipeline ML: {e}")
            raise

def main():
    """Fonction principale"""
    try:
        ml_pipeline = Chap30MLAdvanced()
        ml_pipeline.run_complete_ml_pipeline()
    except Exception as e:
        logger.error(f"‚ùå Erreur dans main: {e}")
        raise

if __name__ == "__main__":
    main()
