#!/usr/bin/env python3
"""
Script pour vÃ©rifier la calibration des meilleurs modÃ¨les de chaque chapitre
"""

import sys
import os
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model_and_data(chapter):
    """Charger le modÃ¨le et les donnÃ©es de test pour un chapitre"""
    logger.info(f"ðŸ” Chargement du chapitre {chapter}...")
    
    # Chemins
    models_dir = f"/Users/macbook/Desktop/inspectia_app/backend/models/chap{chapter}"
    splits_dir = f"/Users/macbook/Desktop/inspectia_app/backend/data/ml_splits/chap{chapter}"
    
    # Charger les donnÃ©es de test
    X_test = pd.read_csv(f"{splits_dir}/X_test.csv")
    y_test = pd.read_csv(f"{splits_dir}/y_test.csv").values.ravel()
    
    # Charger les features
    features_path = f"{models_dir}/features.pkl"
    if os.path.exists(features_path):
        features = joblib.load(features_path)
        X_test = X_test[features]
    
    # Identifier le meilleur modÃ¨le
    if chapter == 30:
        best_model_name = "XGBoost"
    elif chapter == 84:
        best_model_name = "CatBoost"
    elif chapter == 85:
        best_model_name = "XGBoost"
    else:
        raise ValueError(f"Chapitre {chapter} non supportÃ©")
    
    # Charger le modÃ¨le
    model_path = f"{models_dir}/{best_model_name.lower()}_model.pkl"
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ModÃ¨le non trouvÃ©: {model_path}")
    
    model = joblib.load(model_path)
    
    logger.info(f"   âœ… ModÃ¨le {best_model_name} chargÃ©")
    logger.info(f"   âœ… DonnÃ©es de test: {X_test.shape}")
    logger.info(f"   âœ… Taux de fraude: {y_test.mean():.3f}")
    
    return model, X_test, y_test, best_model_name

def check_calibration(model, X_test, y_test, model_name, chapter):
    """VÃ©rifier la calibration d'un modÃ¨le"""
    logger.info(f"ðŸŽ¯ VÃ©rification de la calibration - {model_name} (Chapitre {chapter})")
    
    # Obtenir les probabilitÃ©s prÃ©dites
    try:
        y_prob = model.predict_proba(X_test)[:, 1]
    except Exception as e:
        logger.error(f"   âŒ Erreur prÃ©diction: {e}")
        return None, None, None
    
    # Calculer la calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_test, y_prob, n_bins=10, strategy='uniform'
    )
    
    # Calculer le Brier Score
    brier_score = brier_score_loss(y_test, y_prob)
    
    # Calculer l'ECE (Expected Calibration Error)
    bin_boundaries = np.linspace(0, 1, 11)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)
        prop_in_bin = in_bin.mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = y_test[in_bin].mean()
            avg_confidence_in_bin = y_prob[in_bin].mean()
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
    
    logger.info(f"   ðŸ“Š Brier Score: {brier_score:.4f}")
    logger.info(f"   ðŸ“Š ECE (Expected Calibration Error): {ece:.4f}")
    
    # InterprÃ©tation
    if brier_score < 0.1:
        brier_interpretation = "Excellent"
    elif brier_score < 0.2:
        brier_interpretation = "Bon"
    elif brier_score < 0.3:
        brier_interpretation = "Moyen"
    else:
        brier_interpretation = "Mauvais"
    
    if ece < 0.05:
        ece_interpretation = "Excellent"
    elif ece < 0.1:
        ece_interpretation = "Bon"
    elif ece < 0.2:
        ece_interpretation = "Moyen"
    else:
        ece_interpretation = "Mauvais"
    
    logger.info(f"   ðŸŽ¯ Brier Score: {brier_interpretation}")
    logger.info(f"   ðŸŽ¯ ECE: {ece_interpretation}")
    
    return (fraction_of_positives, mean_predicted_value), {
        'brier_score': brier_score,
        'ece': ece,
        'brier_interpretation': brier_interpretation,
        'ece_interpretation': ece_interpretation
    }

def plot_calibration_curves(results):
    """CrÃ©er un graphique de calibration pour tous les modÃ¨les"""
    plt.figure(figsize=(15, 5))
    
    for i, (chapter, data) in enumerate(results.items(), 1):
        if data is None:
            continue
            
        plt.subplot(1, 3, i)
        
        # Courbe de calibration
        fraction_of_positives, mean_predicted_value = data['calibration_curve']
        plt.plot(mean_predicted_value, fraction_of_positives, "s-", 
                label=f"Chapitre {chapter} ({data['model_name']})", 
                linewidth=2, markersize=6)
        
        # Ligne de calibration parfaite
        plt.plot([0, 1], [0, 1], "k:", label="Calibration parfaite", linewidth=2)
        
        plt.xlabel('ProbabilitÃ© moyenne prÃ©dite')
        plt.ylabel('Fraction de positifs')
        plt.title(f'Chapitre {chapter} - {data["model_name"]}\n'
                 f'Brier: {data["metrics"]["brier_score"]:.4f} ({data["metrics"]["brier_interpretation"]})\n'
                 f'ECE: {data["metrics"]["ece"]:.4f} ({data["metrics"]["ece_interpretation"]})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xlim([0, 1])
        plt.ylim([0, 1])
    
    plt.tight_layout()
    plt.savefig('/Users/macbook/Desktop/inspectia_app/backend/calibration_verification.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    logger.info("ðŸ“Š Graphique de calibration sauvegardÃ©: calibration_verification.png")

def main():
    """Fonction principale"""
    logger.info("ðŸš€ VÃ‰RIFICATION DE LA CALIBRATION DES MEILLEURS MODÃˆLES")
    logger.info("=" * 70)
    
    results = {}
    
    # VÃ©rifier chaque chapitre
    for chapter in [30, 84, 85]:
        try:
            # Charger le modÃ¨le et les donnÃ©es
            model, X_test, y_test, model_name = load_model_and_data(chapter)
            
            # VÃ©rifier la calibration
            calibration_curve_data, metrics = check_calibration(
                model, X_test, y_test, model_name, chapter
            )
            
            if calibration_curve_data is not None:
                results[chapter] = {
                    'model_name': model_name,
                    'calibration_curve': calibration_curve_data,
                    'metrics': metrics
                }
            
            logger.info("")
            
        except Exception as e:
            logger.error(f"âŒ Erreur chapitre {chapter}: {e}")
            results[chapter] = None
    
    # CrÃ©er le graphique de calibration
    if any(data is not None for data in results.values()):
        plot_calibration_curves(results)
    
    # RÃ©sumÃ© final
    logger.info("ðŸ“‹ RÃ‰SUMÃ‰ DE LA CALIBRATION")
    logger.info("=" * 70)
    
    for chapter, data in results.items():
        if data is not None:
            metrics = data['metrics']
            logger.info(f"Chapitre {chapter} ({data['model_name']}):")
            logger.info(f"   Brier Score: {metrics['brier_score']:.4f} ({metrics['brier_interpretation']})")
            logger.info(f"   ECE: {metrics['ece']:.4f} ({metrics['ece_interpretation']})")
            
            # VÃ©rification de la calibration
            if metrics['brier_score'] < 0.1 and metrics['ece'] < 0.05:
                logger.info(f"   âœ… CALIBRATION EXCELLENTE")
            elif metrics['brier_score'] < 0.2 and metrics['ece'] < 0.1:
                logger.info(f"   âœ… CALIBRATION BONNE")
            else:
                logger.info(f"   âš ï¸ CALIBRATION Ã€ AMÃ‰LIORER")
        else:
            logger.info(f"Chapitre {chapter}: âŒ ERREUR")
        
        logger.info("")

if __name__ == "__main__":
    main()
