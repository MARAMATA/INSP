chapter: chap84
created_date: '2025-09-15T17:36:37.077799'
data_hash: a80e0257ad18276d29ed59090a3ea4d13535e030f76fba6584c508b43ac53a46
deployment_requirements:
  dependencies:
  - scikit-learn
  - pandas
  - numpy
  - joblib
  python_version: 3.8+
models_included:
- lightgbm
- xgboost
- catboost
- randomforest
- logisticregression
package_version: '1.0'
validation_results:
  business_thresholds:
    catboost:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - &id001 !!python/object/apply:numpy.dtype
          args:
          - f8
          - false
          - true
          state: !!python/tuple
          - 3
          - <
          - null
          - null
          - null
          - -1
          - -1
          - 0
        - !!binary |
          kML1KFyP0j8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          kML1KFyP0j8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          KVyPwvUo5D8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: &id003 []
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - &id002 !!python/name:numpy.ndarray ''
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/LO5vsprZ7z8s7m+ymtnvPyzub7Ka2e8/LO5vsprZ7z8s7m+y
                  mtnvPyzub7Ka2e8/LO5vsprZ7z8s7m+ymtnvPyzub7Ka2e8/LO5vsprZ7z8s7m+ymtnvPyzub7Ka
                  2e8/LO5vsprZ7z8s7m+ymtnvPyzub7Ka2e8/LO5vsprZ7z8s7m+ymtnvPyzub7Ka2e8/LO5vsprZ
                  7z8JlsG7VdzvPwmWwbtV3O8/CZbBu1Xc7z8JlsG7VdzvPwmWwbtV3O8/25a5rFLc7z/blrmsUtzv
                  P9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/
                  25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/25a5rFLc7z/b
                  lrmsUtzvP9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP9uW
                  uaxS3O8/25a5rFLc7z/blrmsUtzvP9uWuaxS3O8/25a5rFLc7z/blrmsUtzvP2dciI339O8/Z1yI
                  jff07z9nXIiN9/TvP2dciI339O8/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/Dn+C1j797z8Of4LWPv3v
                  Pw5/gtY+/e8/Dn+C1j797z8Of4LWPv3vPw5/gtY+/e8/Dn+C1j797z8Of4LWPv3vPw5/gtY+/e8/
                  Dn+C1j797z8Of4LWPv3vPw5/gtY+/e8/Dn+C1j797z8Of4LWPv3vPw5/gtY+/e8/Dn+C1j797z8O
                  f4LWPv3vPw5/gtY+/e8/Dn+C1j797z8Of4LWPv3vPw5/gtY+/e8/Dn+C1j797z8Of4LWPv3vPw5/
                  gtY+/e8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4E
                  rX367z8b/gStffrvPxv+BK19+u8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4ErX367z8b/gSt
                  ffrvPxv+BK19+u8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4ErX367z8b/gStffrvPxv+BK19
                  +u8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4ErX367z8b/gStffrvPxv+BK19+u8/G/4ErX36
                  7z8b/gStffrvP234E7T26e8/bfgTtPbp7z9t+BO09unvP234E7T26e8/Ruk7HOS97z9G6Tsc5L3v
                  P0bpOxzkve8/Ruk7HOS97z9G6Tsc5L3vP0bpOxzkve8/Ruk7HOS97z9G6Tsc5L3vP0bpOxzkve8/
                  Ruk7HOS97z9G6Tsc5L3vP0bpOxzkve8/Ruk7HOS97z9G6Tsc5L3vP0bpOxzkve8/Ruk7HOS97z9G
                  6Tsc5L3vP0bpOxzkve8/Ruk7HOS97z9G6Tsc5L3vP0bpOxzkve8/Ruk7HOS97z9G6Tsc5L3vP0bp
                  Oxzkve8/3Ft90WLr7z/cW33RYuvvP9xbfdFi6+8/3Ft90WLr7z/cW33RYuvvP9xbfdFi6+8/3Ft9
                  0WLr7z/cW33RYuvvP9xbfdFi6+8/3Ft90WLr7z/cW33RYuvvP9xbfdFi6+8/3Ft90WLr7z/cW33R
                  YuvvP9xbfdFi6+8/3Ft90WLr7z/cW33RYuvvP9xbfdFi6+8/3Ft90WLr7z8PtNTNwezvPw+01M3B
                  7O8/D7TUzcHs7z8PtNTNwezvPw+01M3B7O8/EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr
                  7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr7z8RYxIMYevv
                  PxFjEgxh6+8/EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/
                  EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr7z8RYxIMYevvPxFjEgxh6+8/EWMSDGHr7z8R
                  YxIMYevvPxFjEgxh6+8/EWMSDGHr7z8RYxIMYevvP8dpKy527+8/x2krLnbv7z/HaSsudu/vP8dp
                  Ky527+8/XJrrxc/e7z9cmuvFz97vP1ya68XP3u8/XJrrxc/e7z9cmuvFz97vP1ya68XP3u8/XJrr
                  xc/e7z9cmuvFz97vP1ya68XP3u8/XJrrxc/e7z9cmuvFz97vP1ya68XP3u8/XJrrxc/e7z9cmuvF
                  z97vP1ya68XP3u8/XJrrxc/e7z9cmuvFz97vP1ya68XP3u8/XJrrxc/e7z9cmuvFz97vP1ya68XP
                  3u8/XJrrxc/e7z9cmuvFz97vP1ya68XP3u8/AAAAAAAAM0AAAAAAAAAzQAAAAAAAADNAAAAAAAAA
                  M0AAAAAAAAAzQAAAAAAAADNAAAAAAAAAM0AAAAAAAAAzQAAAAAAAADNAAAAAAAAAM0AAAAAAAAAz
                  QAAAAAAAADNAAAAAAAAAM0AAAAAAAAAzQAAAAAAAADNAAAAAAAAAM0AAAAAAAAAzQAAAAAAAADNA
                  AAAAAAAAM0AAAAAAAAAyQAAAAAAAADJAAAAAAAAAMkAAAAAAAAAyQAAAAAAAADJAAAAAAAAAN0AA
                  AAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAA
                  AAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAAADdAAAAA
                  AAAAN0AAAAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAA
                  AAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAAADdAAAAAAAAAN0AAAAAAAAA3QAAAAAAA
                  AEZAAAAAAAAARkAAAAAAAABGQAAAAAAAAEZAAAAAAAAAXkAAAAAAAABeQAAAAAAAAF5AAAAAAAAA
                  XkAAAAAAAABeQAAAAAAAAF5AAAAAAAAAXkAAAAAAAABeQAAAAAAAAF5AAAAAAAAAXkAAAAAAAABe
                  QAAAAAAAAF5AAAAAAAAAXkAAAAAAAABeQAAAAAAAAF5AAAAAAAAAXkAAAAAAAABeQAAAAAAAAF5A
                  AAAAAAAAXkAAAAAAAABeQAAAAAAAAF5AAAAAAAAAXkAAAAAAAABeQAAAAAAAAF5AcUkgsSCcQj9x
                  SSCxIJxCP3FJILEgnEI/cUkgsSCcQj9xSSCxIJxCP3FJILEgnEI/cUkgsSCcQj9xSSCxIJxCP3FJ
                  ILEgnEI/cUkgsSCcQj9xSSCxIJxCP3FJILEgnEI/cUkgsSCcQj9xSSCxIJxCP3FJILEgnEI/cUkg
                  sSCcQj9xSSCxIJxCP3FJILEgnEI/cUkgsSCcQj9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI2
                  1UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbV
                  R0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/V43CNtVH
                  QT9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdB
                  P1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/V43CNtVHQT9XjcI21UdBP1eNwjbVR0E/
                  V43CNtVHQT9XjcI21UdBP6bB26W3RCU/psHbpbdEJT+mwdult0QlP6bB26W3RCU/AAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAMpMH7EsJNj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJNj8ykwfsSwk2PzKTB+xL
                  CTY/MpMH7EsJNj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJNj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJ
                  Nj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJNj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJNj8ykwfsSwk2
                  PzKTB+xLCTY/MpMH7EsJNj8ykwfsSwk2PzKTB+xLCTY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/
                  MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8y
                  kwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKT
                  B+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH
                  7EsJRj8ykwfsSwlGPzKTB+xLCUY/MpMH7EsJRj8ykwfsSwlGPzKTB+xLCWY/MpMH7EsJZj8ykwfs
                  SwlmPzKTB+xLCWY/Zq4F8fiGgD9mrgXx+IaAP2auBfH4hoA/Zq4F8fiGgD9mrgXx+IaAP2auBfH4
                  hoA/Zq4F8fiGgD9mrgXx+IaAP2auBfH4hoA/Zq4F8fiGgD9mrgXx+IaAP2auBfH4hoA/Zq4F8fiG
                  gD9mrgXx+IaAP2auBfH4hoA/Zq4F8fiGgD9mrgXx+IaAP2auBfH4hoA/Zq4F8fiGgD9mrgXx+IaA
                  P2auBfH4hoA/Zq4F8fiGgD9mrgXx+IaAP2auBfH4hoA/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - &id004 !!python/object/apply:numpy.dtype
                  args:
                  - i8
                  - false
                  - true
                  state: !!python/tuple
                  - 3
                  - <
                  - null
                  - null
                  - null
                  - -1
                  - -1
                  - 0
                - false
                - !!binary |
                  nQsAAAAAAACdCwAAAAAAAJ0LAAAAAAAAnQsAAAAAAACdCwAAAAAAAJ0LAAAAAAAAnQsAAAAAAACd
                  CwAAAAAAAJ0LAAAAAAAAnQsAAAAAAACdCwAAAAAAAJ0LAAAAAAAAnQsAAAAAAACdCwAAAAAAAJ0L
                  AAAAAAAAnQsAAAAAAACdCwAAAAAAAJ0LAAAAAAAAnQsAAAAAAACdCwAAAAAAAJ0LAAAAAAAAnQsA
                  AAAAAACdCwAAAAAAAJ0LAAAAAAAAnAsAAAAAAACcCwAAAAAAAJwLAAAAAAAAnAsAAAAAAACcCwAA
                  AAAAAJwLAAAAAAAAnAsAAAAAAACcCwAAAAAAAJwLAAAAAAAAnAsAAAAAAACcCwAAAAAAAJwLAAAA
                  AAAAnAsAAAAAAACcCwAAAAAAAJwLAAAAAAAAnAsAAAAAAACcCwAAAAAAAJwLAAAAAAAAnAsAAAAA
                  AACcCwAAAAAAAJwLAAAAAAAAnAsAAAAAAACcCwAAAAAAAJwLAAAAAAAAnAsAAAAAAACcCwAAAAAA
                  AJwLAAAAAAAAnAsAAAAAAACcCwAAAAAAAJYLAAAAAAAAlgsAAAAAAACWCwAAAAAAAJYLAAAAAAAA
                  hgsAAAAAAACGCwAAAAAAAIYLAAAAAAAAhgsAAAAAAACGCwAAAAAAAIYLAAAAAAAAhgsAAAAAAACG
                  CwAAAAAAAIYLAAAAAAAAhgsAAAAAAACGCwAAAAAAAIYLAAAAAAAAhgsAAAAAAACGCwAAAAAAAIYL
                  AAAAAAAAhgsAAAAAAACGCwAAAAAAAIYLAAAAAAAAhgsAAAAAAACGCwAAAAAAAIYLAAAAAAAAhgsA
                  AAAAAACGCwAAAAAAAIYLAAAAAAAADgAAAAAAAAAOAAAAAAAAAA4AAAAAAAAADgAAAAAAAAAOAAAA
                  AAAAAA4AAAAAAAAADgAAAAAAAAAOAAAAAAAAAA4AAAAAAAAADgAAAAAAAAAOAAAAAAAAAA4AAAAA
                  AAAADgAAAAAAAAAOAAAAAAAAAA4AAAAAAAAADgAAAAAAAAAOAAAAAAAAAA4AAAAAAAAADgAAAAAA
                  AAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAA
                  AA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAA
                  DQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAAN
                  AAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAA0A
                  AAAAAAAADQAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAANAAAAAAAAAAQAAAAAAAAABAAA
                  AAAAAAAEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAABAAAAAAAA
                  AAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAA
                  AQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAAB
                  AAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEA
                  AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAA
                  AAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAA
                  AAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAA
                  AAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAA
                  AAACAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAAGAAAAAAAAAAYAAAAAAAA
                  ABgAAAAAAAAAGAAAAAAAAAAYAAAAAAAAABgAAAAAAAAAGAAAAAAAAAAYAAAAAAAAABgAAAAAAAAA
                  GAAAAAAAAAAYAAAAAAAAABgAAAAAAAAAGAAAAAAAAAAYAAAAAAAAABgAAAAAAAAAGAAAAAAAAAAY
                  AAAAAAAAABgAAAAAAAAAGAAAAAAAAAAYAAAAAAAAABgAAAAAAAAAGAAAAAAAAAAYAAAAAAAAABgA
                  AAAAAAAAPWAAAAAAAAA9YAAAAAAAAD1gAAAAAAAAPWAAAAAAAAA9YAAAAAAAAD1gAAAAAAAAPWAA
                  AAAAAAA9YAAAAAAAAD1gAAAAAAAAPWAAAAAAAAA9YAAAAAAAAD1gAAAAAAAAPWAAAAAAAAA9YAAA
                  AAAAAD1gAAAAAAAAPWAAAAAAAAA9YAAAAAAAAD1gAAAAAAAAPWAAAAAAAAA+YAAAAAAAAD5gAAAA
                  AAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAA
                  AAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAA
                  AD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAA
                  PmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+
                  YAAAAAAAAD5gAAAAAAAAPmAAAAAAAAA+YAAAAAAAAEdgAAAAAAAAR2AAAAAAAABHYAAAAAAAAEdg
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - &id005 !!python/name:pandas.core.indexes.base.Index ''
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - &id006 !!python/object/apply:numpy.dtype
                    args:
                    - O8
                    - false
                    - true
                    state: !!python/tuple
                    - 3
                    - '|'
                    - null
                    - null
                    - null
                    - -1
                    - -1
                    - 63
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - &id007 !!python/name:pandas.core.indexes.range.RangeIndex ''
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
    lightgbm:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADw
                  PwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/
                  AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8A
                  AAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAA
                  AAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAA
                  AAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/feVFdt+y7z995UV237Lv
                  P33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/
                  feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z99
                  5UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33l
                  RXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVF
                  dt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV2
                  37LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbf
                  su8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y
                  7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237Lv
                  P33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/
                  feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z99
                  5UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33l
                  RXbfsu8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2v
                  CUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8J
                  QdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB
                  2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ
                  7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnv
                  P5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/
                  lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+U
                  va8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9
                  rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2v
                  CUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8J
                  QdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB
                  2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACA
                  YUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBh
                  QAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFA
                  AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAA
                  AAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAA
                  AAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAA
                  AACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAA
                  AIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAA
                  gGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACA
                  YUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBh
                  QAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFA
                  AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAzKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4i
                  SIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJI
                  gz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiD
                  P8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
                  zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/M
                  oIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yg
                  hm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCG
                  biJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZu
                  IkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4i
                  SIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJI
                  gz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiD
                  P8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  ggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACC
                  CwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIIL
                  AAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAA
                  AAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAA
                  AAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAA
                  AACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAA
                  AIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAA
                  ggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACC
                  CwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIIL
                  AAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHAAAAAAAAAAcAAAAAAAA
                  ABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAA
                  HAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAc
                  AAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAA
                  AAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAA
                  AAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAA
                  AAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAA
                  AAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAA
                  ABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAA
                  HAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAc
                  AAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAA
                  AABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAA
                  AEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
                  S2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABL
                  YAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtg
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
    logisticregression:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4noUrkfhyj8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/cTrR9pyt7z9xOtH2nK3vP3E60facre8/cTrR9pyt7z9xOtH2
                  nK3vP3E60facre8/cTrR9pyt7z9xOtH2nK3vP3E60facre8/+SEMcpbs7z9HqczpH/LvPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADw
                  PwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/
                  AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8A
                  AAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAA
                  AAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAA
                  AAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/Hmy0mCfG7z8ebLSYJ8bv
                  Px5stJgnxu8/Hmy0mCfG7z8ebLSYJ8bvPx5stJgnxu8/Hmy0mCfG7z8ebLSYJ8bvPx5stJgnxu8/
                  b2bDn6C17z9vZsOfoLXvP29mw5+gte8/b2bDn6C17z9vZsOfoLXvP29mw5+gte8/b2bDn6C17z9v
                  ZsOfoLXvP29mw5+gte8/b2bDn6C17z9vZsOfoLXvP29mw5+gte8/b2bDn6C17z995UV237LvP33l
                  RXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVF
                  dt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV2
                  37LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbf
                  su8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y
                  7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237Lv
                  P33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/
                  feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z99
                  5UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33l
                  RXbfsu8/GM/JiN257z8Yz8mI3bnvPxjPyYjdue8/GM/JiN257z8Yz8mI3bnvPxjPyYjdue8/GM/J
                  iN257z8Yz8mI3bnvPxjPyYjdue8/EH3ezAPR7z+9Q1CFw9PvP15C4eak2u8/XkLh5qTa7z9eQuHm
                  pNrvP15C4eak2u8/XkLh5qTa7z9eQuHmpNrvP15C4eak2u8/XkLh5qTa7z9eQuHmpNrvP15C4eak
                  2u8/XkLh5qTa7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ
                  7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnv
                  P5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/
                  lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+U
                  va8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9
                  rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2v
                  CUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8J
                  QdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB
                  2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/AAAAAADgYEAAAAAAAOBgQAAAAAAA4GBAAAAAAADg
                  YEAAAAAAAOBgQAAAAAAA4GBAAAAAAADgYEAAAAAAAOBgQAAAAAAA4GBAAAAAAADAYUAAAAAAAIBh
                  QAAAAAAA4GBAAAAAAADgYEAAAAAAAOBgQAAAAAAA4GBAAAAAAADgYEAAAAAAAOBgQAAAAAAA4GBA
                  AAAAAADgYEAAAAAAAOBgQAAAAAAA4GBAAAAAAADgYEAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAA
                  AAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAA
                  AAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAA
                  AACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAA
                  AIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAA
                  gGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACA
                  YUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBh
                  QAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFA
                  AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAjAV+K2zwUz+M
                  BX4rbPBTP4wFfits8FM/jAV+K2zwUz+MBX4rbPBTP4wFfits8FM/jAV+K2zwUz+MBX4rbPBTP4wF
                  fits8FM/cUkgsSCcMj8QslKP5ZUqPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAMvHJpTPsfD8y8cmlM+x8PzLxyaUz7Hw/MvHJpTPsfD8y8cmlM+x8PzLxyaUz
                  7Hw/MvHJpTPsfD8y8cmlM+x8PzLxyaUz7Hw/MmQmD9iXgj8yZCYP2JeCPzJkJg/Yl4I/MmQmD9iX
                  gj8yZCYP2JeCPzJkJg/Yl4I/MmQmD9iXgj8yZCYP2JeCPzJkJg/Yl4I/MmQmD9iXgj8yZCYP2JeC
                  PzJkJg/Yl4I/MmQmD9iXgj/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
                  zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/M
                  oIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yg
                  hm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCG
                  biJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZu
                  IkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4i
                  SIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJI
                  gz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiD
                  P8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  iQsAAAAAAACJCwAAAAAAAIkLAAAAAAAAiQsAAAAAAACJCwAAAAAAAIkLAAAAAAAAiQsAAAAAAACJ
                  CwAAAAAAAIkLAAAAAAAAgwsAAAAAAACDCwAAAAAAAIMLAAAAAAAAgwsAAAAAAACDCwAAAAAAAIML
                  AAAAAAAAgwsAAAAAAACDCwAAAAAAAIMLAAAAAAAAgwsAAAAAAACDCwAAAAAAAIMLAAAAAAAAgwsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAA
                  AAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAA
                  AAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAA
                  AACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAA
                  AIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAA
                  ggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACC
                  CwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIIL
                  AAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAHgAAAAAAAAAeAAAAAAAAAB4AAAAAAAAAHgAAAAAAAAAeAAAA
                  AAAAAB4AAAAAAAAAHgAAAAAAAAAeAAAAAAAAAB4AAAAAAAAABwAAAAAAAAAFAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFQAAAAAAAAAVAAAAAAAA
                  ABUAAAAAAAAAFQAAAAAAAAAVAAAAAAAAABUAAAAAAAAAFQAAAAAAAAAVAAAAAAAAABUAAAAAAAAA
                  GwAAAAAAAAAbAAAAAAAAABsAAAAAAAAAGwAAAAAAAAAbAAAAAAAAABsAAAAAAAAAGwAAAAAAAAAb
                  AAAAAAAAABsAAAAAAAAAGwAAAAAAAAAbAAAAAAAAABsAAAAAAAAAGwAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAA
                  AAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAA
                  AAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAA
                  AAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAA
                  AAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAA
                  ABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAA
                  HAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAc
                  AAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAALWAAAAAAAAAtYAAAAAAAAC1gAAAAAAAALWAAAAAAAAAtYAAAAAAAAC1gAAAAAAAALWAA
                  AAAAAAAtYAAAAAAAAC1gAAAAAAAARGAAAAAAAABGYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAA
                  AABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAA
                  AEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
                  S2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABL
                  YAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtg
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
    randomforest:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          cT0K16NwzT8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          uR6F61G4vj8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA0D8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/zmJ01qUb4T/OYnTWpRvhP0Yb8tHwyOE/Rhvy0fDI4T+8BL6G
                  NcjhP7wEvoY1yOE/mf0wspnA4z/WsB6mYvHjP0Xio6Pd8eM/ReKjo93x4z9F4qOj3fHjP5nH89by
                  SeQ/6jMshXxK5D+Z176HutDnP5nXvoe60Oc/5Pj+u8ES6T/k+P67wRLpP+T4/rvBEuk/zARWzykS
                  6T/MBFbPKRLpP8wEVs8pEuk/zARWzykS6T/MBFbPKRLpP8wEVs8pEuk/zARWzykS6T/MBFbPKRLp
                  P8wEVs8pEuk/zARWzykS6T/MBFbPKRLpP8wEVs8pEuk/10jv+g1K6z/5yQ32FwztP/nJDfYXDO0/
                  +ckN9hcM7T/5yQ32FwztP/nJDfYXDO0/+ckN9hcM7T/5yQ32FwztP/nJDfYXDO0/+ckN9hcM7T/5
                  yQ32FwztP+iWRvBsC+0/6JZG8GwL7T/olkbwbAvtP+iWRvBsC+0/vaZQ1msK7T+9plDWawrtP72m
                  UNZrCu0/vaZQ1msK7T+9plDWawrtP72mUNZrCu0/A6vTubZF7T89WRxiNAPuPz1ZHGI0A+4/PVkc
                  YjQD7j89WRxiNAPuP/SH7Uvkce4/9IftS+Rx7j/0h+1L5HHuP/SH7Uvkce4/9IftS+Rx7j/0h+1L
                  5HHuP/SH7Uvkce4/9IftS+Rx7j/0h+1L5HHuP/SH7Uvkce4/9IftS+Rx7j/0h+1L5HHuP/SH7Uvk
                  ce4/9IftS+Rx7j9N9Udv4K3uP031R2/gre4/TfVHb+Ct7j9N9Udv4K3uP031R2/gre4/TfVHb+Ct
                  7j9N9Udv4K3uP3cpK99nAu8/dykr32cC7z93KSvfZwLvP3cpK99nAu8/785Sseb87T/vzlKx5vzt
                  Pxu6iSBNwO0/G7qJIE3A7T8oOQz3i73tPyg5DPeLve0/K7xVkvJR7D/PsHOg5DDsP9wv9nYjLuw/
                  3C/2diMu7D/cL/Z2Iy7sP2eUvsNB3us/dBNBmoDb6z8907uzVKzpPz3Tu7NUrOk/lKkpkiEz6T+U
                  qSmSITPpP5SpKZIhM+k/oSisaGAw6T+hKKxoYDDpP6EorGhgMOk/oSisaGAw6T+hKKxoYDDpP6Eo
                  rGhgMOk/oSisaGAw6T+hKKxoYDDpP6EorGhgMOk/oSisaGAw6T+hKKxoYDDpP6EorGhgMOk/g3h8
                  Qwgw5z/FmvM3VR/mP8Wa8zdVH+Y/xZrzN1Uf5j/FmvM3VR/mP8Wa8zdVH+Y/xZrzN1Uf5j/FmvM3
                  VR/mP8Wa8zdVH+Y/xZrzN1Uf5j/FmvM3VR/mP+CY+OTSGeY/4Jj45NIZ5j/gmPjk0hnmP+CY+OTS
                  GeY/CRaAaI8R5j8JFoBojxHmPwkWgGiPEeY/CRaAaI8R5j8JFoBojxHmPwkWgGiPEeY/rQqedoHw
                  5T+CNAG31/XkP4I0AbfX9eQ/gjQBt9f15D+CNAG31/XkPx2G+8XebuQ/HYb7xd5u5D8dhvvF3m7k
                  Px2G+8XebuQ/HYb7xd5u5D8dhvvF3m7kPx2G+8XebuQ/HYb7xd5u5D8dhvvF3m7kPx2G+8XebuQ/
                  HYb7xd5u5D8dhvvF3m7kPx2G+8XebuQ/HYb7xd5u5D+DSZtmlL7jP4NJm2aUvuM/g0mbZpS+4z+D
                  SZtmlL7jP4NJm2aUvuM/g0mbZpS+4z+DSZtmlL7jP977Zxy13OI/3vtnHLXc4j/e+2cctdziP977
                  Zxy13OI/r8YqeGfJ5T+vxip4Z8nlP+9PBiwNQ+Y/708GLA1D5j8xa0AEtUHmPzFrQAS1QeY/Rhdd
                  dNFF5z9SgGV9OlznP2yGqa6cW+c/bIaprpxb5z9shqmunFvnP10a05Fpe+c/LkwMXct65z8IwkVD
                  mrXoPwjCRUOateg/nviDOuci6T+e+IM65yLpP574gzrnIuk/LLQ2Bzwh6T8stDYHPCHpPyy0Ngc8
                  Iek/LLQ2Bzwh6T8stDYHPCHpPyy0Ngc8Iek/LLQ2Bzwh6T8stDYHPCHpPyy0Ngc8Iek/LLQ2Bzwh
                  6T8stDYHPCHpPyy0Ngc8Iek/fFEd0mES6T8Vr0cjwh3pPxWvRyPCHek/Fa9HI8Id6T8Vr0cjwh3p
                  PxWvRyPCHek/Fa9HI8Id6T8Vr0cjwh3pPxWvRyPCHek/Fa9HI8Id6T8Vr0cjwh3pP14B1PP0Gek/
                  XgHU8/QZ6T9eAdTz9BnpP14B1PP0Gek/qJGnxz8U6T+okafHPxTpP6iRp8c/FOk/qJGnxz8U6T+o
                  kafHPxTpP6iRp8c/FOk/I8YH1rYU6T/6QBpOwa7oP/pAGk7Brug/+kAaTsGu6D/6QBpOwa7oP96K
                  TFU0dOg/3opMVTR06D/eikxVNHToP96KTFU0dOg/3opMVTR06D/eikxVNHToP96KTFU0dOg/3opM
                  VTR06D/eikxVNHToP96KTFU0dOg/3opMVTR06D/eikxVNHToP96KTFU0dOg/3opMVTR06D/eBeYp
                  tAboP94F5im0Bug/3gXmKbQG6D/eBeYptAboP94F5im0Bug/3gXmKbQG6D/eBeYptAboPy3xeinX
                  dOc/LfF6Kdd05z8t8Xop13TnPy3xeinXdOc/AAAAAABCqkAAAAAAAEKqQAAAAAAAbqlAAAAAAABu
                  qUAAAAAAAHipQAAAAAAAeKlAAAAAAAAcqkAAAAAAADSqQAAAAAAAPKpAAAAAAAA8qkAAAAAAADyq
                  QAAAAAAArqpAAAAAAAC2qkAAAAAAAGCtQAAAAAAAYK1AAAAAAAC+rUAAAAAAAL6tQAAAAAAAvq1A
                  AAAAAADIrUAAAAAAAMitQAAAAAAAyK1AAAAAAADIrUAAAAAAAMitQAAAAAAAyK1AAAAAAADIrUAA
                  AAAAAMitQAAAAAAAyK1AAAAAAADIrUAAAAAAAMitQAAAAAAAyK1AAAAAAABzsUAAAAAAAL+yQAAA
                  AAAAv7JAAAAAAAC/skAAAAAAAL+yQAAAAAAAv7JAAAAAAAC/skAAAAAAAL+yQAAAAAAAv7JAAAAA
                  AAC/skAAAAAAAL+yQAAAAAAAybJAAAAAAADJskAAAAAAAMmyQAAAAAAAybJAAAAAAADYskAAAAAA
                  ANiyQAAAAAAA2LJAAAAAAADYskAAAAAAANiyQAAAAAAA2LJAAAAAAAABs0AAAAAAAIu0QAAAAAAA
                  i7RAAAAAAACLtEAAAAAAAIu0QAAAAAAAYLVAAAAAAABgtUAAAAAAAGC1QAAAAAAAYLVAAAAAAABg
                  tUAAAAAAAGC1QAAAAAAAYLVAAAAAAABgtUAAAAAAAGC1QAAAAAAAYLVAAAAAAABgtUAAAAAAAGC1
                  QAAAAAAAYLVAAAAAAABgtUAAAAAAAI62QAAAAAAAjrZAAAAAAACOtkAAAAAAAI62QAAAAAAAjrZA
                  AAAAAACOtkAAAAAAAI62QAAAAAAAEbhAAAAAAAARuEAAAAAAABG4QAAAAAAAEbhAJJE4i6YxuT8k
                  kTiLpjG5P7djuixn87Y/t2O6LGfztj+3Y7osZ/O2P7djuixn87Y/UB4rWMLysD/G90cKJnOwP048
                  U3N9cLA/TjxTc31wsD9OPFNzfXCwP/iVZR4TDa8/CB988MEHrz8S+tAPBwihPxL60A8HCKE/NTMX
                  Elbgmj81MxcSVuCaPzUzFxJW4Jo/NTMXElbgmj81MxcSVuCaPzUzFxJW4Jo/NTMXElbgmj81MxcS
                  VuCaPzUzFxJW4Jo/NTMXElbgmj81MxcSVuCaPzUzFxJW4Jo/NTMXElbgmj81MxcSVuCaPzUzFxJW
                  4Jo/ZVUD3drnjj8ZaWjuGV2BPxlpaO4ZXYE/GWlo7hldgT8ZaWjuGV2BPxlpaO4ZXYE/GWlo7hld
                  gT8ZaWjuGV2BPxlpaO4ZXYE/GWlo7hldgT8ZaWjuGV2BPxlpaO4ZXYE/GWlo7hldgT8ZaWjuGV2B
                  PxlpaO4ZXYE/GWlo7hldgT8ZaWjuGV2BPxlpaO4ZXYE/GWlo7hldgT8ZaWjuGV2BPxlpaO4ZXYE/
                  cjMymgCSfz8peScVQW91Pyl5JxVBb3U/KXknFUFvdT8peScVQW91P8CIsCsTHnA/wIiwKxMecD/A
                  iLArEx5wP8CIsCsTHnA/wIiwKxMecD/AiLArEx5wP8CIsCsTHnA/wIiwKxMecD/AiLArEx5wP8CI
                  sCsTHnA/wIiwKxMecD/AiLArEx5wP8CIsCsTHnA/wIiwKxMecD8JQ7uw0kBqPwlDu7DSQGo/CUO7
                  sNJAaj8JQ7uw0kBqPwlDu7DSQGo/CUO7sNJAaj8JQ7uw0kBqP3FJILEgnGI/cUkgsSCcYj9xSSCx
                  IJxiP3FJILEgnGI/hohpdcoYsD+GiGl1yhiwPywvsvuW/bE/LC+y+5b9sT+/Np5HoBOyP782nkeg
                  E7I/pR5SbWtwvT+LeWL82ni+Px6BTkjkjr4/HoFOSOSOvj8egU5I5I6+P2auBfH4hsA/L7L7lv2R
                  wD8MsxAxrU7JPwyzEDGtTsk/sllZt3kzyz+yWVm3eTPLP7JZWbd5M8s/e11PXX4+yz97XU9dfj7L
                  P3tdT11+Pss/e11PXX4+yz97XU9dfj7LP3tdT11+Pss/e11PXX4+yz97XU9dfj7LP3tdT11+Pss/
                  e11PXX4+yz97XU9dfj7LP3tdT11+Pss/+g4Hee+f0T91yhiQVcHTP3XKGJBVwdM/dcoYkFXB0z91
                  yhiQVcHTP3XKGJBVwdM/dcoYkFXB0z91yhiQVcHTP3XKGJBVwdM/dcoYkFXB0z91yhiQVcHTPz/O
                  DjZazNM/P84ONlrM0z8/zg42WszTPz/ODjZazNM/7dP/LuHc0z/t0/8u4dzTP+3T/y7h3NM/7dP/
                  LuHc0z/t0/8u4dzTP+3T/y7h3NM/p+rDEv0e1D/8lv2RUBTWP/yW/ZFQFNY//Jb9kVAU1j/8lv2R
                  UBTWP8fzCHRCItc/x/MIdEIi1z/H8wh0QiLXP8fzCHRCItc/x/MIdEIi1z/H8wh0QiLXP8fzCHRC
                  Itc/x/MIdEIi1z/H8wh0QiLXP8fzCHRCItc/x/MIdEIi1z/H8wh0QiLXP8fzCHRCItc/x/MIdEIi
                  1z/6bMky14LYP/psyTLXgtg/+mzJMteC2D/6bMky14LYP/psyTLXgtg/+mzJMteC2D/6bMky14LY
                  P0MIMMeVRto/Qwgwx5VG2j9DCDDHlUbaP0MIMMeVRto/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  4woAAAAAAADjCgAAAAAAAM0KAAAAAAAAzQoAAAAAAADMCgAAAAAAAMwKAAAAAAAASAoAAAAAAAA8
                  CgAAAAAAADsKAAAAAAAAOwoAAAAAAAA7CgAAAAAAAB4KAAAAAAAAHQoAAAAAAABSCQAAAAAAAFIJ
                  AAAAAAAAJgkAAAAAAAAmCQAAAAAAACYJAAAAAAAAJQkAAAAAAAAlCQAAAAAAACUJAAAAAAAAJQkA
                  AAAAAAAlCQAAAAAAACUJAAAAAAAAJQkAAAAAAAAlCQAAAAAAACUJAAAAAAAAJQkAAAAAAAAlCQAA
                  AAAAACUJAAAAAAAAawgAAAAAAAAICAAAAAAAAAgIAAAAAAAACAgAAAAAAAAICAAAAAAAAAgIAAAA
                  AAAACAgAAAAAAAAICAAAAAAAAAgIAAAAAAAACAgAAAAAAAAICAAAAAAAAAYIAAAAAAAABggAAAAA
                  AAAGCAAAAAAAAAYIAAAAAAAAAwgAAAAAAAADCAAAAAAAAAMIAAAAAAAAAwgAAAAAAAADCAAAAAAA
                  AAMIAAAAAAAA9wcAAAAAAACcBwAAAAAAAJwHAAAAAAAAnAcAAAAAAACcBwAAAAAAAGsHAAAAAAAA
                  awcAAAAAAABrBwAAAAAAAGsHAAAAAAAAawcAAAAAAABrBwAAAAAAAGsHAAAAAAAAawcAAAAAAABr
                  BwAAAAAAAGsHAAAAAAAAawcAAAAAAABrBwAAAAAAAGsHAAAAAAAAawcAAAAAAAArBwAAAAAAACsH
                  AAAAAAAAKwcAAAAAAAArBwAAAAAAACsHAAAAAAAAKwcAAAAAAAArBwAAAAAAANkGAAAAAAAA2QYA
                  AAAAAADZBgAAAAAAANkGAAAAAAAAegkAAAAAAAB6CQAAAAAAAKIIAAAAAAAAoggAAAAAAACiCAAA
                  AAAAAKIIAAAAAAAAYAYAAAAAAAAwBgAAAAAAAC8GAAAAAAAALwYAAAAAAAAvBgAAAAAAANcFAAAA
                  AAAA1gUAAAAAAAA0AwAAAAAAADQDAAAAAAAAhwIAAAAAAACHAgAAAAAAAIcCAAAAAAAAhwIAAAAA
                  AACHAgAAAAAAAIcCAAAAAAAAhwIAAAAAAACHAgAAAAAAAIcCAAAAAAAAhwIAAAAAAACHAgAAAAAA
                  AIcCAAAAAAAAhwIAAAAAAACHAgAAAAAAAIcCAAAAAAAAdAEAAAAAAADRAAAAAAAAANEAAAAAAAAA
                  0QAAAAAAAADRAAAAAAAAANEAAAAAAAAA0QAAAAAAAADRAAAAAAAAANEAAAAAAAAA0QAAAAAAAADR
                  AAAAAAAAANEAAAAAAAAA0QAAAAAAAADRAAAAAAAAANEAAAAAAAAA0QAAAAAAAADRAAAAAAAAANEA
                  AAAAAAAA0QAAAAAAAADRAAAAAAAAANEAAAAAAAAAvgAAAAAAAACBAAAAAAAAAIEAAAAAAAAAgQAA
                  AAAAAACBAAAAAAAAAGEAAAAAAAAAYQAAAAAAAABhAAAAAAAAAGEAAAAAAAAAYQAAAAAAAABhAAAA
                  AAAAAGEAAAAAAAAAYQAAAAAAAABhAAAAAAAAAGEAAAAAAAAAYQAAAAAAAABhAAAAAAAAAGEAAAAA
                  AAAAYQAAAAAAAABPAAAAAAAAAE8AAAAAAAAATwAAAAAAAABPAAAAAAAAAE8AAAAAAAAATwAAAAAA
                  AABPAAAAAAAAADgAAAAAAAAAOAAAAAAAAAA4AAAAAAAAADgAAAAAAAAAuwAAAAAAAAC7AAAAAAAA
                  ANEAAAAAAAAA0QAAAAAAAADSAAAAAAAAANIAAAAAAAAAVgEAAAAAAABiAQAAAAAAAGMBAAAAAAAA
                  YwEAAAAAAABjAQAAAAAAAIABAAAAAAAAgQEAAAAAAABMAgAAAAAAAEwCAAAAAAAAeAIAAAAAAAB4
                  AgAAAAAAAHgCAAAAAAAAeQIAAAAAAAB5AgAAAAAAAHkCAAAAAAAAeQIAAAAAAAB5AgAAAAAAAHkC
                  AAAAAAAAeQIAAAAAAAB5AgAAAAAAAHkCAAAAAAAAeQIAAAAAAAB5AgAAAAAAAHkCAAAAAAAAMwMA
                  AAAAAACWAwAAAAAAAJYDAAAAAAAAlgMAAAAAAACWAwAAAAAAAJYDAAAAAAAAlgMAAAAAAACWAwAA
                  AAAAAJYDAAAAAAAAlgMAAAAAAACWAwAAAAAAAJgDAAAAAAAAmAMAAAAAAACYAwAAAAAAAJgDAAAA
                  AAAAmwMAAAAAAACbAwAAAAAAAJsDAAAAAAAAmwMAAAAAAACbAwAAAAAAAJsDAAAAAAAApwMAAAAA
                  AAACBAAAAAAAAAIEAAAAAAAAAgQAAAAAAAACBAAAAAAAADMEAAAAAAAAMwQAAAAAAAAzBAAAAAAA
                  ADMEAAAAAAAAMwQAAAAAAAAzBAAAAAAAADMEAAAAAAAAMwQAAAAAAAAzBAAAAAAAADMEAAAAAAAA
                  MwQAAAAAAAAzBAAAAAAAADMEAAAAAAAAMwQAAAAAAABzBAAAAAAAAHMEAAAAAAAAcwQAAAAAAABz
                  BAAAAAAAAHMEAAAAAAAAcwQAAAAAAABzBAAAAAAAAMUEAAAAAAAAxQQAAAAAAADFBAAAAAAAAMUE
                  AAAAAAAA0VYAAAAAAADRVgAAAAAAAKlXAAAAAAAAqVcAAAAAAACpVwAAAAAAAKlXAAAAAAAA61kA
                  AAAAAAAbWgAAAAAAABxaAAAAAAAAHFoAAAAAAAAcWgAAAAAAAHRaAAAAAAAAdVoAAAAAAAAXXQAA
                  AAAAABddAAAAAAAAxF0AAAAAAADEXQAAAAAAAMRdAAAAAAAAxF0AAAAAAADEXQAAAAAAAMRdAAAA
                  AAAAxF0AAAAAAADEXQAAAAAAAMRdAAAAAAAAxF0AAAAAAADEXQAAAAAAAMRdAAAAAAAAxF0AAAAA
                  AADEXQAAAAAAAMRdAAAAAAAA114AAAAAAAB6XwAAAAAAAHpfAAAAAAAAel8AAAAAAAB6XwAAAAAA
                  AHpfAAAAAAAAel8AAAAAAAB6XwAAAAAAAHpfAAAAAAAAel8AAAAAAAB6XwAAAAAAAHpfAAAAAAAA
                  el8AAAAAAAB6XwAAAAAAAHpfAAAAAAAAel8AAAAAAAB6XwAAAAAAAHpfAAAAAAAAel8AAAAAAAB6
                  XwAAAAAAAHpfAAAAAAAAjV8AAAAAAADKXwAAAAAAAMpfAAAAAAAAyl8AAAAAAADKXwAAAAAAAOpf
                  AAAAAAAA6l8AAAAAAADqXwAAAAAAAOpfAAAAAAAA6l8AAAAAAADqXwAAAAAAAOpfAAAAAAAA6l8A
                  AAAAAADqXwAAAAAAAOpfAAAAAAAA6l8AAAAAAADqXwAAAAAAAOpfAAAAAAAA6l8AAAAAAAD8XwAA
                  AAAAAPxfAAAAAAAA/F8AAAAAAAD8XwAAAAAAAPxfAAAAAAAA/F8AAAAAAAD8XwAAAAAAABNgAAAA
                  AAAAE2AAAAAAAAATYAAAAAAAABNgAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gZVDi2zn7z8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5zS1nh/c7j8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
    xgboost:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADw
                  PwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/
                  AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8A
                  AAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAA
                  AAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAA
                  AAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAA
                  AADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/feVFdt+y7z995UV237Lv
                  P33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/
                  feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z99
                  5UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33l
                  RXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVF
                  dt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV2
                  37LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbf
                  su8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y
                  7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237Lv
                  P33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/
                  feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z99
                  5UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33lRXbfsu8/feVFdt+y7z995UV237LvP33l
                  RXbfsu8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2v
                  CUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8J
                  QdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB
                  2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ
                  7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnv
                  P5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/
                  lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+U
                  va8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9
                  rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2v
                  CUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8J
                  QdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB
                  2e8/lL2vCUHZ7z+Uva8JQdnvP5S9rwlB2e8/AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACA
                  YUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBh
                  QAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFA
                  AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAA
                  AAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAA
                  AAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAA
                  AACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAA
                  AIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAA
                  gGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACA
                  YUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBh
                  QAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFA
                  AAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAACAYUAAAAAAAIBhQAAAAAAAgGFAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAzKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4i
                  SIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJI
                  gz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiD
                  P8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
                  zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/M
                  oIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yg
                  hm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCG
                  biJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZu
                  IkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4i
                  SIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJI
                  gz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/zKCGbiJIgz/MoIZuIkiD
                  P8yghm4iSIM/zKCGbiJIgz/MoIZuIkiDP8yghm4iSIM/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  ggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACC
                  CwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIIL
                  AAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAA
                  AAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAA
                  AAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAA
                  AACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAA
                  AIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAA
                  ggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACC
                  CwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIIL
                  AAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsAAAAAAACCCwAAAAAAAIILAAAAAAAAggsA
                  AAAAAACCCwAAAAAAAIILAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHAAAAAAAAAAcAAAAAAAA
                  ABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAA
                  HAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAc
                  AAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAA
                  AAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAA
                  AAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAA
                  AAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAA
                  AAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAA
                  ABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAA
                  HAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAc
                  AAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAcAAAAAAAAABwA
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAA
                  AABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAA
                  AEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
                  S2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABL
                  YAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtg
                  AAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AA
                  AAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAA
                  AAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAAS2AAAAAAAABLYAAAAAAAAEtgAAAA
                  AAAAS2AAAAAAAABLYAAAAAAAAEtgAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
  calibration_metrics:
    catboost:
      brier_score: 0.000341659761698173
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 5
          - *id001
          - false
          - !!binary |
            WiIVGZdHBT8AAAAAAAAAAAAAAAAAAPA/1EEd1EEd5D8AAAAAAADwPw==
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 5
          - *id001
          - false
          - !!binary |
            AAAAAAAAAAAOyAq/VvzRP1VVVVVVVdU/MQzDMAzD5D8AAAAAAADwPw==
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NpZzoOdtDz8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        coZ9WbPHyD8=
    lightgbm:
      brier_score: 0.00098810853211704
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 2
          - *id001
          - false
          - !!binary |
            JPc59LiWUj8AAAAAAADwPw==
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 2
          - *id001
          - false
          - !!binary |
            qER6xnWJUj8AAAAAAADwPw==
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        l+QBbDeyxz4=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        APhkf1uGuj4=
    logisticregression:
      brier_score: 0.0009328527598474668
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 4
          - *id001
          - false
          - !!binary |
            Ev3sMs3sSz9VVVVVVVXFPwAAAAAAAPA/AAAAAAAA8D8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 4
          - *id001
          - false
          - !!binary |
            Yqo7iFV1Tj8QclVmfrLHP0sVcC7o89M/AAAAAAAA8D8=
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        iNhAH87yKD8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NFAdT/idxj8=
    randomforest:
      brier_score: 0.031038629394008248
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 9
          - *id001
          - false
          - !!binary |
            N/1ea4YWgT9RBSmqIEXFPwWMzMW1T80/WWw6WWw62T8AAAAAAADwP22yySabbOI/+hicj8H56D/Z
            j/3Yj/3oP3cpK99nAu8/
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 9
          - *id001
          - false
          - !!binary |
            T2MDdyqMgD8h4uTETrfCP9dtLuFvCc0/SVYEGG6T2T9jTCF7VQbhP/LGaM+kHOQ/TlVVVVVV6T/1
            OI7jOI7rP8oCXq7eQ+8/
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        ilzi+cqMZT8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        pxezt2ptsj8=
    xgboost:
      brier_score: 0.0009546292925459328
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 2
          - *id001
          - false
          - !!binary |
            JPc59LiWUj8AAAAAAADwPw==
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 2
          - *id001
          - false
          - !!binary |
            c0W/nzOmUD8AAAAAAADwPw==
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        rwp1GiG5Gz8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        EBurR1UIDz8=
  leakage_tests:
    adversarial:
      auc_adversarial: 0.9989624602337757
      distribution_shift: true
      shift_risk: HIGH
    catboost_null_importance:
      auc_reference: 0.9999452775385544
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          +NC9yMlbtj8=
        BUSINESS_FAUSSE_DECLARATION_ASSEMBLAGE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          qCFnj7HBsj8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          +Ir+lzzYsz8=
        BUSINESS_IS_ELECTROMENAGER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACg7AAZ6D4=
        BUSINESS_IS_MACHINE_BUREAU: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ACjxk08gYj8=
        BUSINESS_QUANTITE_ANORMALE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          cPVKlZwtqj8=
        BUSINESS_RISK_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gLtbI/LeuT8=
        BUSINESS_SOUS_EVALUATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          wEC+iORnqz8=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AHRMvdQ7UD8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAU3/bb74=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AKruqvrzWz8=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          APQKBg8sTz8=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AADcVPPmCD8=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAsKEEML4=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACA7cgNcL4=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAC4LEpPMT8=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABQZ/MP9j4=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
      null_importance_risk: HIGH
      suspicious_features:
      - POIDS_NET_KG
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - TAUX_DROITS_PERCENT
      - BUSINESS_IS_ELECTROMENAGER
      - BUSINESS_IS_MACHINE_BUREAU
      - BUSINESS_VALEUR_ELEVEE
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    catboost_permutation:
      auc_normal: 0.9998304713081778
      auc_permuted: 0.4997851786323764
      leakage_detected: false
      leakage_risk: LOW
    lightgbm_null_importance:
      auc_reference: 0.9997595456363817
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAFXnASlmD8=
        BUSINESS_FAUSSE_DECLARATION_ASSEMBLAGE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ACqQxCOeZj8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4FDqr+P+mT8=
        BUSINESS_IS_ELECTROMENAGER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AESh2uH2eT8=
        BUSINESS_IS_MACHINE_BUREAU: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AGC2jqhEaj8=
        BUSINESS_QUANTITE_ANORMALE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gOOWI8W4eD8=
        BUSINESS_RISK_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          8LzQvIuuqj8=
        BUSINESS_SOUS_EVALUATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gEKEZAy9hj8=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AGBAaRa7Vz8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACgwVTusr4=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABAfHHG8b4=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACtQpaOFD8=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ACBWlfFsID8=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ANA0an7TPj8=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAox1fazD4=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACArJzIrT4=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ABouxWmEVT8=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AJx9lSXpSj8=
      null_importance_risk: HIGH
      suspicious_features:
      - POIDS_NET_KG
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - TAUX_DROITS_PERCENT
      - BUSINESS_IS_ELECTROMENAGER
      - BUSINESS_FAUSSE_DECLARATION_ASSEMBLAGE
      - BUSINESS_QUANTITE_ANORMALE
      - BUSINESS_IS_MACHINE_BUREAU
      - BUSINESS_VALEUR_ELEVEE
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    lightgbm_permutation:
      auc_normal: 0.9997988871519309
      auc_permuted: 0.4915372193671673
      leakage_detected: false
      leakage_risk: LOW
    logisticregression_permutation:
      auc_normal: 0.9995973923749586
      auc_permuted: 0.5022892750244659
      leakage_detected: false
      leakage_risk: LOW
    randomforest_permutation:
      auc_normal: 0.9745069277401701
      auc_permuted: 0.49257951016363277
      leakage_detected: false
      leakage_risk: LOW
    xgboost_null_importance:
      auc_reference: 0.9997342143207413
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4JJ4Dbc3lT8=
        BUSINESS_FAUSSE_DECLARATION_ASSEMBLAGE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          APdNBvd8YT8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4OMDfgPZlj8=
        BUSINESS_IS_ELECTROMENAGER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AJ9mqNhkYT8=
        BUSINESS_IS_MACHINE_BUREAU: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AEzt7dNDZD8=
        BUSINESS_QUANTITE_ANORMALE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAko0xyieT8=
        BUSINESS_RISK_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4N558SUiqz8=
        BUSINESS_SOUS_EVALUATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gE0EI8hrjz8=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ANRoY4K5UT8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACUveMP6b4=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACcfmc2674=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AECNkckHJD8=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIjbmZr9Mj8=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABgAqICoT4=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoDw=
      null_importance_risk: HIGH
      suspicious_features:
      - POIDS_NET_KG
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - TAUX_DROITS_PERCENT
      - BUSINESS_IS_ELECTROMENAGER
      - BUSINESS_FAUSSE_DECLARATION_ASSEMBLAGE
      - BUSINESS_QUANTITE_ANORMALE
      - BUSINESS_IS_MACHINE_BUREAU
      - BUSINESS_VALEUR_ELEVEE
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    xgboost_permutation:
      auc_normal: 0.9997183137937143
      auc_permuted: 0.5023902406580395
      leakage_detected: false
      leakage_risk: LOW
  monitoring_config:
    alert_channels:
    - email
    - dashboard
    alert_thresholds:
      distribution_shift_threshold: 0.05
      ks_threshold: 0.05
      missing_rate_threshold: 0.1
      psi_threshold: 0.2
    features_to_monitor: []
    monitoring_frequency: daily
    reference_stats: {}
  overfitting_analysis:
    catboost:
      auc_gap: 0.00011480623037662951
      f1_gap: -0.0006776873062841471
      test_auc: 0.9998304713081778
      test_f1: 0.9974827991273704
      train_auc: 0.9999452775385544
      train_f1: 0.9968051118210862
    lightgbm:
      auc_gap: -3.934151554918497e-05
      f1_gap: 0.0006704751564786671
      test_auc: 0.9997988871519309
      test_f1: 0.9952702702702703
      train_auc: 0.9997595456363817
      train_f1: 0.9959407454267489
    logisticregression:
      auc_gap: 0.00024249341104354283
      f1_gap: 0.0010922158913618807
      test_auc: 0.9995973923749586
      test_f1: 0.9952702702702703
      train_auc: 0.9998398857860021
      train_f1: 0.9963624861616321
    randomforest:
      auc_gap: 0.0013156972648390441
      f1_gap: 0.019028562755873835
      test_auc: 0.9751394702051398
      test_f1: 0.7848826111853408
      train_auc: 0.9764551674699788
      train_f1: 0.8039111739412146
    xgboost:
      auc_gap: 1.590052702693523e-05
      f1_gap: 0.0009330623599764776
      test_auc: 0.9997183137937143
      test_f1: 0.9952702702702703
      train_auc: 0.9997342143207413
      train_f1: 0.9962033326302467
  subgroup_analysis:
    subgroup_results:
      CODE_SH_COMPLET:
        8402900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SpCnBHlK4D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SpCnBHlK4D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SpCnBHlK4D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9253380364491475
            f1: 0.7671232876712328
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SpCnBHlK4D8=
            n_samples: 165
            precision: 0.9032258064516129
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SpCnBHlK4D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
        8406900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              t6QtaUvawj8=
            n_samples: 129
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              t6QtaUvawj8=
            n_samples: 129
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              t6QtaUvawj8=
            n_samples: 129
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              t6QtaUvawj8=
            n_samples: 129
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              t6QtaUvawj8=
            n_samples: 129
            precision: 1.0
            recall: 1.0
        8407210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3xx9c/TNwT8=
            n_samples: 532
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3xx9c/TNwT8=
            n_samples: 532
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3xx9c/TNwT8=
            n_samples: 532
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9996459341437507
            f1: 0.9866666666666667
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3xx9c/TNwT8=
            n_samples: 532
            precision: 0.9736842105263158
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3xx9c/TNwT8=
            n_samples: 532
            precision: 1.0
            recall: 1.0
        8407329000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
        8407340000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 315
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 315
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 315
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 315
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 315
            precision: 0.0
            recall: 0.0
        8407900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ePshgbcfgj8=
            n_samples: 339
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ePshgbcfgj8=
            n_samples: 339
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ePshgbcfgj8=
            n_samples: 339
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ePshgbcfgj8=
            n_samples: 339
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ePshgbcfgj8=
            n_samples: 339
            precision: 1.0
            recall: 1.0
        8408200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BINxjACJaT8=
            n_samples: 5133
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BINxjACJaT8=
            n_samples: 5133
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BINxjACJaT8=
            n_samples: 5133
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BINxjACJaT8=
            n_samples: 5133
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BINxjACJaT8=
            n_samples: 5133
            precision: 1.0
            recall: 1.0
        8408900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              X43ziYEmgz8=
            n_samples: 3743
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              X43ziYEmgz8=
            n_samples: 3743
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              X43ziYEmgz8=
            n_samples: 3743
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9980775157959625
            f1: 0.9444444444444444
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              X43ziYEmgz8=
            n_samples: 3743
            precision: 0.918918918918919
            recall: 0.9714285714285714
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              X43ziYEmgz8=
            n_samples: 3743
            precision: 1.0
            recall: 1.0
        8409910000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              nhLkKUGecj8=
            n_samples: 220
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              nhLkKUGecj8=
            n_samples: 220
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              nhLkKUGecj8=
            n_samples: 220
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              nhLkKUGecj8=
            n_samples: 220
            precision: 0.3333333333333333
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              nhLkKUGecj8=
            n_samples: 220
            precision: 1.0
            recall: 1.0
        8409990000:
          catboost:
            auc: 0.9944444444444445
            f1: 0.994413407821229
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZBDgAlWiT8=
            n_samples: 7275
            precision: 1.0
            recall: 0.9888888888888889
          lightgbm:
            auc: 1.0
            f1: 0.994413407821229
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZBDgAlWiT8=
            n_samples: 7275
            precision: 1.0
            recall: 0.9888888888888889
          logisticregression:
            auc: 0.9944444444444445
            f1: 0.994413407821229
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZBDgAlWiT8=
            n_samples: 7275
            precision: 1.0
            recall: 0.9888888888888889
          randomforest:
            auc: 0.984120467022346
            f1: 0.8791208791208791
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZBDgAlWiT8=
            n_samples: 7275
            precision: 0.8695652173913043
            recall: 0.8888888888888888
          xgboost:
            auc: 0.9997316941158277
            f1: 0.994413407821229
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZBDgAlWiT8=
            n_samples: 7275
            precision: 1.0
            recall: 0.9888888888888889
        8411990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CB988MEHrz8=
            n_samples: 264
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CB988MEHrz8=
            n_samples: 264
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CB988MEHrz8=
            n_samples: 264
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CB988MEHrz8=
            n_samples: 264
            precision: 0.8888888888888888
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CB988MEHrz8=
            n_samples: 264
            precision: 1.0
            recall: 1.0
        8412210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kAMmQKyCjD8=
            n_samples: 431
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kAMmQKyCjD8=
            n_samples: 431
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kAMmQKyCjD8=
            n_samples: 431
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9864705882352941
            f1: 0.7692307692307693
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kAMmQKyCjD8=
            n_samples: 431
            precision: 0.7142857142857143
            recall: 0.8333333333333334
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kAMmQKyCjD8=
            n_samples: 431
            precision: 1.0
            recall: 1.0
        8412290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              chzHcRzHkT8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              chzHcRzHkT8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              chzHcRzHkT8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8333333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              chzHcRzHkT8=
            n_samples: 288
            precision: 0.7142857142857143
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              chzHcRzHkT8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
        8412900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9waUiSvdez8=
            n_samples: 294
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9waUiSvdez8=
            n_samples: 294
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9waUiSvdez8=
            n_samples: 294
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9waUiSvdez8=
            n_samples: 294
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9waUiSvdez8=
            n_samples: 294
            precision: 1.0
            recall: 1.0
        8413110000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FhYWFhYWlj8=
            n_samples: 510
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FhYWFhYWlj8=
            n_samples: 510
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FhYWFhYWlj8=
            n_samples: 510
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.997540535616688
            f1: 0.8421052631578947
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FhYWFhYWlj8=
            n_samples: 510
            precision: 1.0
            recall: 0.7272727272727273
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FhYWFhYWlj8=
            n_samples: 510
            precision: 1.0
            recall: 1.0
        8413190090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Af8AX/Afz8=
            n_samples: 258
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Af8AX/Afz8=
            n_samples: 258
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Af8AX/Afz8=
            n_samples: 258
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.998046875
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Af8AX/Afz8=
            n_samples: 258
            precision: 1.0
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Af8AX/Afz8=
            n_samples: 258
            precision: 1.0
            recall: 1.0
        8413200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sak05Nxndz8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sak05Nxndz8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sak05Nxndz8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sak05Nxndz8=
            n_samples: 175
            precision: 0.5
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sak05Nxndz8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
        8413300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYjD8=
            n_samples: 656
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYjD8=
            n_samples: 656
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYjD8=
            n_samples: 656
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9964794779323374
            f1: 0.8421052631578947
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYjD8=
            n_samples: 656
            precision: 0.8
            recall: 0.8888888888888888
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYjD8=
            n_samples: 656
            precision: 1.0
            recall: 1.0
        8413400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRBqj8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRBqj8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRBqj8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8340840840840841
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRBqj8=
            n_samples: 117
            precision: 0.6666666666666666
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRBqj8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
        8413500090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              w+zgCCKtlz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              w+zgCCKtlz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              w+zgCCKtlz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9666337935568705
            f1: 0.782608695652174
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              w+zgCCKtlz8=
            n_samples: 519
            precision: 0.8181818181818182
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              w+zgCCKtlz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
        8413600090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WlpaWlpaij8=
            n_samples: 544
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WlpaWlpaij8=
            n_samples: 544
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WlpaWlpaij8=
            n_samples: 544
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9737962223995744
            f1: 0.7272727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WlpaWlpaij8=
            n_samples: 544
            precision: 1.0
            recall: 0.5714285714285714
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WlpaWlpaij8=
            n_samples: 544
            precision: 1.0
            recall: 1.0
        8413700010:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Hh4eHh4enj8=
            n_samples: 170
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Hh4eHh4enj8=
            n_samples: 170
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Hh4eHh4enj8=
            n_samples: 170
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Hh4eHh4enj8=
            n_samples: 170
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Hh4eHh4enj8=
            n_samples: 170
            precision: 1.0
            recall: 1.0
        8413700090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fHYGfhlxnT8=
            n_samples: 1426
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fHYGfhlxnT8=
            n_samples: 1426
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fHYGfhlxnT8=
            n_samples: 1426
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9914590120630448
            f1: 0.825
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fHYGfhlxnT8=
            n_samples: 1426
            precision: 0.8461538461538461
            recall: 0.8048780487804879
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fHYGfhlxnT8=
            n_samples: 1426
            precision: 1.0
            recall: 1.0
        8413810010:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
        8413810090:
          catboost:
            auc: 0.9642857142857143
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bj9QjNDoej8=
            n_samples: 2131
            precision: 1.0
            recall: 0.9285714285714286
          lightgbm:
            auc: 0.9630035764896416
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bj9QjNDoej8=
            n_samples: 2131
            precision: 1.0
            recall: 0.9285714285714286
          logisticregression:
            auc: 0.9642857142857143
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bj9QjNDoej8=
            n_samples: 2131
            precision: 1.0
            recall: 0.9285714285714286
          randomforest:
            auc: 0.9254335650178825
            f1: 0.7272727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bj9QjNDoej8=
            n_samples: 2131
            precision: 1.0
            recall: 0.5714285714285714
          xgboost:
            auc: 0.963206019299548
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bj9QjNDoej8=
            n_samples: 2131
            precision: 1.0
            recall: 0.9285714285714286
        8413820000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 463
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 463
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 463
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 463
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 463
            precision: 0.0
            recall: 0.0
        8413911000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
        8413919000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              az5nQHfRmD8=
            n_samples: 2063
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              az5nQHfRmD8=
            n_samples: 2063
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              az5nQHfRmD8=
            n_samples: 2063
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9802334823646299
            f1: 0.8775510204081632
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              az5nQHfRmD8=
            n_samples: 2063
            precision: 0.8958333333333334
            recall: 0.86
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              az5nQHfRmD8=
            n_samples: 2063
            precision: 1.0
            recall: 1.0
        8413920000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 124
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 124
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 124
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 124
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 124
            precision: 0.0
            recall: 0.0
        8414100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EARBEARBgD8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EARBEARBgD8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EARBEARBgD8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EARBEARBgD8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EARBEARBgD8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
        8414209000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 109
            precision: 0.0
            recall: 0.0
        8414309000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZCELWchChj8=
            n_samples: 368
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZCELWchChj8=
            n_samples: 368
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZCELWchChj8=
            n_samples: 368
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9893543956043956
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZCELWchChj8=
            n_samples: 368
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZCELWchChj8=
            n_samples: 368
            precision: 1.0
            recall: 1.0
        8414400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XvOSowrwlz8=
            n_samples: 385
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XvOSowrwlz8=
            n_samples: 385
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XvOSowrwlz8=
            n_samples: 385
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9546394799054374
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XvOSowrwlz8=
            n_samples: 385
            precision: 1.0
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XvOSowrwlz8=
            n_samples: 385
            precision: 1.0
            recall: 1.0
        8414510000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqABGqABaj8=
            n_samples: 2835
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqABGqABaj8=
            n_samples: 2835
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqABGqABaj8=
            n_samples: 2835
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9761932845796966
            f1: 0.7142857142857143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqABGqABaj8=
            n_samples: 2835
            precision: 1.0
            recall: 0.5555555555555556
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqABGqABaj8=
            n_samples: 2835
            precision: 1.0
            recall: 1.0
        8414590000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hC+/lu4sgD8=
            n_samples: 2279
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hC+/lu4sgD8=
            n_samples: 2279
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hC+/lu4sgD8=
            n_samples: 2279
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9773698953265516
            f1: 0.7878787878787878
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hC+/lu4sgD8=
            n_samples: 2279
            precision: 0.8666666666666667
            recall: 0.7222222222222222
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hC+/lu4sgD8=
            n_samples: 2279
            precision: 1.0
            recall: 1.0
        8414600000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 112
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 112
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 112
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 112
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 112
            precision: 0.0
            recall: 0.0
        8414801000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAoD8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAoD8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAoD8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9906411788132218
            f1: 0.8235294117647058
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAoD8=
            n_samples: 288
            precision: 0.875
            recall: 0.7777777777777778
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAoD8=
            n_samples: 288
            precision: 1.0
            recall: 1.0
        8414809000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lsTZ63GZoj8=
            n_samples: 991
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lsTZ63GZoj8=
            n_samples: 991
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lsTZ63GZoj8=
            n_samples: 991
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.989717859220477
            f1: 0.8484848484848485
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lsTZ63GZoj8=
            n_samples: 991
            precision: 0.9333333333333333
            recall: 0.7777777777777778
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lsTZ63GZoj8=
            n_samples: 991
            precision: 1.0
            recall: 1.0
        8414900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              RX8nq+WNkT8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              RX8nq+WNkT8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              RX8nq+WNkT8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              RX8nq+WNkT8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              RX8nq+WNkT8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
        8414909000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EnJDClUuoj8=
            n_samples: 873
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EnJDClUuoj8=
            n_samples: 873
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EnJDClUuoj8=
            n_samples: 873
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9992529308098996
            f1: 0.9836065573770492
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EnJDClUuoj8=
            n_samples: 873
            precision: 1.0
            recall: 0.967741935483871
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EnJDClUuoj8=
            n_samples: 873
            precision: 1.0
            recall: 1.0
        8415109000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FL/eUqeHsz8=
            n_samples: 1940
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FL/eUqeHsz8=
            n_samples: 1940
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FL/eUqeHsz8=
            n_samples: 1940
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9634392344353282
            f1: 0.8333333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FL/eUqeHsz8=
            n_samples: 1940
            precision: 0.9482758620689655
            recall: 0.7432432432432432
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FL/eUqeHsz8=
            n_samples: 1940
            precision: 1.0
            recall: 1.0
        8415820000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 357
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 357
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 357
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9101211072664359
            f1: 0.8387096774193549
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 357
            precision: 0.9285714285714286
            recall: 0.7647058823529411
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 357
            precision: 1.0
            recall: 1.0
        8415830000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              c6hY0Bwqpj8=
            n_samples: 231
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              c6hY0Bwqpj8=
            n_samples: 231
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              c6hY0Bwqpj8=
            n_samples: 231
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              c6hY0Bwqpj8=
            n_samples: 231
            precision: 0.9090909090909091
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              c6hY0Bwqpj8=
            n_samples: 231
            precision: 1.0
            recall: 1.0
        8415909000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAApD8=
            n_samples: 512
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAApD8=
            n_samples: 512
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAApD8=
            n_samples: 512
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9985772357723578
            f1: 0.9
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAApD8=
            n_samples: 512
            precision: 0.9
            recall: 0.9
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAApD8=
            n_samples: 512
            precision: 1.0
            recall: 1.0
        8416200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 112
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 112
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 112
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 112
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 112
            precision: 1.0
            recall: 1.0
        8416900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xajhb1OMqj8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xajhb1OMqj8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xajhb1OMqj8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xajhb1OMqj8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xajhb1OMqj8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
        8417200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtaj8=
            n_samples: 310
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtaj8=
            n_samples: 310
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtaj8=
            n_samples: 310
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtaj8=
            n_samples: 310
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtaj8=
            n_samples: 310
            precision: 1.0
            recall: 1.0
        8417900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6w/0SAk5pT8=
            n_samples: 193
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6w/0SAk5pT8=
            n_samples: 193
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6w/0SAk5pT8=
            n_samples: 193
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8421052631578947
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6w/0SAk5pT8=
            n_samples: 193
            precision: 0.7272727272727273
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6w/0SAk5pT8=
            n_samples: 193
            precision: 1.0
            recall: 1.0
        8418109000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LUaTeddi5D8=
            n_samples: 518
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LUaTeddi5D8=
            n_samples: 518
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LUaTeddi5D8=
            n_samples: 518
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9798355899419728
            f1: 0.6363636363636364
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LUaTeddi5D8=
            n_samples: 518
            precision: 1.0
            recall: 0.4666666666666667
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LUaTeddi5D8=
            n_samples: 518
            precision: 1.0
            recall: 1.0
        8418219000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2jOZMJs94z8=
            n_samples: 1417
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2jOZMJs94z8=
            n_samples: 1417
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2jOZMJs94z8=
            n_samples: 1417
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.993816735219577
            f1: 0.5882352941176471
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2jOZMJs94z8=
            n_samples: 1417
            precision: 1.0
            recall: 0.4166666666666667
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2jOZMJs94z8=
            n_samples: 1417
            precision: 1.0
            recall: 1.0
        8418299000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gWVI+Z5N6D8=
            n_samples: 2744
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9962121212121212
            f1: 0.9988018212317278
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gWVI+Z5N6D8=
            n_samples: 2744
            precision: 0.9976065102920058
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gWVI+Z5N6D8=
            n_samples: 2744
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9812685395218984
            f1: 0.8886532343584306
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gWVI+Z5N6D8=
            n_samples: 2744
            precision: 0.9928909952606635
            recall: 0.8042226487523992
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gWVI+Z5N6D8=
            n_samples: 2744
            precision: 1.0
            recall: 1.0
        8418309000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QgghhBBC5j8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QgghhBBC5j8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QgghhBBC5j8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9853536807755063
            f1: 0.45982142857142855
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QgghhBBC5j8=
            n_samples: 496
            precision: 1.0
            recall: 0.2985507246376812
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QgghhBBC5j8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
        8418409000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              obgViluh6D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              obgViluh6D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              obgViluh6D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9877745544964774
            f1: 0.35064935064935066
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              obgViluh6D8=
            n_samples: 165
            precision: 1.0
            recall: 0.2125984251968504
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              obgViluh6D8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
        8418509000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Chul9Xmc4D8=
            n_samples: 445
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Chul9Xmc4D8=
            n_samples: 445
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Chul9Xmc4D8=
            n_samples: 445
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9748452482097343
            f1: 0.35587188612099646
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Chul9Xmc4D8=
            n_samples: 445
            precision: 1.0
            recall: 0.21645021645021645
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Chul9Xmc4D8=
            n_samples: 445
            precision: 1.0
            recall: 1.0
        8418690000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bZKmhXAD4T8=
            n_samples: 521
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bZKmhXAD4T8=
            n_samples: 521
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bZKmhXAD4T8=
            n_samples: 521
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9760978280168078
            f1: 0.4782608695652174
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bZKmhXAD4T8=
            n_samples: 521
            precision: 0.967032967032967
            recall: 0.3176895306859206
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bZKmhXAD4T8=
            n_samples: 521
            precision: 1.0
            recall: 1.0
        8418990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Oo9L3/7K5D8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Oo9L3/7K5D8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Oo9L3/7K5D8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9762072273744804
            f1: 0.4350132625994695
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Oo9L3/7K5D8=
            n_samples: 454
            precision: 1.0
            recall: 0.27796610169491526
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Oo9L3/7K5D8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
        8419200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYrD8=
            n_samples: 164
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYrD8=
            n_samples: 164
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYrD8=
            n_samples: 164
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9731182795698925
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYrD8=
            n_samples: 164
            precision: 1.0
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GZyPwfkYrD8=
            n_samples: 164
            precision: 1.0
            recall: 1.0
        8419500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jlmz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jlmz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jlmz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8888888888888888
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jlmz8=
            n_samples: 299
            precision: 0.8
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jlmz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
        8419810000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
        8419890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U59WsNSnlT8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U59WsNSnlT8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U59WsNSnlT8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9162257495590829
            f1: 0.4
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U59WsNSnlT8=
            n_samples: 331
            precision: 0.375
            recall: 0.42857142857142855
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U59WsNSnlT8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
        8419900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wQ6Ed3ouoz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wQ6Ed3ouoz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wQ6Ed3ouoz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9995393827729157
            f1: 0.88
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wQ6Ed3ouoz8=
            n_samples: 347
            precision: 0.9166666666666666
            recall: 0.8461538461538461
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wQ6Ed3ouoz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
        8421190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVpT8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVpT8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVpT8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9051932367149759
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVpT8=
            n_samples: 144
            precision: 1.0
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVpT8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
        8421211000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vxArSuMjgz8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vxArSuMjgz8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vxArSuMjgz8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8160377358490566
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vxArSuMjgz8=
            n_samples: 107
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vxArSuMjgz8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
        8421219000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iCH8nfeAmD8=
            n_samples: 794
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iCH8nfeAmD8=
            n_samples: 794
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iCH8nfeAmD8=
            n_samples: 794
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9993887945670628
            f1: 0.85
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iCH8nfeAmD8=
            n_samples: 794
            precision: 0.8095238095238095
            recall: 0.8947368421052632
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iCH8nfeAmD8=
            n_samples: 794
            precision: 1.0
            recall: 1.0
        8421230000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhWD8=
            n_samples: 1344
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhWD8=
            n_samples: 1344
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhWD8=
            n_samples: 1344
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.6549925484351714
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhWD8=
            n_samples: 1344
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhWD8=
            n_samples: 1344
            precision: 1.0
            recall: 1.0
        8421290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fv0iXK7Tkz8=
            n_samples: 878
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fv0iXK7Tkz8=
            n_samples: 878
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fv0iXK7Tkz8=
            n_samples: 878
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9989410398305664
            f1: 0.9696969696969697
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fv0iXK7Tkz8=
            n_samples: 878
            precision: 1.0
            recall: 0.9411764705882353
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fv0iXK7Tkz8=
            n_samples: 878
            precision: 1.0
            recall: 1.0
        8421310000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tdugrBBjbz8=
            n_samples: 783
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tdugrBBjbz8=
            n_samples: 783
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tdugrBBjbz8=
            n_samples: 783
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9412393162393162
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tdugrBBjbz8=
            n_samples: 783
            precision: 1.0
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tdugrBBjbz8=
            n_samples: 783
            precision: 1.0
            recall: 1.0
        8421399000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yar7mxCqlT8=
            n_samples: 709
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yar7mxCqlT8=
            n_samples: 709
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yar7mxCqlT8=
            n_samples: 709
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999039385206533
            f1: 0.8823529411764706
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yar7mxCqlT8=
            n_samples: 709
            precision: 0.7894736842105263
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yar7mxCqlT8=
            n_samples: 709
            precision: 1.0
            recall: 1.0
        8421910000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQgD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQgD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQgD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQgD8=
            n_samples: 255
            precision: 0.6666666666666666
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQgD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
        8421990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j37aMPYAqT8=
            n_samples: 1331
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j37aMPYAqT8=
            n_samples: 1331
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j37aMPYAqT8=
            n_samples: 1331
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9733442702637015
            f1: 0.8108108108108109
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j37aMPYAqT8=
            n_samples: 1331
            precision: 0.9782608695652174
            recall: 0.6923076923076923
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j37aMPYAqT8=
            n_samples: 1331
            precision: 1.0
            recall: 1.0
        8422300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cv8uZWertj8=
            n_samples: 463
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cv8uZWertj8=
            n_samples: 463
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cv8uZWertj8=
            n_samples: 463
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9443416946017801
            f1: 0.7761194029850746
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cv8uZWertj8=
            n_samples: 463
            precision: 1.0
            recall: 0.6341463414634146
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cv8uZWertj8=
            n_samples: 463
            precision: 1.0
            recall: 1.0
        8422400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KOeSWKY+nj8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KOeSWKY+nj8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KOeSWKY+nj8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9885093167701864
            f1: 0.8461538461538461
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KOeSWKY+nj8=
            n_samples: 474
            precision: 0.9166666666666666
            recall: 0.7857142857142857
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KOeSWKY+nj8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
        8422900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BhhggAEGWD8=
            n_samples: 1364
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BhhggAEGWD8=
            n_samples: 1364
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BhhggAEGWD8=
            n_samples: 1364
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BhhggAEGWD8=
            n_samples: 1364
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BhhggAEGWD8=
            n_samples: 1364
            precision: 1.0
            recall: 1.0
        8423890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQoD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQoD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQoD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9888663967611335
            f1: 0.7692307692307693
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQoD8=
            n_samples: 255
            precision: 1.0
            recall: 0.625
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EBAQEBAQoD8=
            n_samples: 255
            precision: 1.0
            recall: 1.0
        8423900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 266
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 266
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 266
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 266
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 266
            precision: 1.0
            recall: 1.0
        8424100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qvNrD7msiD8=
            n_samples: 415
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qvNrD7msiD8=
            n_samples: 415
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qvNrD7msiD8=
            n_samples: 415
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qvNrD7msiD8=
            n_samples: 415
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qvNrD7msiD8=
            n_samples: 415
            precision: 1.0
            recall: 1.0
        8424200000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
        8424300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              58sBlm2+jD8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              58sBlm2+jD8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              58sBlm2+jD8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9924377224199289
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              58sBlm2+jD8=
            n_samples: 285
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              58sBlm2+jD8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
        8424410000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HdRBHdRBfT8=
            n_samples: 140
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HdRBHdRBfT8=
            n_samples: 140
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HdRBHdRBfT8=
            n_samples: 140
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HdRBHdRBfT8=
            n_samples: 140
            precision: 0.5
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HdRBHdRBfT8=
            n_samples: 140
            precision: 1.0
            recall: 1.0
        8424490000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 133
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 133
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 133
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 133
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 133
            precision: 0.0
            recall: 0.0
        8424820000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZpz8=
            n_samples: 399
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZpz8=
            n_samples: 399
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZpz8=
            n_samples: 399
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8181818181818182
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZpz8=
            n_samples: 399
            precision: 0.6923076923076923
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZpz8=
            n_samples: 399
            precision: 1.0
            recall: 1.0
        8424890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ov9igM6/mD8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ov9igM6/mD8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ov9igM6/mD8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9876160990712074
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ov9igM6/mD8=
            n_samples: 331
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ov9igM6/mD8=
            n_samples: 331
            precision: 1.0
            recall: 1.0
        8424900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vUouZ/XYkT8=
            n_samples: 459
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vUouZ/XYkT8=
            n_samples: 459
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vUouZ/XYkT8=
            n_samples: 459
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8421052631578947
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vUouZ/XYkT8=
            n_samples: 459
            precision: 0.7272727272727273
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              vUouZ/XYkT8=
            n_samples: 459
            precision: 1.0
            recall: 1.0
        8425110000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pxBoCoGmgD8=
            n_samples: 123
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pxBoCoGmgD8=
            n_samples: 123
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pxBoCoGmgD8=
            n_samples: 123
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pxBoCoGmgD8=
            n_samples: 123
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pxBoCoGmgD8=
            n_samples: 123
            precision: 1.0
            recall: 1.0
        8425190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oeRO0cIZoD8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oeRO0cIZoD8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oeRO0cIZoD8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9694805194805195
            f1: 0.75
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oeRO0cIZoD8=
            n_samples: 159
            precision: 1.0
            recall: 0.6
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oeRO0cIZoD8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
        8425390000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bzBFPusGsz8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bzBFPusGsz8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bzBFPusGsz8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.8148148148148148
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bzBFPusGsz8=
            n_samples: 148
            precision: 0.6875
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bzBFPusGsz8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
        8425420000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ffji6gcdhT8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ffji6gcdhT8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ffji6gcdhT8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9890046296296297
            f1: 0.4
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ffji6gcdhT8=
            n_samples: 291
            precision: 0.5
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ffji6gcdhT8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
        8425490000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 263
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 263
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 263
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 263
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 263
            precision: 0.0
            recall: 0.0
        8426110000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGpD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGpD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGpD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8888888888888888
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGpD8=
            n_samples: 101
            precision: 0.8
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGpD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
        8426190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HEyRz7rBpD8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HEyRz7rBpD8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HEyRz7rBpD8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.994718309859155
            f1: 0.7692307692307693
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HEyRz7rBpD8=
            n_samples: 148
            precision: 0.7142857142857143
            recall: 0.8333333333333334
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HEyRz7rBpD8=
            n_samples: 148
            precision: 1.0
            recall: 1.0
        8426200000:
          catboost:
            auc: 1.0
            f1: 0.9885057471264368
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Zu10/IMLwz8=
            n_samples: 289
            precision: 0.9772727272727273
            recall: 1.0
          lightgbm:
            auc: 0.9982038192474948
            f1: 0.9882352941176471
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Zu10/IMLwz8=
            n_samples: 289
            precision: 1.0
            recall: 0.9767441860465116
          logisticregression:
            auc: 1.0
            f1: 0.9882352941176471
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Zu10/IMLwz8=
            n_samples: 289
            precision: 1.0
            recall: 0.9767441860465116
          randomforest:
            auc: 0.9644545282662129
            f1: 0.7708333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Zu10/IMLwz8=
            n_samples: 289
            precision: 0.6981132075471698
            recall: 0.8604651162790697
          xgboost:
            auc: 0.9990073737946682
            f1: 0.9882352941176471
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Zu10/IMLwz8=
            n_samples: 289
            precision: 1.0
            recall: 0.9767441860465116
        8426410000:
          catboost:
            auc: 0.999596875787352
            f1: 0.9759036144578314
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PF5sCb7Nzz8=
            n_samples: 326
            precision: 0.9529411764705882
            recall: 1.0
          lightgbm:
            auc: 0.9992945326278659
            f1: 0.9746835443037974
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PF5sCb7Nzz8=
            n_samples: 326
            precision: 1.0
            recall: 0.9506172839506173
          logisticregression:
            auc: 0.999143361048123
            f1: 0.9746835443037974
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PF5sCb7Nzz8=
            n_samples: 326
            precision: 1.0
            recall: 0.9506172839506173
          randomforest:
            auc: 0.9682287729906778
            f1: 0.8135593220338984
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PF5sCb7Nzz8=
            n_samples: 326
            precision: 0.75
            recall: 0.8888888888888888
          xgboost:
            auc: 0.9914336104812296
            f1: 0.9746835443037974
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PF5sCb7Nzz8=
            n_samples: 326
            precision: 1.0
            recall: 0.9506172839506173
        8426490000:
          catboost:
            auc: 0.9999431947284708
            f1: 0.9908256880733946
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /2h/tD/azz8=
            n_samples: 217
            precision: 0.9818181818181818
            recall: 1.0
          lightgbm:
            auc: 0.9999431947284708
            f1: 0.9906542056074766
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /2h/tD/azz8=
            n_samples: 217
            precision: 1.0
            recall: 0.9814814814814815
          logisticregression:
            auc: 0.9994319472847081
            f1: 0.9906542056074766
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /2h/tD/azz8=
            n_samples: 217
            precision: 1.0
            recall: 0.9814814814814815
          randomforest:
            auc: 0.9794364917064304
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /2h/tD/azz8=
            n_samples: 217
            precision: 0.7846153846153846
            recall: 0.9444444444444444
          xgboost:
            auc: 0.995966825721427
            f1: 0.9906542056074766
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /2h/tD/azz8=
            n_samples: 217
            precision: 1.0
            recall: 0.9814814814814815
        8426990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wE0xavjbtD8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wE0xavjbtD8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wE0xavjbtD8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8555718475073313
            f1: 0.4166666666666667
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wE0xavjbtD8=
            n_samples: 135
            precision: 0.38461538461538464
            recall: 0.45454545454545453
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wE0xavjbtD8=
            n_samples: 135
            precision: 1.0
            recall: 1.0
        8427100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lLovj60Imj8=
            n_samples: 236
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lLovj60Imj8=
            n_samples: 236
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lLovj60Imj8=
            n_samples: 236
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9786231884057971
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lLovj60Imj8=
            n_samples: 236
            precision: 0.6666666666666666
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lLovj60Imj8=
            n_samples: 236
            precision: 1.0
            recall: 1.0
        8427200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fTseSgjrtj8=
            n_samples: 525
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fTseSgjrtj8=
            n_samples: 525
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fTseSgjrtj8=
            n_samples: 525
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9548206178224874
            f1: 0.75
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fTseSgjrtj8=
            n_samples: 525
            precision: 0.8048780487804879
            recall: 0.7021276595744681
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fTseSgjrtj8=
            n_samples: 525
            precision: 1.0
            recall: 1.0
        8427900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lZhKTCWmoj8=
            n_samples: 906
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lZhKTCWmoj8=
            n_samples: 906
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lZhKTCWmoj8=
            n_samples: 906
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9610017702801208
            f1: 0.7017543859649122
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lZhKTCWmoj8=
            n_samples: 906
            precision: 0.8333333333333334
            recall: 0.6060606060606061
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lZhKTCWmoj8=
            n_samples: 906
            precision: 1.0
            recall: 1.0
        8428100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              27Zt27Ztmz8=
            n_samples: 784
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              27Zt27Ztmz8=
            n_samples: 784
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              27Zt27Ztmz8=
            n_samples: 784
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.945016538725582
            f1: 0.7619047619047619
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              27Zt27Ztmz8=
            n_samples: 784
            precision: 0.7619047619047619
            recall: 0.7619047619047619
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              27Zt27Ztmz8=
            n_samples: 784
            precision: 1.0
            recall: 1.0
        8428200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreuD8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreuD8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreuD8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9791511541325391
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreuD8=
            n_samples: 175
            precision: 0.9411764705882353
            recall: 0.9411764705882353
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreuD8=
            n_samples: 175
            precision: 1.0
            recall: 1.0
        8428390000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              99kFxKbSsD8=
            n_samples: 350
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              99kFxKbSsD8=
            n_samples: 350
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              99kFxKbSsD8=
            n_samples: 350
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9982715064486106
            f1: 0.8461538461538461
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              99kFxKbSsD8=
            n_samples: 350
            precision: 0.7586206896551724
            recall: 0.9565217391304348
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              99kFxKbSsD8=
            n_samples: 350
            precision: 1.0
            recall: 1.0
        8428900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              77333nvvvT8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              77333nvvvT8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              77333nvvvT8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9957880648716737
            f1: 0.8455284552845529
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              77333nvvvT8=
            n_samples: 496
            precision: 0.8
            recall: 0.896551724137931
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              77333nvvvT8=
            n_samples: 496
            precision: 1.0
            recall: 1.0
        8429110000:
          catboost:
            auc: 1.0
            f1: 0.9932432432432432
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j7gj7og70j8=
            n_samples: 516
            precision: 0.9865771812080537
            recall: 1.0
          lightgbm:
            auc: 0.9986265508913592
            f1: 0.9965870307167235
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j7gj7og70j8=
            n_samples: 516
            precision: 1.0
            recall: 0.9931972789115646
          logisticregression:
            auc: 0.9999999999999999
            f1: 0.9965870307167235
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j7gj7og70j8=
            n_samples: 516
            precision: 1.0
            recall: 0.9931972789115646
          randomforest:
            auc: 0.9511642055196062
            f1: 0.8473520249221184
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j7gj7og70j8=
            n_samples: 516
            precision: 0.7816091954022989
            recall: 0.9251700680272109
          xgboost:
            auc: 0.9989676087237063
            f1: 0.9965870307167235
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j7gj7og70j8=
            n_samples: 516
            precision: 1.0
            recall: 0.9931972789115646
        8429190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              doMp8lk3yD8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              doMp8lk3yD8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              doMp8lk3yD8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9351190476190476
            f1: 0.6818181818181818
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              doMp8lk3yD8=
            n_samples: 222
            precision: 0.6521739130434783
            recall: 0.7142857142857143
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              doMp8lk3yD8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
        8429200000:
          catboost:
            auc: 0.9998430933034463
            f1: 0.9907834101382489
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dl3XdV3X1T8=
            n_samples: 630
            precision: 0.9817351598173516
            recall: 1.0
          lightgbm:
            auc: 0.9984309330344635
            f1: 0.981042654028436
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dl3XdV3X1T8=
            n_samples: 630
            precision: 1.0
            recall: 0.9627906976744186
          logisticregression:
            auc: 0.9990081255253572
            f1: 0.981042654028436
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dl3XdV3X1T8=
            n_samples: 630
            precision: 1.0
            recall: 0.9627906976744186
          randomforest:
            auc: 0.951067525917624
            f1: 0.8380952380952381
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dl3XdV3X1T8=
            n_samples: 630
            precision: 0.8585365853658536
            recall: 0.8186046511627907
          xgboost:
            auc: 0.9935892406836649
            f1: 0.981042654028436
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dl3XdV3X1T8=
            n_samples: 630
            precision: 1.0
            recall: 0.9627906976744186
        8429400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NS68KXAdzz8=
            n_samples: 687
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NS68KXAdzz8=
            n_samples: 687
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NS68KXAdzz8=
            n_samples: 687
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9373906034085675
            f1: 0.7947882736156352
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NS68KXAdzz8=
            n_samples: 687
            precision: 0.8714285714285714
            recall: 0.7305389221556886
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NS68KXAdzz8=
            n_samples: 687
            precision: 1.0
            recall: 1.0
        8429510000:
          catboost:
            auc: 0.9999902026100247
            f1: 0.9967213114754099
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XMEcqUCgxz8=
            n_samples: 1647
            precision: 0.9934640522875817
            recall: 1.0
          lightgbm:
            auc: 0.9999804052200494
            f1: 0.9933774834437086
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XMEcqUCgxz8=
            n_samples: 1647
            precision: 1.0
            recall: 0.9868421052631579
          logisticregression:
            auc: 0.9999265195751852
            f1: 0.9933774834437086
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XMEcqUCgxz8=
            n_samples: 1647
            precision: 1.0
            recall: 0.9868421052631579
          randomforest:
            auc: 0.9554965807108986
            f1: 0.8041958041958042
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XMEcqUCgxz8=
            n_samples: 1647
            precision: 0.8582089552238806
            recall: 0.756578947368421
          xgboost:
            auc: 0.9983246463142219
            f1: 0.9933774834437086
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XMEcqUCgxz8=
            n_samples: 1647
            precision: 1.0
            recall: 0.9868421052631579
        8429520000:
          catboost:
            auc: 0.9999936937959564
            f1: 0.9988331388564761
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2XN/Gpdu1z8=
            n_samples: 1169
            precision: 0.9976689976689976
            recall: 1.0
          lightgbm:
            auc: 0.9995948263902027
            f1: 0.9941245593419507
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2XN/Gpdu1z8=
            n_samples: 1169
            precision: 1.0
            recall: 0.9883177570093458
          logisticregression:
            auc: 0.9998927945312599
            f1: 0.9941245593419507
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2XN/Gpdu1z8=
            n_samples: 1169
            precision: 1.0
            recall: 0.9883177570093458
          randomforest:
            auc: 0.915529973387819
            f1: 0.7168367346938775
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2XN/Gpdu1z8=
            n_samples: 1169
            precision: 0.7893258426966292
            recall: 0.6565420560747663
          xgboost:
            auc: 0.9974049970360841
            f1: 0.9941245593419507
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2XN/Gpdu1z8=
            n_samples: 1169
            precision: 1.0
            recall: 0.9883177570093458
        8429590000:
          catboost:
            auc: 0.9999985049121113
            f1: 0.9977220956719818
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6P8qFGRMzz8=
            n_samples: 2691
            precision: 0.9969650986342944
            recall: 0.9984802431610942
          lightgbm:
            auc: 0.999913284902453
            f1: 0.9984779299847792
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6P8qFGRMzz8=
            n_samples: 2691
            precision: 1.0
            recall: 0.9969604863221885
          logisticregression:
            auc: 0.9999783212256133
            f1: 0.9984779299847792
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6P8qFGRMzz8=
            n_samples: 2691
            precision: 1.0
            recall: 0.9969604863221885
          randomforest:
            auc: 0.946536404642547
            f1: 0.6933115823817292
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6P8qFGRMzz8=
            n_samples: 2691
            precision: 0.7482394366197183
            recall: 0.6458966565349544
          xgboost:
            auc: 0.999864694546069
            f1: 0.9984779299847792
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              6P8qFGRMzz8=
            n_samples: 2691
            precision: 1.0
            recall: 0.9969604863221885
        8430410000:
          catboost:
            auc: 1.0
            f1: 0.9928057553956835
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sHdMDewdwz8=
            n_samples: 462
            precision: 0.9857142857142858
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sHdMDewdwz8=
            n_samples: 462
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sHdMDewdwz8=
            n_samples: 462
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.992329534978058
            f1: 0.8701298701298701
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sHdMDewdwz8=
            n_samples: 462
            precision: 0.788235294117647
            recall: 0.9710144927536232
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sHdMDewdwz8=
            n_samples: 462
            precision: 1.0
            recall: 1.0
        8430490000:
          catboost:
            auc: 0.9999191919191919
            f1: 0.9782608695652174
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E4FcE4Fcsz8=
            n_samples: 595
            precision: 0.9574468085106383
            recall: 1.0
          lightgbm:
            auc: 0.9998787878787878
            f1: 0.9772727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E4FcE4Fcsz8=
            n_samples: 595
            precision: 1.0
            recall: 0.9555555555555556
          logisticregression:
            auc: 0.9997171717171718
            f1: 0.9772727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E4FcE4Fcsz8=
            n_samples: 595
            precision: 1.0
            recall: 0.9555555555555556
          randomforest:
            auc: 0.9804242424242424
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E4FcE4Fcsz8=
            n_samples: 595
            precision: 0.76
            recall: 0.8444444444444444
          xgboost:
            auc: 0.9954343434343433
            f1: 0.9772727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E4FcE4Fcsz8=
            n_samples: 595
            precision: 1.0
            recall: 0.9555555555555556
        8430690000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rIikIUCKnz8=
            n_samples: 487
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rIikIUCKnz8=
            n_samples: 487
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rIikIUCKnz8=
            n_samples: 487
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9781779661016949
            f1: 0.65
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rIikIUCKnz8=
            n_samples: 487
            precision: 0.52
            recall: 0.8666666666666667
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rIikIUCKnz8=
            n_samples: 487
            precision: 1.0
            recall: 1.0
        8431100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GYnVcPFDqD8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GYnVcPFDqD8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GYnVcPFDqD8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9582089552238806
            f1: 0.3333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GYnVcPFDqD8=
            n_samples: 211
            precision: 1.0
            recall: 0.2
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GYnVcPFDqD8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
        8431200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7ViBMNKOlT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7ViBMNKOlT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7ViBMNKOlT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.7058823529411765
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7ViBMNKOlT8=
            n_samples: 285
            precision: 0.5454545454545454
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7ViBMNKOlT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
        8431310000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2cj8=
            n_samples: 216
            precision: 1.0
            recall: 1.0
        8431390000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UzIlUzIlsz8=
            n_samples: 936
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UzIlUzIlsz8=
            n_samples: 936
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UzIlUzIlsz8=
            n_samples: 936
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9982266578686902
            f1: 0.971830985915493
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UzIlUzIlsz8=
            n_samples: 936
            precision: 0.9583333333333334
            recall: 0.9857142857142858
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UzIlUzIlsz8=
            n_samples: 936
            precision: 1.0
            recall: 1.0
        8431410000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CqtbFnw9pT8=
            n_samples: 458
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CqtbFnw9pT8=
            n_samples: 458
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CqtbFnw9pT8=
            n_samples: 458
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9864524637333654
            f1: 0.8780487804878049
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CqtbFnw9pT8=
            n_samples: 458
            precision: 0.8181818181818182
            recall: 0.9473684210526315
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CqtbFnw9pT8=
            n_samples: 458
            precision: 1.0
            recall: 1.0
        8431430000:
          catboost:
            auc: 1.0
            f1: 0.9915966386554622
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C9u8kDzVlj8=
            n_samples: 2646
            precision: 0.9833333333333333
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C9u8kDzVlj8=
            n_samples: 2646
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C9u8kDzVlj8=
            n_samples: 2646
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9762797036027596
            f1: 0.696969696969697
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C9u8kDzVlj8=
            n_samples: 2646
            precision: 0.6301369863013698
            recall: 0.7796610169491526
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C9u8kDzVlj8=
            n_samples: 2646
            precision: 1.0
            recall: 1.0
        8431490000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              blj9b3sopD8=
            n_samples: 6172
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              blj9b3sopD8=
            n_samples: 6172
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              blj9b3sopD8=
            n_samples: 6172
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9789661196587603
            f1: 0.8626609442060086
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              blj9b3sopD8=
            n_samples: 6172
            precision: 0.9013452914798207
            recall: 0.8271604938271605
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              blj9b3sopD8=
            n_samples: 6172
            precision: 1.0
            recall: 1.0
        8432290000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 120
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 120
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 120
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 120
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 120
            precision: 0.0
            recall: 0.0
        8432800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hBBCCCGEkD8=
            n_samples: 248
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hBBCCCGEkD8=
            n_samples: 248
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hBBCCCGEkD8=
            n_samples: 248
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9928278688524591
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hBBCCCGEkD8=
            n_samples: 248
            precision: 0.5
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hBBCCCGEkD8=
            n_samples: 248
            precision: 1.0
            recall: 1.0
        8432900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izm2a6qbhz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izm2a6qbhz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izm2a6qbhz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9566326530612245
            f1: 0.2857142857142857
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izm2a6qbhz8=
            n_samples: 347
            precision: 0.3333333333333333
            recall: 0.25
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izm2a6qbhz8=
            n_samples: 347
            precision: 1.0
            recall: 1.0
        8433510000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0Qu90Au9wD8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0Qu90Au9wD8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0Qu90Au9wD8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9945340968245705
            f1: 0.9696969696969697
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0Qu90Au9wD8=
            n_samples: 130
            precision: 1.0
            recall: 0.9411764705882353
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0Qu90Au9wD8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
        8433900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gUPzuf54mT8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gUPzuf54mT8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gUPzuf54mT8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gUPzuf54mT8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gUPzuf54mT8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
        8436210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2oj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2oj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2oj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2oj8=
            n_samples: 108
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2oj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
        8436290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tQojRPYlnz8=
            n_samples: 263
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tQojRPYlnz8=
            n_samples: 263
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tQojRPYlnz8=
            n_samples: 263
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9936274509803922
            f1: 0.7058823529411765
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tQojRPYlnz8=
            n_samples: 263
            precision: 0.6666666666666666
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tQojRPYlnz8=
            n_samples: 263
            precision: 1.0
            recall: 1.0
        8436800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 208
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 208
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 208
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.996936274509804
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 208
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 208
            precision: 1.0
            recall: 1.0
        8436910000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +n5qvHSTmD8=
            n_samples: 125
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +n5qvHSTmD8=
            n_samples: 125
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +n5qvHSTmD8=
            n_samples: 125
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +n5qvHSTmD8=
            n_samples: 125
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +n5qvHSTmD8=
            n_samples: 125
            precision: 1.0
            recall: 1.0
        8436990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KeLQSfsgeT8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KeLQSfsgeT8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KeLQSfsgeT8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KeLQSfsgeT8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KeLQSfsgeT8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
        8437100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +mtDEypVeD8=
            n_samples: 505
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +mtDEypVeD8=
            n_samples: 505
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +mtDEypVeD8=
            n_samples: 505
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.6789508632138114
            f1: 0.3333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +mtDEypVeD8=
            n_samples: 505
            precision: 0.3333333333333333
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +mtDEypVeD8=
            n_samples: 505
            precision: 1.0
            recall: 1.0
        8437800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E+idupaDuz8=
            n_samples: 214
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E+idupaDuz8=
            n_samples: 214
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E+idupaDuz8=
            n_samples: 214
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9592533576143866
            f1: 0.75
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E+idupaDuz8=
            n_samples: 214
            precision: 0.8823529411764706
            recall: 0.6521739130434783
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              E+idupaDuz8=
            n_samples: 214
            precision: 1.0
            recall: 1.0
        8437900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tjcxMN/fdT8=
            n_samples: 749
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tjcxMN/fdT8=
            n_samples: 749
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tjcxMN/fdT8=
            n_samples: 749
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tjcxMN/fdT8=
            n_samples: 749
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tjcxMN/fdT8=
            n_samples: 749
            precision: 1.0
            recall: 1.0
        8438100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVlT8=
            n_samples: 432
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVlT8=
            n_samples: 432
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVlT8=
            n_samples: 432
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9473684210526315
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVlT8=
            n_samples: 432
            precision: 0.9
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVlT8=
            n_samples: 432
            precision: 1.0
            recall: 1.0
        8438500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bgT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bgT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bgT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bgT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bgT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
        8438600000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4MCBAwcOjD8=
            n_samples: 146
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4MCBAwcOjD8=
            n_samples: 146
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4MCBAwcOjD8=
            n_samples: 146
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4MCBAwcOjD8=
            n_samples: 146
            precision: 0.6666666666666666
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4MCBAwcOjD8=
            n_samples: 146
            precision: 1.0
            recall: 1.0
        8438800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2cBnDEcLoj8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2cBnDEcLoj8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2cBnDEcLoj8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9614012557077625
            f1: 0.6896551724137931
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2cBnDEcLoj8=
            n_samples: 454
            precision: 0.7692307692307693
            recall: 0.625
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2cBnDEcLoj8=
            n_samples: 454
            precision: 1.0
            recall: 1.0
        8438900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dYzdij5usD8=
            n_samples: 483
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dYzdij5usD8=
            n_samples: 483
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dYzdij5usD8=
            n_samples: 483
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9854410505281188
            f1: 0.6896551724137931
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dYzdij5usD8=
            n_samples: 483
            precision: 0.7407407407407407
            recall: 0.6451612903225806
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dYzdij5usD8=
            n_samples: 483
            precision: 1.0
            recall: 1.0
        8441800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9M3RN0ffvD8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9M3RN0ffvD8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9M3RN0ffvD8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9737288135593221
            f1: 0.8148148148148148
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9M3RN0ffvD8=
            n_samples: 133
            precision: 0.9166666666666666
            recall: 0.7333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9M3RN0ffvD8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
        8441900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0Zz8=
            n_samples: 342
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0Zz8=
            n_samples: 342
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0Zz8=
            n_samples: 342
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0Zz8=
            n_samples: 342
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0Zz8=
            n_samples: 342
            precision: 1.0
            recall: 1.0
        8442500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z9Cy4zmVdD8=
            n_samples: 199
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z9Cy4zmVdD8=
            n_samples: 199
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z9Cy4zmVdD8=
            n_samples: 199
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z9Cy4zmVdD8=
            n_samples: 199
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z9Cy4zmVdD8=
            n_samples: 199
            precision: 1.0
            recall: 1.0
        8443190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              G3DFGnDFmj8=
            n_samples: 153
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              G3DFGnDFmj8=
            n_samples: 153
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              G3DFGnDFmj8=
            n_samples: 153
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9706375838926173
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              G3DFGnDFmj8=
            n_samples: 153
            precision: 1.0
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              G3DFGnDFmj8=
            n_samples: 153
            precision: 1.0
            recall: 1.0
        8443311000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              uxK1K1G7gj8=
            n_samples: 328
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              uxK1K1G7gj8=
            n_samples: 328
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              uxK1K1G7gj8=
            n_samples: 328
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9441025641025641
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              uxK1K1G7gj8=
            n_samples: 328
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              uxK1K1G7gj8=
            n_samples: 328
            precision: 1.0
            recall: 1.0
        8443319000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 169
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 169
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 169
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 169
            precision: 1.0
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 169
            precision: 1.0
            recall: 1.0
        8443321000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HKAuObUmbD8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HKAuObUmbD8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HKAuObUmbD8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9551724137931035
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HKAuObUmbD8=
            n_samples: 291
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HKAuObUmbD8=
            n_samples: 291
            precision: 1.0
            recall: 1.0
        8443329000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              H5lA79S1nD8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              H5lA79S1nD8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              H5lA79S1nD8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              H5lA79S1nD8=
            n_samples: 107
            precision: 1.0
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              H5lA79S1nD8=
            n_samples: 107
            precision: 1.0
            recall: 1.0
        8443391000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 118
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 118
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 118
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 118
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 118
            precision: 0.0
            recall: 0.0
        8443399000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 256
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 256
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 256
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 256
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 256
            precision: 0.0
            recall: 0.0
        8443910000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 162
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 162
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 162
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 162
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 162
            precision: 0.0
            recall: 0.0
        8443990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              17pUMmdhcT8=
            n_samples: 707
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              17pUMmdhcT8=
            n_samples: 707
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              17pUMmdhcT8=
            n_samples: 707
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9209280303030303
            f1: 0.4
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              17pUMmdhcT8=
            n_samples: 707
            precision: 0.5
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              17pUMmdhcT8=
            n_samples: 707
            precision: 1.0
            recall: 1.0
        8450110000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mpmZmZmZ5z8=
            n_samples: 240
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mpmZmZmZ5z8=
            n_samples: 240
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mpmZmZmZ5z8=
            n_samples: 240
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9642632947717693
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mpmZmZmZ5z8=
            n_samples: 240
            precision: 1.0
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mpmZmZmZ5z8=
            n_samples: 240
            precision: 1.0
            recall: 1.0
        8450190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YYp81g2m6D8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YYp81g2m6D8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YYp81g2m6D8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.979360165118679
            f1: 0.40186915887850466
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YYp81g2m6D8=
            n_samples: 222
            precision: 1.0
            recall: 0.25146198830409355
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YYp81g2m6D8=
            n_samples: 222
            precision: 1.0
            recall: 1.0
        8450200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              f5bzZzl/5j8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              f5bzZzl/5j8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              f5bzZzl/5j8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9597114707952147
            f1: 0.5644171779141104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              f5bzZzl/5j8=
            n_samples: 165
            precision: 0.9787234042553191
            recall: 0.39655172413793105
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              f5bzZzl/5j8=
            n_samples: 165
            precision: 1.0
            recall: 1.0
        8452100000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 324
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 324
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 324
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 324
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 324
            precision: 0.0
            recall: 0.0
        8452290000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 346
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 346
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 346
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 346
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 346
            precision: 0.0
            recall: 0.0
        8452900000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 195
            precision: 0.0
            recall: 0.0
        8455900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRCso837pD8=
            n_samples: 366
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRCso837pD8=
            n_samples: 366
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRCso837pD8=
            n_samples: 366
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9748338081671414
            f1: 0.7692307692307693
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRCso837pD8=
            n_samples: 366
            precision: 0.9090909090909091
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRCso837pD8=
            n_samples: 366
            precision: 1.0
            recall: 1.0
        8462290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERsT8=
            n_samples: 105
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERsT8=
            n_samples: 105
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERsT8=
            n_samples: 105
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9978134110787171
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERsT8=
            n_samples: 105
            precision: 0.8571428571428571
            recall: 0.8571428571428571
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERsT8=
            n_samples: 105
            precision: 1.0
            recall: 1.0
        8465990000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 205
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 205
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 205
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 205
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 205
            precision: 0.0
            recall: 0.0
        8466940000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJkj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
        8467210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              F2iBFmiBdj8=
            n_samples: 364
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              F2iBFmiBdj8=
            n_samples: 364
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              F2iBFmiBdj8=
            n_samples: 364
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9212707182320442
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              F2iBFmiBdj8=
            n_samples: 364
            precision: 1.0
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              F2iBFmiBdj8=
            n_samples: 364
            precision: 1.0
            recall: 1.0
        8467290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 338
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 338
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 338
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9866967871485943
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 338
            precision: 1.0
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Jg8GnHEtkj8=
            n_samples: 338
            precision: 1.0
            recall: 1.0
        8467890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FVABFVABhT8=
            n_samples: 195
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FVABFVABhT8=
            n_samples: 195
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FVABFVABhT8=
            n_samples: 195
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9922279792746114
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FVABFVABhT8=
            n_samples: 195
            precision: 0.5
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FVABFVABhT8=
            n_samples: 195
            precision: 1.0
            recall: 1.0
        8467990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 104
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 104
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 104
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9607843137254902
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 104
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FDuxEzuxkz8=
            n_samples: 104
            precision: 1.0
            recall: 1.0
        8471301000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bIo+d8vE7D8=
            n_samples: 307
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9963636363636363
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bIo+d8vE7D8=
            n_samples: 307
            precision: 1.0
            recall: 0.9927536231884058
          logisticregression:
            auc: 0.9981884057971014
            f1: 0.9963636363636363
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bIo+d8vE7D8=
            n_samples: 307
            precision: 1.0
            recall: 0.9927536231884058
          randomforest:
            auc: 0.990825151940159
            f1: 0.9606003752345216
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bIo+d8vE7D8=
            n_samples: 307
            precision: 0.9961089494163424
            recall: 0.927536231884058
          xgboost:
            auc: 0.9998831229546518
            f1: 0.9963636363636363
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bIo+d8vE7D8=
            n_samples: 307
            precision: 1.0
            recall: 0.9927536231884058
        8471309000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pHvMKm7n5D8=
            n_samples: 1938
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9863891112890312
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pHvMKm7n5D8=
            n_samples: 1938
            precision: 1.0
            recall: 0.9731437598736177
          logisticregression:
            auc: 0.9909632889490709
            f1: 0.9859943977591037
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pHvMKm7n5D8=
            n_samples: 1938
            precision: 0.9991889699918897
            recall: 0.9731437598736177
          randomforest:
            auc: 0.9820100334762657
            f1: 0.955964771817454
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pHvMKm7n5D8=
            n_samples: 1938
            precision: 0.9691558441558441
            recall: 0.943127962085308
          xgboost:
            auc: 0.9999200707139096
            f1: 0.9863891112890312
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              pHvMKm7n5D8=
            n_samples: 1938
            precision: 1.0
            recall: 0.9731437598736177
        8471419000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              z0jF3OqM5D8=
            n_samples: 545
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9884393063583815
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              z0jF3OqM5D8=
            n_samples: 545
            precision: 1.0
            recall: 0.9771428571428571
          logisticregression:
            auc: 0.9932893772893772
            f1: 0.9884393063583815
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              z0jF3OqM5D8=
            n_samples: 545
            precision: 1.0
            recall: 0.9771428571428571
          randomforest:
            auc: 0.9874725274725276
            f1: 0.9099378881987578
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              z0jF3OqM5D8=
            n_samples: 545
            precision: 0.9965986394557823
            recall: 0.8371428571428572
          xgboost:
            auc: 0.9994725274725275
            f1: 0.9884393063583815
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              z0jF3OqM5D8=
            n_samples: 545
            precision: 1.0
            recall: 0.9771428571428571
        8471499000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2QboU9qD4j8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9941860465116279
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2QboU9qD4j8=
            n_samples: 299
            precision: 1.0
            recall: 0.9884393063583815
          logisticregression:
            auc: 0.9962152491054225
            f1: 0.9941860465116279
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2QboU9qD4j8=
            n_samples: 299
            precision: 1.0
            recall: 0.9884393063583815
          randomforest:
            auc: 0.9849527479585282
            f1: 0.89171974522293
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2QboU9qD4j8=
            n_samples: 299
            precision: 0.9929078014184397
            recall: 0.8092485549132948
          xgboost:
            auc: 0.9999082484631618
            f1: 0.9941860465116279
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2QboU9qD4j8=
            n_samples: 299
            precision: 1.0
            recall: 0.9884393063583815
        8471509000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ScTTr55P4D8=
            n_samples: 463
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.980561555075594
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ScTTr55P4D8=
            n_samples: 463
            precision: 1.0
            recall: 0.961864406779661
          logisticregression:
            auc: 0.994297394161129
            f1: 0.980561555075594
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ScTTr55P4D8=
            n_samples: 463
            precision: 1.0
            recall: 0.961864406779661
          randomforest:
            auc: 0.9789068916598223
            f1: 0.9124423963133641
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ScTTr55P4D8=
            n_samples: 463
            precision: 1.0
            recall: 0.8389830508474576
          xgboost:
            auc: 0.9994960053759426
            f1: 0.980561555075594
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ScTTr55P4D8=
            n_samples: 463
            precision: 1.0
            recall: 0.961864406779661
        8471609000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TNx+idsv4T8=
            n_samples: 337
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9916434540389972
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TNx+idsv4T8=
            n_samples: 337
            precision: 1.0
            recall: 0.9834254143646409
          logisticregression:
            auc: 0.9958386457005242
            f1: 0.9916434540389972
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TNx+idsv4T8=
            n_samples: 337
            precision: 1.0
            recall: 0.9834254143646409
          randomforest:
            auc: 0.9573948151296218
            f1: 0.8454258675078864
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TNx+idsv4T8=
            n_samples: 337
            precision: 0.9852941176470589
            recall: 0.7403314917127072
          xgboost:
            auc: 0.9998406289842754
            f1: 0.9916434540389972
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TNx+idsv4T8=
            n_samples: 337
            precision: 1.0
            recall: 0.9834254143646409
        8471709000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DhBS3Ncy2T8=
            n_samples: 287
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9727272727272728
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DhBS3Ncy2T8=
            n_samples: 287
            precision: 1.0
            recall: 0.9469026548672567
          logisticregression:
            auc: 0.9880225816295393
            f1: 0.9727272727272728
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DhBS3Ncy2T8=
            n_samples: 287
            precision: 1.0
            recall: 0.9469026548672567
          randomforest:
            auc: 0.971060929712135
            f1: 0.7675675675675676
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DhBS3Ncy2T8=
            n_samples: 287
            precision: 0.9861111111111112
            recall: 0.6283185840707964
          xgboost:
            auc: 0.9996948428440647
            f1: 0.9727272727272728
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DhBS3Ncy2T8=
            n_samples: 287
            precision: 1.0
            recall: 0.9469026548672567
        8471809000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              d7M8iUzD5j8=
            n_samples: 194
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9927007299270073
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              d7M8iUzD5j8=
            n_samples: 194
            precision: 1.0
            recall: 0.9855072463768116
          logisticregression:
            auc: 0.9912008281573499
            f1: 0.9927007299270073
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              d7M8iUzD5j8=
            n_samples: 194
            precision: 1.0
            recall: 0.9855072463768116
          randomforest:
            auc: 0.9805253623188406
            f1: 0.8734693877551021
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              d7M8iUzD5j8=
            n_samples: 194
            precision: 1.0
            recall: 0.7753623188405797
          xgboost:
            auc: 0.9998706004140786
            f1: 0.9927007299270073
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              d7M8iUzD5j8=
            n_samples: 194
            precision: 1.0
            recall: 0.9855072463768116
        8471900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CkHZ+5oQ5D8=
            n_samples: 555
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9956709956709957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CkHZ+5oQ5D8=
            n_samples: 555
            precision: 1.0
            recall: 0.9913793103448276
          logisticregression:
            auc: 0.9969876173024599
            f1: 0.9956709956709957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CkHZ+5oQ5D8=
            n_samples: 555
            precision: 1.0
            recall: 0.9913793103448276
          randomforest:
            auc: 0.9921081126103616
            f1: 0.9158878504672897
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CkHZ+5oQ5D8=
            n_samples: 555
            precision: 1.0
            recall: 0.8448275862068966
          xgboost:
            auc: 0.9999167083125104
            f1: 0.9956709956709957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              CkHZ+5oQ5D8=
            n_samples: 555
            precision: 1.0
            recall: 0.9913793103448276
        8472900000:
          catboost:
            auc: 1.0
            f1: 0.9002695417789758
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qGUM3AHh2T8=
            n_samples: 413
            precision: 0.8186274509803921
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qGUM3AHh2T8=
            n_samples: 413
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qGUM3AHh2T8=
            n_samples: 413
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.892203398081885
            f1: 0.8156028368794326
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qGUM3AHh2T8=
            n_samples: 413
            precision: 1.0
            recall: 0.688622754491018
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              qGUM3AHh2T8=
            n_samples: 413
            precision: 1.0
            recall: 1.0
        8473300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O7kOswUc3z8=
            n_samples: 539
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O7kOswUc3z8=
            n_samples: 539
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O7kOswUc3z8=
            n_samples: 539
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9903660886319846
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O7kOswUc3z8=
            n_samples: 539
            precision: 1.0
            recall: 0.9809160305343512
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O7kOswUc3z8=
            n_samples: 539
            precision: 1.0
            recall: 1.0
        8474100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +s3Wod9svT8=
            n_samples: 261
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +s3Wod9svT8=
            n_samples: 261
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +s3Wod9svT8=
            n_samples: 261
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.971933621933622
            f1: 0.7936507936507936
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +s3Wod9svT8=
            n_samples: 261
            precision: 0.7575757575757576
            recall: 0.8333333333333334
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +s3Wod9svT8=
            n_samples: 261
            precision: 1.0
            recall: 1.0
        8474200000:
          catboost:
            auc: 0.9999762808349146
            f1: 0.9876543209876543
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0fBbmq7ewD8=
            n_samples: 607
            precision: 0.975609756097561
            recall: 1.0
          lightgbm:
            auc: 0.9999525616698292
            f1: 0.9937106918238994
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0fBbmq7ewD8=
            n_samples: 607
            precision: 1.0
            recall: 0.9875
          logisticregression:
            auc: 0.9998339658444023
            f1: 0.9937106918238994
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0fBbmq7ewD8=
            n_samples: 607
            precision: 1.0
            recall: 0.9875
          randomforest:
            auc: 0.9816888045540797
            f1: 0.8287292817679558
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0fBbmq7ewD8=
            n_samples: 607
            precision: 0.7425742574257426
            recall: 0.9375
          xgboost:
            auc: 0.9981143263757116
            f1: 0.9937106918238994
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0fBbmq7ewD8=
            n_samples: 607
            precision: 1.0
            recall: 0.9875
        8474310000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j+YPNh7Nrz8=
            n_samples: 644
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9990687086092715
            f1: 0.9873417721518988
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j+YPNh7Nrz8=
            n_samples: 644
            precision: 1.0
            recall: 0.975
          logisticregression:
            auc: 1.0
            f1: 0.9873417721518988
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j+YPNh7Nrz8=
            n_samples: 644
            precision: 1.0
            recall: 0.975
          randomforest:
            auc: 0.9778145695364239
            f1: 0.6829268292682927
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j+YPNh7Nrz8=
            n_samples: 644
            precision: 0.6666666666666666
            recall: 0.7
          xgboost:
            auc: 0.9993584437086093
            f1: 0.9873417721518988
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              j+YPNh7Nrz8=
            n_samples: 644
            precision: 1.0
            recall: 0.975
        8474320000:
          catboost:
            auc: 0.9994285714285714
            f1: 0.9722222222222222
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              l2/5lm/51j8=
            n_samples: 195
            precision: 0.9459459459459459
            recall: 1.0
          lightgbm:
            auc: 0.9986285714285714
            f1: 0.9781021897810219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              l2/5lm/51j8=
            n_samples: 195
            precision: 1.0
            recall: 0.9571428571428572
          logisticregression:
            auc: 0.9988571428571429
            f1: 0.9781021897810219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              l2/5lm/51j8=
            n_samples: 195
            precision: 1.0
            recall: 0.9571428571428572
          randomforest:
            auc: 0.9272571428571429
            f1: 0.782608695652174
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              l2/5lm/51j8=
            n_samples: 195
            precision: 0.7941176470588235
            recall: 0.7714285714285715
          xgboost:
            auc: 0.9923428571428572
            f1: 0.9781021897810219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              l2/5lm/51j8=
            n_samples: 195
            precision: 1.0
            recall: 0.9571428571428572
        8474390090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              khu5kRu5wT8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              khu5kRu5wT8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              khu5kRu5wT8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9598214285714285
            f1: 0.8235294117647058
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              khu5kRu5wT8=
            n_samples: 130
            precision: 0.875
            recall: 0.7777777777777778
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              khu5kRu5wT8=
            n_samples: 130
            precision: 1.0
            recall: 1.0
        8474800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ln87x8dKqz8=
            n_samples: 469
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ln87x8dKqz8=
            n_samples: 469
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ln87x8dKqz8=
            n_samples: 469
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9590990990990991
            f1: 0.76
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ln87x8dKqz8=
            n_samples: 469
            precision: 0.76
            recall: 0.76
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ln87x8dKqz8=
            n_samples: 469
            precision: 1.0
            recall: 1.0
        8474900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cMMEJXAIsj8=
            n_samples: 3762
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cMMEJXAIsj8=
            n_samples: 3762
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cMMEJXAIsj8=
            n_samples: 3762
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.976634419799181
            f1: 0.833976833976834
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cMMEJXAIsj8=
            n_samples: 3762
            precision: 0.8537549407114624
            recall: 0.8150943396226416
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cMMEJXAIsj8=
            n_samples: 3762
            precision: 1.0
            recall: 1.0
        8477100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TjbZZJNNxj8=
            n_samples: 132
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TjbZZJNNxj8=
            n_samples: 132
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TjbZZJNNxj8=
            n_samples: 132
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9164339848424411
            f1: 0.7804878048780488
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TjbZZJNNxj8=
            n_samples: 132
            precision: 0.8888888888888888
            recall: 0.6956521739130435
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TjbZZJNNxj8=
            n_samples: 132
            precision: 1.0
            recall: 1.0
        8477300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cJd6Mn67xD8=
            n_samples: 142
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cJd6Mn67xD8=
            n_samples: 142
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cJd6Mn67xD8=
            n_samples: 142
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.957983193277311
            f1: 0.8780487804878049
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cJd6Mn67xD8=
            n_samples: 142
            precision: 1.0
            recall: 0.782608695652174
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cJd6Mn67xD8=
            n_samples: 142
            precision: 1.0
            recall: 1.0
        8477800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UV5DeQ3ltT8=
            n_samples: 304
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UV5DeQ3ltT8=
            n_samples: 304
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UV5DeQ3ltT8=
            n_samples: 304
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9412700608743774
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UV5DeQ3ltT8=
            n_samples: 304
            precision: 0.9473684210526315
            recall: 0.6923076923076923
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UV5DeQ3ltT8=
            n_samples: 304
            precision: 1.0
            recall: 1.0
        8477900000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 481
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 481
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 481
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 481
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 481
            precision: 0.0
            recall: 0.0
        8478900000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 128
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 128
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 128
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 128
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 128
            precision: 0.0
            recall: 0.0
        8479100000:
          catboost:
            auc: 0.9999496069340859
            f1: 0.9879518072289156
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2wocs06Lwj8=
            n_samples: 283
            precision: 0.9761904761904762
            recall: 1.0
          lightgbm:
            auc: 0.9998992138681717
            f1: 0.975
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2wocs06Lwj8=
            n_samples: 283
            precision: 1.0
            recall: 0.9512195121951219
          logisticregression:
            auc: 0.9986897802862326
            f1: 0.975
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2wocs06Lwj8=
            n_samples: 283
            precision: 1.0
            recall: 0.9512195121951219
          randomforest:
            auc: 0.9581737552912719
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2wocs06Lwj8=
            n_samples: 283
            precision: 0.8205128205128205
            recall: 0.7804878048780488
          xgboost:
            auc: 0.9967244507155816
            f1: 0.975
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2wocs06Lwj8=
            n_samples: 283
            precision: 1.0
            recall: 0.9512195121951219
        8479200000:
          catboost:
            auc: 1.0
            f1: 0.9444444444444444
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8RVf8RVfwT8=
            n_samples: 140
            precision: 1.0
            recall: 0.8947368421052632
          lightgbm:
            auc: 1.0
            f1: 0.9444444444444444
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8RVf8RVfwT8=
            n_samples: 140
            precision: 1.0
            recall: 0.8947368421052632
          logisticregression:
            auc: 0.9473684210526316
            f1: 0.9444444444444444
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8RVf8RVfwT8=
            n_samples: 140
            precision: 1.0
            recall: 0.8947368421052632
          randomforest:
            auc: 0.9830361026533275
            f1: 0.85
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8RVf8RVfwT8=
            n_samples: 140
            precision: 0.8095238095238095
            recall: 0.8947368421052632
          xgboost:
            auc: 0.9843410178338408
            f1: 0.9444444444444444
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8RVf8RVfwT8=
            n_samples: 140
            precision: 1.0
            recall: 0.8947368421052632
        8479600000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cB/BfQT3kT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cB/BfQT3kT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cB/BfQT3kT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8888888888888888
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cB/BfQT3kT8=
            n_samples: 285
            precision: 1.0
            recall: 0.8
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cB/BfQT3kT8=
            n_samples: 285
            precision: 1.0
            recall: 1.0
        8479820000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mK+63pPgqT8=
            n_samples: 277
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mK+63pPgqT8=
            n_samples: 277
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mK+63pPgqT8=
            n_samples: 277
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9945681694731126
            f1: 0.9285714285714286
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mK+63pPgqT8=
            n_samples: 277
            precision: 0.9285714285714286
            recall: 0.9285714285714286
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mK+63pPgqT8=
            n_samples: 277
            precision: 1.0
            recall: 1.0
        8479890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              df3iToNyuT8=
            n_samples: 1509
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              df3iToNyuT8=
            n_samples: 1509
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              df3iToNyuT8=
            n_samples: 1509
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9888741721854305
            f1: 0.9305555555555556
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              df3iToNyuT8=
            n_samples: 1509
            precision: 0.9710144927536232
            recall: 0.8933333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              df3iToNyuT8=
            n_samples: 1509
            precision: 1.0
            recall: 1.0
        8479900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jMxrCVw0oz8=
            n_samples: 1413
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jMxrCVw0oz8=
            n_samples: 1413
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jMxrCVw0oz8=
            n_samples: 1413
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.967806603773585
            f1: 0.8775510204081632
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jMxrCVw0oz8=
            n_samples: 1413
            precision: 0.9555555555555556
            recall: 0.8113207547169812
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jMxrCVw0oz8=
            n_samples: 1413
            precision: 1.0
            recall: 1.0
        8480710000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UybBrsKXqz8=
            n_samples: 167
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UybBrsKXqz8=
            n_samples: 167
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UybBrsKXqz8=
            n_samples: 167
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9827707454289734
            f1: 0.875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UybBrsKXqz8=
            n_samples: 167
            precision: 1.0
            recall: 0.7777777777777778
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UybBrsKXqz8=
            n_samples: 167
            precision: 1.0
            recall: 1.0
        8480790000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Y8zxdK1hT8=
            n_samples: 283
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Y8zxdK1hT8=
            n_samples: 283
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Y8zxdK1hT8=
            n_samples: 283
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8666666666666667
            f1: 0.3333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Y8zxdK1hT8=
            n_samples: 283
            precision: 0.3333333333333333
            recall: 0.3333333333333333
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Y8zxdK1hT8=
            n_samples: 283
            precision: 1.0
            recall: 1.0
        8481100000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 174
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 174
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 174
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 174
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 174
            precision: 0.0
            recall: 0.0
        8481200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3b6yepc8eD8=
            n_samples: 676
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3b6yepc8eD8=
            n_samples: 676
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3b6yepc8eD8=
            n_samples: 676
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8863467261904763
            f1: 0.4
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3b6yepc8eD8=
            n_samples: 676
            precision: 1.0
            recall: 0.25
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3b6yepc8eD8=
            n_samples: 676
            precision: 1.0
            recall: 1.0
        8481300000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 286
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 286
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 286
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 286
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 286
            precision: 0.0
            recall: 0.0
        8481400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              MchZsr/WpT8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              MchZsr/WpT8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              MchZsr/WpT8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.7142857142857143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              MchZsr/WpT8=
            n_samples: 211
            precision: 1.0
            recall: 0.5555555555555556
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              MchZsr/WpT8=
            n_samples: 211
            precision: 1.0
            recall: 1.0
        8481800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              eVqgPhZhnD8=
            n_samples: 4799
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              eVqgPhZhnD8=
            n_samples: 4799
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              eVqgPhZhnD8=
            n_samples: 4799
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9890618101189537
            f1: 0.8177777777777778
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              eVqgPhZhnD8=
            n_samples: 4799
            precision: 1.0
            recall: 0.6917293233082706
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              eVqgPhZhnD8=
            n_samples: 4799
            precision: 1.0
            recall: 1.0
        8481900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Ym8a/QdkT8=
            n_samples: 1017
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Ym8a/QdkT8=
            n_samples: 1017
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Ym8a/QdkT8=
            n_samples: 1017
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9512352941176471
            f1: 0.8666666666666667
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Ym8a/QdkT8=
            n_samples: 1017
            precision: 1.0
            recall: 0.7647058823529411
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              8Ym8a/QdkT8=
            n_samples: 1017
            precision: 1.0
            recall: 1.0
        8482100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGZD8=
            n_samples: 404
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGZD8=
            n_samples: 404
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGZD8=
            n_samples: 404
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGZD8=
            n_samples: 404
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGZD8=
            n_samples: 404
            precision: 1.0
            recall: 1.0
        8482200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1bG3Nkxzlz8=
            n_samples: 131
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1bG3Nkxzlz8=
            n_samples: 131
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1bG3Nkxzlz8=
            n_samples: 131
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1bG3Nkxzlz8=
            n_samples: 131
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1bG3Nkxzlz8=
            n_samples: 131
            precision: 1.0
            recall: 1.0
        8482300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iob449blfT8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iob449blfT8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iob449blfT8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iob449blfT8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              iob449blfT8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
        8482800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jliz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jliz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jliz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9614406779661018
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jliz8=
            n_samples: 299
            precision: 1.0
            recall: 0.75
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Be6+4+Jliz8=
            n_samples: 299
            precision: 1.0
            recall: 1.0
        8482910000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wOwBswfMfj8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wOwBswfMfj8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wOwBswfMfj8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wOwBswfMfj8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wOwBswfMfj8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
        8482990000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtej8=
            n_samples: 155
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtej8=
            n_samples: 155
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtej8=
            n_samples: 155
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtej8=
            n_samples: 155
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bRrQpgFtej8=
            n_samples: 155
            precision: 1.0
            recall: 1.0
        8483100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              peLsw2fYlT8=
            n_samples: 375
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              peLsw2fYlT8=
            n_samples: 375
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              peLsw2fYlT8=
            n_samples: 375
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9947207084468664
            f1: 0.9333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              peLsw2fYlT8=
            n_samples: 375
            precision: 1.0
            recall: 0.875
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              peLsw2fYlT8=
            n_samples: 375
            precision: 1.0
            recall: 1.0
        8483200000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 136
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 136
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 136
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 136
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 136
            precision: 0.0
            recall: 0.0
        8483300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rpoxyn/QgD8=
            n_samples: 609
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rpoxyn/QgD8=
            n_samples: 609
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rpoxyn/QgD8=
            n_samples: 609
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.9090909090909091
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rpoxyn/QgD8=
            n_samples: 609
            precision: 0.8333333333333334
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rpoxyn/QgD8=
            n_samples: 609
            precision: 1.0
            recall: 1.0
        8483400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hAgRIkSIoD8=
            n_samples: 1022
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hAgRIkSIoD8=
            n_samples: 1022
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hAgRIkSIoD8=
            n_samples: 1022
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9785213101694396
            f1: 0.8852459016393442
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hAgRIkSIoD8=
            n_samples: 1022
            precision: 0.9642857142857143
            recall: 0.8181818181818182
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hAgRIkSIoD8=
            n_samples: 1022
            precision: 1.0
            recall: 1.0
        8483500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rnfjC7tceT8=
            n_samples: 323
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rnfjC7tceT8=
            n_samples: 323
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rnfjC7tceT8=
            n_samples: 323
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rnfjC7tceT8=
            n_samples: 323
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rnfjC7tceT8=
            n_samples: 323
            precision: 1.0
            recall: 1.0
        8483600000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +CqfX85dYz8=
            n_samples: 423
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +CqfX85dYz8=
            n_samples: 423
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +CqfX85dYz8=
            n_samples: 423
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +CqfX85dYz8=
            n_samples: 423
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +CqfX85dYz8=
            n_samples: 423
            precision: 1.0
            recall: 1.0
        8483900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lOFQy546mT8=
            n_samples: 690
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lOFQy546mT8=
            n_samples: 690
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lOFQy546mT8=
            n_samples: 690
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lOFQy546mT8=
            n_samples: 690
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lOFQy546mT8=
            n_samples: 690
            precision: 1.0
            recall: 1.0
        8484100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJgj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJgj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJgj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJgj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kiRJkiRJgj8=
            n_samples: 224
            precision: 1.0
            recall: 1.0
        8484200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7seT8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7seT8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7seT8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7seT8=
            n_samples: 474
            precision: 0.75
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7seT8=
            n_samples: 474
            precision: 1.0
            recall: 1.0
        8484900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HuABHuABfj8=
            n_samples: 819
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HuABHuABfj8=
            n_samples: 819
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HuABHuABfj8=
            n_samples: 819
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9997949979499795
            f1: 0.9090909090909091
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HuABHuABfj8=
            n_samples: 819
            precision: 1.0
            recall: 0.8333333333333334
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HuABHuABfj8=
            n_samples: 819
            precision: 1.0
            recall: 1.0
        8487900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rX9cYuH2rT8=
            n_samples: 393
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rX9cYuH2rT8=
            n_samples: 393
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rX9cYuH2rT8=
            n_samples: 393
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.8846153846153846
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rX9cYuH2rT8=
            n_samples: 393
            precision: 0.7931034482758621
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rX9cYuH2rT8=
            n_samples: 393
            precision: 1.0
            recall: 1.0
    worst_group_gaps:
      CODE_SH_COMPLET_catboost:
        best_f1: 1.0
        gap: 1.0
        n_groups: 216
        worst_f1: 0.0
      CODE_SH_COMPLET_lightgbm:
        best_f1: 1.0
        gap: 1.0
        n_groups: 216
        worst_f1: 0.0
      CODE_SH_COMPLET_xgboost:
        best_f1: 1.0
        gap: 1.0
        n_groups: 216
        worst_f1: 0.0
  temporal_backtest: null
