chapter: chap85
created_date: '2025-09-15T13:22:14.713468'
data_hash: 2cc2c85d343fd2b96a9bbd5dc18c4234fafc0bad1929751a1ae04b0c6286bfe5
deployment_requirements:
  dependencies:
  - scikit-learn
  - pandas
  - numpy
  - joblib
  python_version: 3.8+
models_included:
- lightgbm
- xgboost
- catboost
- randomforest
- logisticregression
package_version: '1.0'
validation_results:
  business_thresholds:
    catboost:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - &id001 !!python/object/apply:numpy.dtype
          args:
          - f8
          - false
          - true
          state: !!python/tuple
          - 3
          - <
          - null
          - null
          - null
          - -1
          - -1
          - 0
        - !!binary |
          KVyPwvUovD8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          KVyPwvUovD8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA0D8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: &id003 []
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - &id002 !!python/name:numpy.ndarray ''
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/KZ8YIDxl7z8//L1JXITvP4lpC3q5nO8/y5LENzWg7z8Qjkke
                  sqPvP6uqqqqqqu8/AQxszC+u7z+cbT4OALfvP+n1BBeTwe8/AmgqJm/K7z8CaComb8rvPx1DHpIv
                  zO8/70LzRbrP7z9mhn9egdHvP7E47ilG0+8/nEeoPtPW7z8S+tXw0NbvP+qC1BIi3O8/pcBeWend
                  7z+lgOLVst/vP93ODgax3+8/3c4OBrHf7z9PQMtlrd/vP20mpO4I5e8/Soi15gXl7z9KiLXmBeXv
                  P0qIteYF5e8/33l05NDm7z/O4dAVnOjvPza/D8Wa6O8/27kodJno7z8rYgzZ/O3vP92zIyPI7+8/
                  3bMjI8jv7z8yfrvUk/HvPzJ+u9ST8e8/DaRKZJHx7z9YbgbzjvHvP9XbP0P19u8/1ds/Q/X27z/i
                  6GjA9PbvP4n4uzyS+u8/ifi7PJL67z9+13ufkfrvP37Xe5+R+u8/hxwYApH67z+Ku5BkkPrvPwbX
                  FimP+u8/Mjski4767z88G/WdjfrvP/vPqP+M+u8/+8+o/4z67z8uL3WwjPrvP4+POGGM+u8/lu/y
                  EYz67z8u/usji/rvP6SUD4WK+u8/pJQPhYr67z8eSkr3iPrvP2Qk71eI+u8/wZcDeYb67z+dpgU5
                  hfrvP6iDOqiD+u8/pBqpB4P67z/wCgfjVfzvP7SqJ9ZU/O8//c+KNFT87z+vTLaSU/zvPxGCmbpS
                  /O8/M9WzdVH87z/wkO0vUPzvP6qDvrJO/O8/YIAOa0387z+EFU5ZTPzvPxBL/A9L/O8/rDpiQ0j8
                  7z9Hbzf3RvzvP6ekRBlG/O8/JZ1DA0X87z9KJ3++If7vP1+S7Tgf/u8/xfwV81fM7z+M/DHyx8jv
                  Pzf82/Bvw+8/Gvxp8KfB7z/++/fv37/vP6j7oe6Huu8/qPuh7oe67z+o+6Huh7rvP2/7ve33tu8/
                  b/u97fe27z9v+73t97bvPzb72exns+8/Gvtn7J+x7z8a+2fsn7HvP/369evXr+8/4fqD6w+u7z/E
                  +hHrR6zvP1L6Seknpe8/NvrX6F+j7z82+tfoX6PvPxn6ZeiXoe8/Gfpl6Jeh7z/g+YHnB57vP6f5
                  neZ3mu8/bvm55eeW7z9u+bnl55bvP275ueXnlu8/bvm55eeW7z9u+bnl55bvP1L5R+Ufle8/NfnV
                  5FeT7z81+dXkV5PvPxn5Y+SPke8/Gflj5I+R7z/8+PHjx4/vP/z48ePHj+8/p/ib4m+K7z9R+EXh
                  F4XvP8P3C98vfO8/w/cL3y987z+m95neZ3rvP6b3md5neu8/pveZ3md67z9t97Xd13bvP233td3X
                  du8/NPfR3Edz7z/79u3bt2/vP4n2JdqXaO8/UPZB2Qdl7z/79evXr1/vP8L1B9cfXO8/wvUH1x9c
                  7z+l9ZXWV1rvP4n1I9aPWO8/bPWx1cdW7z8X9VvUb1HvP970d9PfTe8/3vR3099N7z9P9D3R90Tv
                  Pxb0WdBnQe8/a/Otzbc27z/58uXLly/vP2vyq8mvJu8/MvLHyB8j7z+H8RvGbxjvP/jw4cOHD+8/
                  o/CLwi8K7z9N8DXB1wTvP9vvbb+3/e4/MO/BvAfz7j+F7hW6V+juP77t97bf2+4/E+1LtC/R7j+E
                  7BGyR8juP9nrZa+Xve4/Z+qbqW+m7j+86e+mv5vuP0rpJ6WflO4/u+jtoreL7j8Q6EGgB4HuP4Hl
                  A5YPWO4/5Ox663WY7z/r8+MjbabvPw5+keIIsO8/e6W6wOWw7z8WirXDwrHvP13KflCXsu8/eM/s
                  qVq07z/8dUnlw7jvP6TLhp9EvO8/d645jrDA7z93rjmOsMDvPwlKs+nGv+8/ntWv9qXA7z/63Ral
                  iMHvPxidACmFwe8/MgCvgGTC7z8PYqkvfsHvP2zH9TGNwO8/NISPponA7z+VtX25bMHvPznZwxqG
                  wO8/OdnDGobA7z+ZQ6W2uL7vP/n7Y6KUv+8/YVFx1sa97z9hUXHWxr3vP2FRcdbGve8/AS13G6q+
                  7z+E6jNtjb/vP98kpnOmvu8/rbMrbb+97z9t7huuacDvP6uevB5mwO8/q568HmbA7z9zPPeOYsDv
                  P3M8945iwO8/CcG53qy97z+v/t659rrvP9ZDRcobue8/1kNFyhu57z96PXYNNLjvP6D7uZ/7ue8/
                  oPu5n/u57z+TOaf+K7jvP5M5p/4ruO8/dOWIKVy27z9JO1YgjLTvPxjLkHHrsO8/R3Psyxqv7z8+
                  K4txYazvP8mgS0mQqu8/yaBLSZCq7z+1WI+hp6nvP2SWvuy+qO8/bD/YKtan7z+ptJiWG6XvP8ox
                  7pxJo+8/yjHunEmj7z/n3kLHu57vPySq5hXpnO8/7lJPxm+X7z+u6fmJyJPvP6dL41U2j+8/9Zek
                  5GGN7z/xKa8nx4jvP/plHZcxhO8/Q1Jw1HCB7z+qVESar37vP/LsnZICe+8/KlfPd3117z9d7vR8
                  9m/vP6+UoE2Bae8/P/ZjP/Zj7z+tuotCVl/vP8O7673HWe8/auH1HbdN7z+MS6SWIkjvP4Xiai1p
                  RO8/NYJ9OMA/7z97F9O2CjvvP/113pWCJe8/AAAAAAAgbUAAAAAAACBsQAAAAAAAQGxAAAAAAACg
                  bEAAAAAAAABtQAAAAAAAYG5AAAAAAAAgbkAAAAAAAIBtQAAAAAAAAG5AAAAAAABgbUAAAAAAAGBt
                  QAAAAAAAgG5AAAAAAADgbkAAAAAAAMBuQAAAAAAAQG9AAAAAAACgb0AAAAAAACBwQAAAAAAAMHFA
                  AAAAAABwcUAAAAAAAGBxQAAAAAAAsHFAAAAAAACwcUAAAAAAAFByQAAAAAAAwHJAAAAAAABgc0AA
                  AAAAAGBzQAAAAAAAYHNAAAAAAABQc0AAAAAAAEBzQAAAAAAAkHNAAAAAAADgc0AAAAAAALBzQAAA
                  AAAA8HNAAAAAAADwc0AAAAAAADB0QAAAAAAAMHRAAAAAAAAgdUAAAAAAABB2QAAAAAAAcHdAAAAA
                  AABwd0AAAAAAAMB3QAAAAAAAoHdAAAAAAACgd0AAAAAAAEB4QAAAAAAAQHhAAAAAAADgeEAAAAAA
                  AIB5QAAAAAAAwHpAAAAAAABge0AAAAAAAFB8QAAAAAAA8HxAAAAAAADwfEAAAAAAAEB9QAAAAAAA
                  kH1AAAAAAADgfUAAAAAAANB+QAAAAAAAcH9AAAAAAABwf0AAAAAAAICAQAAAAAAA0IBAAAAAAADA
                  gUAAAAAAAGCCQAAAAAAAKINAAAAAAAB4g0AAAAAAAGCEQAAAAAAAKIVAAAAAAACghUAAAAAAABiG
                  QAAAAAAAuIZAAAAAAACoh0AAAAAAAJiIQAAAAAAAsIlAAAAAAACgikAAAAAAAGiLQAAAAAAAWIxA
                  AAAAAABgjkAAAAAAAFCPQAAAAAAA8I9AAAAAAABckEAAAAAAANCQQAAAAAAAnJJAVG6GA6XEcD8/
                  mD4RNa1qP5lGZaddV2U/OM1q4T6UZD/XU3AbINFjPxVhe4/iSmI/teeAycOHYT+FcB217T9fPz+Y
                  PhE1rVo/WzlaM5vdVj9bOVozm91WP/q/X218GlY/OM1q4T6UVD/XU3AbINFTP3badVUBDlM/teeA
                  ycOHUT+154DJw4dRPyT3Iu/OfE4/YgQuY5H2TD+gETnXU3BLP6AROddTcEs/oBE511NwSz+gETnX
                  U3BLP1s5WjOb3UY/WzlaM5vdRj9bOVozm91GP1s5WjOb3UY/mUZlp11XRT/XU3AbINFDP9dTcBsg
                  0UM/11NwGyDRQz8k9yLvznw+P6AROddTcDs/oBE511NwOz8dLE+/2GM4Px0sT7/YYzg/HSxPv9hj
                  OD8dLE+/2GM4PyT3Iu/OfC4/JPci7858Lj8k9yLvznwuPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koi
                  PxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/
                  FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8V
                  YXuP4koiPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8dLE+/2GMYPx0s
                  T7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxP
                  v9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8dLE+/
                  2GMIPx0sT7/YYwg/QJ0BdQbUeT/AuQHnBpx7P4DkAZIHSH4/wPIBywcsfz+AAAECBAiAP+AVgVcE
                  XoE/4BWBVwRegT/gFYFXBF6BPyAkgZAEQoI/ICSBkARCgj8gJIGQBEKCP2AygckEJoM/gDkB5gSY
                  gz+AOQHmBJiDP6BAgQIFCoQ/wEcBHwV8hD/gToE7Be6EP2Brga0FtoY/gHIBygUohz+AcgHKBSiH
                  P6B5geYFmoc/oHmB5gWahz/gh4EfBn6IPyCWgVgGYok/YKSBkQZGij9gpIGRBkaKP2CkgZEGRoo/
                  YKSBkQZGij9gpIGRBkaKP4CrAa4GuIo/oLKBygYqiz+gsoHKBiqLP8C5AecGnIs/wLkB5waciz/g
                  wIEDBw6MP+DAgQMHDow/QNYBWQdkjT+g64GuB7qOP6AHgR4EepA/oAeBHgR6kD8wC8EsBLOQPzAL
                  wSwEs5A/MAvBLASzkD9QEkFJBCWRP1ASQUkEJZE/cBnBZQSXkT+QIEGCBAmSP9AuQbsE7ZI/8DXB
                  1wRfkz+gQIECBQqUP8BHAR8FfJQ/wEcBHwV8lD9QS0EtBbWUP+BOgTsF7pQ/cFLBSQUnlT8gXYF0
                  BdKVP0BkAZEFRJY/QGQBkQVElj8QdkHYBWGXPzB9wfQF05c/kJJBSgYpmT/QoEGDBg2aP6CygcoG
                  Kps/wLkB5wacmz8gz4E8B/KcP/DgwYMHD54/oOuBrge6nj9Q9kHZB2WfP0gCIQmEJKA/+AzhM4TP
                  oD+oF6FehHqhPyAkgZAEQqI/0C5BuwTtoj+4N+HehHujP2hCoQmFJqQ/kFlBZgWZpT9AZAGRBUSm
                  P2Brga0FtqY/SHQh0YVEpz/4fuH7he+nP/CnwZ8Gf6o/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - &id004 !!python/object/apply:numpy.dtype
                  args:
                  - i8
                  - false
                  - true
                  state: !!python/tuple
                  - 3
                  - <
                  - null
                  - null
                  - null
                  - -1
                  - -1
                  - 0
                - false
                - !!binary |
                  2hEAAAAAAADYEQAAAAAAANURAAAAAAAA1BEAAAAAAADTEQAAAAAAANARAAAAAAAA0BEAAAAAAADQ
                  EQAAAAAAAM4RAAAAAAAAzhEAAAAAAADOEQAAAAAAAMwRAAAAAAAAyxEAAAAAAADLEQAAAAAAAMoR
                  AAAAAAAAyREAAAAAAADIEQAAAAAAAMQRAAAAAAAAwxEAAAAAAADDEQAAAAAAAMIRAAAAAAAAwhEA
                  AAAAAADAEQAAAAAAAL4RAAAAAAAAvBEAAAAAAAC8EQAAAAAAALwRAAAAAAAAvBEAAAAAAAC8EQAA
                  AAAAALsRAAAAAAAAuhEAAAAAAAC6EQAAAAAAALkRAAAAAAAAuREAAAAAAAC4EQAAAAAAALgRAAAA
                  AAAAtREAAAAAAACyEQAAAAAAAK0RAAAAAAAArREAAAAAAACsEQAAAAAAAKwRAAAAAAAArBEAAAAA
                  AACqEQAAAAAAAKoRAAAAAAAAqBEAAAAAAACmEQAAAAAAAKIRAAAAAAAAoBEAAAAAAACdEQAAAAAA
                  AJsRAAAAAAAAmxEAAAAAAACaEQAAAAAAAJkRAAAAAAAAmBEAAAAAAACVEQAAAAAAAJMRAAAAAAAA
                  kxEAAAAAAACOEQAAAAAAAIwRAAAAAAAAhhEAAAAAAACCEQAAAAAAAH0RAAAAAAAAexEAAAAAAAB1
                  EQAAAAAAAHARAAAAAAAAbREAAAAAAABqEQAAAAAAAGYRAAAAAAAAYBEAAAAAAABaEQAAAAAAAFMR
                  AAAAAAAATREAAAAAAABIEQAAAAAAAEIRAAAAAAAANREAAAAAAAAvEQAAAAAAACsRAAAAAAAAJhEA
                  AAAAAAAgEQAAAAAAAAkRAAAAAAAAWAAAAAAAAABGAAAAAAAAADgAAAAAAAAANgAAAAAAAAA0AAAA
                  AAAAADAAAAAAAAAALgAAAAAAAAApAAAAAAAAACMAAAAAAAAAHgAAAAAAAAAeAAAAAAAAAB0AAAAA
                  AAAAGwAAAAAAAAAaAAAAAAAAABkAAAAAAAAAFwAAAAAAAAAXAAAAAAAAABQAAAAAAAAAEwAAAAAA
                  AAASAAAAAAAAABIAAAAAAAAAEgAAAAAAAAASAAAAAAAAAA8AAAAAAAAADwAAAAAAAAAPAAAAAAAA
                  AA8AAAAAAAAADgAAAAAAAAANAAAAAAAAAA0AAAAAAAAADQAAAAAAAAAKAAAAAAAAAAkAAAAAAAAA
                  CQAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAUAAAAAAAAABQAAAAAAAAAF
                  AAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMA
                  AAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAA
                  AAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAA
                  AAAAAAMAAAAAAAAAAwAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAA
                  AAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAA
                  AAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAABAAAAAAAAAAEAAAAAAAAAHQAAAAAAAAAfAAAAAAAA
                  ACIAAAAAAAAAIwAAAAAAAAAkAAAAAAAAACcAAAAAAAAAJwAAAAAAAAAnAAAAAAAAACkAAAAAAAAA
                  KQAAAAAAAAApAAAAAAAAACsAAAAAAAAALAAAAAAAAAAsAAAAAAAAAC0AAAAAAAAALgAAAAAAAAAv
                  AAAAAAAAADMAAAAAAAAANAAAAAAAAAA0AAAAAAAAADUAAAAAAAAANQAAAAAAAAA3AAAAAAAAADkA
                  AAAAAAAAOwAAAAAAAAA7AAAAAAAAADsAAAAAAAAAOwAAAAAAAAA7AAAAAAAAADwAAAAAAAAAPQAA
                  AAAAAAA9AAAAAAAAAD4AAAAAAAAAPgAAAAAAAAA/AAAAAAAAAD8AAAAAAAAAQgAAAAAAAABFAAAA
                  AAAAAEoAAAAAAAAASgAAAAAAAABLAAAAAAAAAEsAAAAAAAAASwAAAAAAAABNAAAAAAAAAE0AAAAA
                  AAAATwAAAAAAAABRAAAAAAAAAFUAAAAAAAAAVwAAAAAAAABaAAAAAAAAAFwAAAAAAAAAXAAAAAAA
                  AABdAAAAAAAAAF4AAAAAAAAAXwAAAAAAAABiAAAAAAAAAGQAAAAAAAAAZAAAAAAAAABpAAAAAAAA
                  AGsAAAAAAAAAcQAAAAAAAAB1AAAAAAAAAHoAAAAAAAAAfAAAAAAAAACCAAAAAAAAAIcAAAAAAAAA
                  igAAAAAAAACNAAAAAAAAAJEAAAAAAAAAlwAAAAAAAACdAAAAAAAAAKQAAAAAAAAAqgAAAAAAAACv
                  AAAAAAAAALUAAAAAAAAAwgAAAAAAAADIAAAAAAAAAMwAAAAAAAAA0QAAAAAAAADXAAAAAAAAAO4A
                  AAAAAAAAoFMAAAAAAACyUwAAAAAAAMBTAAAAAAAAwlMAAAAAAADEUwAAAAAAAMhTAAAAAAAAylMA
                  AAAAAADPUwAAAAAAANVTAAAAAAAA2lMAAAAAAADaUwAAAAAAANtTAAAAAAAA3VMAAAAAAADeUwAA
                  AAAAAN9TAAAAAAAA4VMAAAAAAADhUwAAAAAAAORTAAAAAAAA5VMAAAAAAADmUwAAAAAAAOZTAAAA
                  AAAA5lMAAAAAAADmUwAAAAAAAOlTAAAAAAAA6VMAAAAAAADpUwAAAAAAAOlTAAAAAAAA6lMAAAAA
                  AADrUwAAAAAAAOtTAAAAAAAA61MAAAAAAADuUwAAAAAAAO9TAAAAAAAA71MAAAAAAADwUwAAAAAA
                  APBTAAAAAAAA8FMAAAAAAADwUwAAAAAAAPNTAAAAAAAA81MAAAAAAADzUwAAAAAAAPVTAAAAAAAA
                  9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1
                  UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVT
                  AAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMA
                  AAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAA
                  AAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAA
                  AAAA9lMAAAAAAAD3UwAAAAAAAPdTAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - &id005 !!python/name:pandas.core.indexes.base.Index ''
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - &id006 !!python/object/apply:numpy.dtype
                    args:
                    - O8
                    - false
                    - true
                    state: !!python/tuple
                    - 3
                    - '|'
                    - null
                    - null
                    - null
                    - -1
                    - -1
                    - 63
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - &id007 !!python/name:pandas.core.indexes.range.RangeIndex ''
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          EtymtnX+7z8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          SOPI20/p7z8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          l/ek+S/57z8=
    lightgbm:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          7FG4HoXr0T8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          7FG4HoXr0T8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          SOF6FK5H4T8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/TRSMxiYl7T+pFuEUYj7tP5hhvFFKX+0/p5IOtGd37T8mXlUH
                  GqjtP40HkxiI4u0/faWkUIAf7j8LcGRQXELuP5951TksfO4/4yFpZhWb7j8Uhqt6KrXuP9NYNnT0
                  yO4/mZVVn8LS7j+h7rGuCuruP4nJ90sB9O4/tZVyyrMJ7z+GvdKfsx3vP/NtMMwQMu8/dXhjgx1D
                  7z+rnDUJ1E7vPz8QzethV+8/8uvePf5f7z+8mpbpvm3vP4/GL81HhO8/AORUOaqO7z/NrvpP4pPv
                  P4F+T9fimu8/2GlHfZij7z+h6s+TTKzvP7SDLNtWs+8/btu2bdu27z9AIdwumLjvP9b0U9AevO8/
                  PUL1mbbG7z95fNiuesjvP/Oj/Cg/yu8/Tc7OHczN7z9s5mmIINPvP79VZDLn1O8/cfos8XfY7z+R
                  7ErTQdrvP2NgR94H3O8/JDA9OJvf7z8kMD04m9/vP8SiJ8D55O8/xKInwPnk7z9IAMWo8+TvP/Kh
                  7gK95u8/+vqgM1Xq7z9XG0PTHezvP1cbQ9Md7O8/BEpr8rPv7z8ESmvys+/vPwQY/zmA8e8/SQOx
                  wk7z7z9A+RaL7PbvPxRqSyy7+O8/B8NvR7j47z8puA2ftvjvP2Qk71eI+u8/yUXsyIb67z/o4BbZ
                  hfrvP17cz5iE+u8/WD2nSIT67z+ogzqog/rvP+dRpncr/u8/yUA0Jyv+7z94hYPxKv7vPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/cf/F/Rf37z9x/8X9F/fv
                  P3H/xf0X9+8/cf/F/Rf37z9x/8X9F/fvPzj/4fyH8+8///79+/fv7z/j/ov7L+7vP8b+Gftn7O8/
                  xv4Z+2fs7z+N/jX61+jvP43+NfrX6O8/VP5R+Ufl7z9U/lH5R+XvPzj+3/h/4+8///379+/f7z9w
                  /cH1B9fvP3D9wfUH1+8/cP3B9QfX7z+p/KPyj8rvP4z8MfLHyO8/jPwx8sfI7z9T/E3xN8XvPzf8
                  2/Bvw+8/4fuF7xe+7z/F+xPvT7zvP8X7E+9PvO8/b/u97fe27z/h+oPrD67vP8T6EetHrO8/qPqf
                  6n+q7z9v+rvp76bvP1L6Seknpe8/4PmB5wee7z/E+Q/nP5zvP6f5neZ3mu8/i/kr5q+Y7z9S+Ufl
                  H5XvPzX51eRXk+8/Gflj5I+R7z8Z+WPkj5HvP+D4f+P/je8/w/gN4zeM7z/D+A3jN4zvP4r4KeKn
                  iO8/ivgp4qeI7z8Y+GHgh4HvP9/3fd/3fe8/w/cL3y987z9R90PdD3XvP1H3Q90Pde8/pvaX2l9q
                  7z+m9pfaX2rvP1D2QdkHZe8/NPbP2D9j7z/e9XnX513vP2z1sdXHVu8/pfST0k9K7z8z9MvQL0Pv
                  Pxb0WdBnQe8/iPMfzn847z8y88nMJzPvP8DyAcsHLO8/pPKPyj8q7z9r8qvJrybvPzLyx8gfI+8/
                  3PFxx8cd7z+j8Y3GNxrvP2rxqcWnFu8/MfHFxBcT7z/48OHDhw/vP4bwGcJnCO8/+O/fv3//7j/b
                  722/t/3uP9vua7uv7e4/E+5NuDfh7j/a7Wm3p93uP6HthbYX2u4/aO2htYfW7j+86vGqx6vuP4Pq
                  Dao3qO4/YKPi9XZ97j/z61zhP4vuP41UR0Mpne4/7MqkTT2q7j87ddIEh8TuP5DM2Ugx4u4/xIzi
                  FfIA7z9v2euNhhLvP6SXEb8LMO8/UH790jBA7z+lpUZSDUzvP4LuvztRVu8/ekI3gqxZ7z9btmzZ
                  smXvP8p3p0T4ae8/XF5iHWVz7z/pip8BTXnvP/P4/4S0g+8/paPGX2WM7z/AK5CsOIzvP5YlUBSv
                  j+8/2r6xKgyU7z/VL4rkPpnvPx8ztlq8o+8/UUq6T0+m7z8qOTE3DKjvP/dLUZGQq+8/LhM+P0Wt
                  7z99NWI5Lq3vP8YUf/7Or+8/cPPEd6yw7z+njZGWwa/vP/nmwTOfsO8//OnyMlKy7z963QDdTbLv
                  P0bikoZJsu8/KjU9wyez7z++PVrmAbTvP1uYFKf9s+8/9n1xRty07z9rAD0yv7XvPwclqt7TtO8/
                  wtWZu7K17z/C1Zm7srXvP0zPnaGNtu8/TM+doY227z+j7oHQ77LvPyD7md0Dsu8/qkyv2+Ky7z/7
                  FdAGJ7DvP/sV0AYnsO8/alLNmX2s7z9qUs2ZfazvPzunZUWoqu8/EE50eqOq7z8JDaJVsanvP8f9
                  1F3ypu8/1sbTopOg7z/rG8Sp7pzvPySq5hXpnO8/kk93dFmY7z9xVn5CnJXvP+UiuZz0ke8/JlRP
                  kgqR7z+nS+NVNo/vP+xT25spj+8/HE7FjWqM7z+7W5GXlYrvP1fEAlqkie8/ffh5+c6H7z+anOhj
                  +YXvP9A8iJlNgu8/2kyfsbV97z8TclZbynzvP+6cPvuBdO8/9WF24Q1u7z+gj7lhNWzvP9B8bKxc
                  au8/Uw2GwYNo7z/Bdf9cSFLvP8GcDrZsUO8/AAAAAACwfUAAAAAAAKB8QAAAAAAAQHtAAAAAAABA
                  ekAAAAAAAEB4QAAAAAAAgHZAAAAAAACwdEAAAAAAAKBzQAAAAAAAsHFAAAAAAACAcEAAAAAAACBw
                  QAAAAAAAwG5AAAAAAABAb0AAAAAAAIBtQAAAAAAAYG1AAAAAAAAAbUAAAAAAAKBuQAAAAAAAIG1A
                  AAAAAADga0AAAAAAAGBvQAAAAAAAYG9AAAAAAADAbkAAAAAAAABvQAAAAAAAAG5AAAAAAAAgb0AA
                  AAAAAGBvQAAAAAAA4G5AAAAAAAAQcEAAAAAAAFBxQAAAAAAAYHFAAAAAAACQcUAAAAAAACByQAAA
                  AAAAUHJAAAAAAAAwc0AAAAAAAHBzQAAAAAAAsHNAAAAAAADgc0AAAAAAAFB0QAAAAAAAkHRAAAAA
                  AADAdEAAAAAAALB0QAAAAAAAQHVAAAAAAABwdUAAAAAAAHB1QAAAAAAA4HVAAAAAAADgdUAAAAAA
                  ACB3QAAAAAAAsHdAAAAAAADgd0AAAAAAABB5QAAAAAAAEHlAAAAAAADQekAAAAAAANB6QAAAAAAA
                  sHtAAAAAAADwe0AAAAAAAMB8QAAAAAAA8H1AAAAAAAAQgEAAAAAAALCAQAAAAAAA0IBAAAAAAACY
                  gUAAAAAAABCCQAAAAAAAsIJAAAAAAADYgkAAAAAAACiDQAAAAAAAaINAAAAAAADgg0AAAAAAADCE
                  QAAAAAAAeIRAAAAAAADIhEAAAAAAABiFQAAAAAAAuIVAAAAAAACAhkAAAAAAAKiGQAAAAAAAEIhA
                  AAAAAAAoiUAAAAAAAHiJQAAAAAAAyIlAAAAAAAAYikAAAAAAANiNQAAAAAAAKI5AxZUkgMFvlT/O
                  dMrNcKCUP+kNkn0mlJM/iJSXtwfRkj/GoaIrykqRPwCD3yrC9o4/SHO6JYw/iz99pUmF9yaJP0mD
                  YgrtuIU/A6Mv9IPpgz9CsDpoRmOCPzD6Qj+YPoE/KB/HKkGsgD98laGglq1+P2vfqXfoiH0/8NQ7
                  dMQOez/NaEwiaMV4P6r8XNALfHY/OM1q4T6UdD/PePQGyT5zPxVhe4/iSnI/XEkCGPxWcT81rRoY
                  faFvPz+YPhE1rWo/HSxPv9hjaD8LdleWKj9nP0mDYgrtuGU/11NwGyDRYz9lJH4sU+lhP6MxiaAV
                  Y2A/hXAdte0/Xz8k9yLvznxeP2IELmOR9lw/HSxPv9hjWD+8slT5uaBXP1s5WjOb3VY/mUZlp11X
                  VT922nVVAQ5TPxVhe4/iSlI/VG6GA6XEUD/z9Is9hgFQPyT3Iu/OfE4/oBE511NwSz+gETnXU3BL
                  P1s5WjOb3UY/WzlaM5vdRj9bOVozm91GP5lGZaddV0U/FWF7j+JKQj9UboYDpcRAP1RuhgOlxEA/
                  oBE511NwOz+gETnXU3A7Px0sT7/YYzg/mUZlp11XNT8k9yLvznwuPx0sT7/YYyg/HSxPv9hjKD8d
                  LE+/2GMoPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPx0s
                  T7/YYwg/HSxPv9hjCD8dLE+/2GMIPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAB0BdATQUT8AHQF0BNBRPwAdAXQE0FE/AB0BdATQUT8AHQF0BNBRPwCPATwG
                  8Fg/gAABAgQIYD8AHQF0BNBhP4A5AeYEmGM/gDkB5gSYYz+AcgHKBShnP4ByAcoFKGc/gKsBrga4
                  aj+AqwGuBrhqPwDIASAHgGw/gAABAgQIcD/ARwEfBXx0P8BHAR8FfHQ/wEcBHwV8dD+AqwGuBrh6
                  P8C5AecGnHs/wLkB5wacez9A1gFZB2R9P4DkAZIHSH4/oAeBHgR6gD/ADgE7BOyAP8AOATsE7IA/
                  ICSBkARCgj/ARwEfBXyEP+BOgTsF7oQ/AFYBWAVghT9AZAGRBUSGP2Brga0FtoY/4IeBHwZ+iD8A
                  jwE8BvCIPyCWgVgGYok/QJ0BdQbUiT+AqwGuBriKP6CygcoGKos/wLkB5waciz/AuQHnBpyLPwDI
                  ASAHgIw/IM+BPAfyjD8gz4E8B/KMP2DdgXUH1o0/YN2BdQfWjT/g+YHnB56PPxAEQRAEQZA/oAeB
                  HgR6kD/gFYFXBF6RP+AVgVcEXpE/QCsBrQS0kj9AKwGtBLSSP/A1wdcEX5M/gDkB5gSYkz8wRMEQ
                  BUOUP3BSwUkFJ5U/YGuBrQW2lj+geYHmBZqXPzB9wfQF05c/AI8BPAbwmD+wmcFmBpuZP/CnwZ8G
                  f5o/gKsBrga4mj+gsoHKBiqbP8C5AecGnJs/cMTBEQdHnD+Qy0EuB7mcP7DSwUoHK50/0NlBZwed
                  nT/w4MGDBw+ePzDvwbwH854/gAABAgQIoD9IAiEJhCSgP1ASQUkEJaE/yB4he4TsoT9YImGJhCWi
                  P+gloZeEXqI/eCnhpYSXoj84VOFQhUOlP8hXIV+FfKU/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  8hEAAAAAAADyEQAAAAAAAPIRAAAAAAAA8hEAAAAAAADyEQAAAAAAAPARAAAAAAAA7hEAAAAAAADt
                  EQAAAAAAAOwRAAAAAAAA7BEAAAAAAADqEQAAAAAAAOoRAAAAAAAA6BEAAAAAAADoEQAAAAAAAOcR
                  AAAAAAAA5REAAAAAAADgEQAAAAAAAOARAAAAAAAA4BEAAAAAAADZEQAAAAAAANgRAAAAAAAA2BEA
                  AAAAAADWEQAAAAAAANURAAAAAAAA0hEAAAAAAADREQAAAAAAANERAAAAAAAAzhEAAAAAAADJEQAA
                  AAAAAMgRAAAAAAAAxxEAAAAAAADFEQAAAAAAAMQRAAAAAAAAwBEAAAAAAAC/EQAAAAAAAL4RAAAA
                  AAAAvREAAAAAAAC7EQAAAAAAALoRAAAAAAAAuREAAAAAAAC5EQAAAAAAALcRAAAAAAAAthEAAAAA
                  AAC2EQAAAAAAALQRAAAAAAAAtBEAAAAAAACwEQAAAAAAAK4RAAAAAAAArREAAAAAAACpEQAAAAAA
                  AKkRAAAAAAAAoxEAAAAAAACjEQAAAAAAAKARAAAAAAAAnxEAAAAAAACcEQAAAAAAAJgRAAAAAAAA
                  kREAAAAAAACNEQAAAAAAAIwRAAAAAAAAhxEAAAAAAACEEQAAAAAAAIARAAAAAAAAfxEAAAAAAAB9
                  EQAAAAAAAHsRAAAAAAAAeBEAAAAAAAB2EQAAAAAAAHQRAAAAAAAAchEAAAAAAABwEQAAAAAAAGwR
                  AAAAAAAAZxEAAAAAAABmEQAAAAAAAF0RAAAAAAAAVhEAAAAAAABUEQAAAAAAAFIRAAAAAAAAUBEA
                  AAAAAAA4EQAAAAAAADYRAAAAAAAAwgEAAAAAAACxAQAAAAAAAJsBAAAAAAAAiwEAAAAAAABrAQAA
                  AAAAAEUBAAAAAAAAHgEAAAAAAAAIAQAAAAAAAOQAAAAAAAAA0QAAAAAAAADBAAAAAAAAALUAAAAA
                  AAAArwAAAAAAAAChAAAAAAAAAJsAAAAAAAAAjgAAAAAAAACCAAAAAAAAAHYAAAAAAAAAbAAAAAAA
                  AABlAAAAAAAAAGAAAAAAAAAAWwAAAAAAAABTAAAAAAAAAEYAAAAAAAAAQAAAAAAAAAA9AAAAAAAA
                  ADkAAAAAAAAANAAAAAAAAAAvAAAAAAAAACsAAAAAAAAAKQAAAAAAAAAoAAAAAAAAACYAAAAAAAAA
                  IAAAAAAAAAAfAAAAAAAAAB4AAAAAAAAAHAAAAAAAAAAZAAAAAAAAABgAAAAAAAAAFgAAAAAAAAAV
                  AAAAAAAAABQAAAAAAAAAEgAAAAAAAAASAAAAAAAAAA8AAAAAAAAADwAAAAAAAAAPAAAAAAAAAA4A
                  AAAAAAAADAAAAAAAAAALAAAAAAAAAAsAAAAAAAAACQAAAAAAAAAJAAAAAAAAAAgAAAAAAAAABwAA
                  AAAAAAAFAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAA
                  AAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAFAAAAAAAA
                  AAUAAAAAAAAABQAAAAAAAAAFAAAAAAAAAAcAAAAAAAAACQAAAAAAAAAKAAAAAAAAAAsAAAAAAAAA
                  CwAAAAAAAAANAAAAAAAAAA0AAAAAAAAADwAAAAAAAAAPAAAAAAAAABAAAAAAAAAAEgAAAAAAAAAX
                  AAAAAAAAABcAAAAAAAAAFwAAAAAAAAAeAAAAAAAAAB8AAAAAAAAAHwAAAAAAAAAhAAAAAAAAACIA
                  AAAAAAAAJQAAAAAAAAAmAAAAAAAAACYAAAAAAAAAKQAAAAAAAAAuAAAAAAAAAC8AAAAAAAAAMAAA
                  AAAAAAAyAAAAAAAAADMAAAAAAAAANwAAAAAAAAA4AAAAAAAAADkAAAAAAAAAOgAAAAAAAAA8AAAA
                  AAAAAD0AAAAAAAAAPgAAAAAAAAA+AAAAAAAAAEAAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEMAAAAA
                  AAAAQwAAAAAAAABHAAAAAAAAAEkAAAAAAAAASgAAAAAAAABOAAAAAAAAAE4AAAAAAAAAVAAAAAAA
                  AABUAAAAAAAAAFcAAAAAAAAAWAAAAAAAAABbAAAAAAAAAF8AAAAAAAAAZgAAAAAAAABqAAAAAAAA
                  AGsAAAAAAAAAcAAAAAAAAABzAAAAAAAAAHcAAAAAAAAAeAAAAAAAAAB6AAAAAAAAAHwAAAAAAAAA
                  fwAAAAAAAACBAAAAAAAAAIMAAAAAAAAAhQAAAAAAAACHAAAAAAAAAIsAAAAAAAAAkAAAAAAAAACR
                  AAAAAAAAAJoAAAAAAAAAoQAAAAAAAACjAAAAAAAAAKUAAAAAAAAApwAAAAAAAAC/AAAAAAAAAMEA
                  AAAAAAAANlIAAAAAAABHUgAAAAAAAF1SAAAAAAAAbVIAAAAAAACNUgAAAAAAALNSAAAAAAAA2lIA
                  AAAAAADwUgAAAAAAABRTAAAAAAAAJ1MAAAAAAAA3UwAAAAAAAENTAAAAAAAASVMAAAAAAABXUwAA
                  AAAAAF1TAAAAAAAAalMAAAAAAAB2UwAAAAAAAIJTAAAAAAAAjFMAAAAAAACTUwAAAAAAAJhTAAAA
                  AAAAnVMAAAAAAAClUwAAAAAAALJTAAAAAAAAuFMAAAAAAAC7UwAAAAAAAL9TAAAAAAAAxFMAAAAA
                  AADJUwAAAAAAAM1TAAAAAAAAz1MAAAAAAADQUwAAAAAAANJTAAAAAAAA2FMAAAAAAADZUwAAAAAA
                  ANpTAAAAAAAA3FMAAAAAAADfUwAAAAAAAOBTAAAAAAAA4lMAAAAAAADjUwAAAAAAAORTAAAAAAAA
                  5lMAAAAAAADmUwAAAAAAAOlTAAAAAAAA6VMAAAAAAADpUwAAAAAAAOpTAAAAAAAA7FMAAAAAAADt
                  UwAAAAAAAO1TAAAAAAAA71MAAAAAAADvUwAAAAAAAPBTAAAAAAAA8VMAAAAAAADzUwAAAAAAAPRT
                  AAAAAAAA9FMAAAAAAAD0UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMA
                  AAAAAAD1UwAAAAAAAPdTAAAAAAAA91MAAAAAAAD3UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAA
                  AAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAA
                  AAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          P9jkONT+7z8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          j8iIZ9D37z8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5jmAqOz97z8=
    logisticregression:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          FK5H4XoU3j8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          FK5H4XoU3j8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          rkfhehSu5z8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/4B/jIMoZ6T/C+Ricj8HpP1sKqtviE+o/375SBGFz6j+fId+0
                  L7bqP008KxoJ+Oo/gTqKt8Aw6z+ymljbOF3rP7Dmkh3Vhes/CmXCQvOl6z+qbY5h9r7rP2jkNv93
                  5us/oSCua4sW7D9ySCGivUnsP8mbj9ZubOw/bfM5gGuH7D/BxjH6HJ7sPze/f1LxsOw/ZmMN2Z+6
                  7D8xDMMwDMPsP2j174h11Ow/edkwEt7r7D8SWZS+jffsPxHnzI3mB+0/1+QP+18W7T9HYu7e3CDt
                  P1CFNPM7K+0/IoqWvKg47T9SilabMT3tP1/bTkb/Re0/xFn5CXFW7T/5f+gyX23tP4hb+sIDfu0/
                  nK5okDuK7T9yPc0ebJDtP7xig92UnO0/GFH/Suul7T8WmG23mKrtP1pp9IgKsu0/QdWt1nS+7T9B
                  F5YOAcHtP9sx6mtVyu0/JuiuUkzQ7T/Ao0UgYNvtP+lsAKT25+0/J3l1zKzy7T9lsV+Apv3tP0ZI
                  2Kr0A+4/A/L8EIsV7j+UhgpE0x7uP2JJv8UdLe4/5MhaosA57j/QG5qguETuP/7yT4+kTO4/+d1e
                  XIJW7j+sROr5RVvuP6cvajCzYe4/SZ0aJJ9n7j/5GwBpKXPuP7D+gy49e+4//lzgVASF7j+Z+49D
                  hIzuPzSkWYu/nO4/dtYQWLOr7j9CqeY37r/uP6+u0oPSzO4/NPFSZXrd7j9yjglPLenuP9HzB6mx
                  9u4/Ux2JsWAC7z90wG6PtQzvP3zNhaQpFe8/seTd1HIm7z8tAndDaSvvP8D47tMuOe8/Z6JTZ6JT
                  7z9JWG8mMWfvP3fwtYtYbO8/1U5LVQ6C7z8N8HDcepDvP9CRN1FPm+8/Gvtn7J+x7z/9+vXr16/v
                  P+H6g+sPru8/xPoR60es7z+o+p/qf6rvP6j6n+p/qu8/qPqf6n+q7z+o+p/qf6rvP2/6u+nvpu8/
                  b/q76e+m7z8Z+mXol6HvPxn6ZeiXoe8//fnz58+f7z+n+Z3md5rvPzX51eRXk+8/Gflj5I+R7z/g
                  +H/j/43vP+D4f+P/je8/ivgp4qeI7z9R+EXhF4XvPzX40+BPg+8/GPhh4IeB7z/89+/fv3/vP/z3
                  79+/f+8/pveZ3md67z+m95neZ3rvP4r3J96feO8/bfe13dd27z9t97Xd13bvPzT30dxHc+8/+/bt
                  27dv7z/79u3bt2/vP8L2CdsnbO8/pvaX2l9q7z+m9pfaX2rvP232s9nPZu8/bfaz2c9m7z9t9rPZ
                  z2bvPxf2Xdh3Ye8/+/Xr169f7z9s9bHVx1bvP1D1P9X/VO8/+vTp06dP7z/69OnTp0/vP970d9Pf
                  Te8/bPSv0b9G7z8z9MvQL0PvPxb0WdBnQe8/+vPnz58/7z+I8x/OfzjvPzLzycwnM+8/wPIBywcs
                  7z8y8sfIHyPvP9zxccfHHe8/3PFxx8cd7z+j8Y3GNxrvP2rxqcWnFu8/hvAZwmcI7z9q8KfBnwbv
                  PxTwUcBHAe8/v+/7vu/77j+F7hW6V+juP6HthbYX2u4/L+29tPfS7j/a7Gezn83uP4TrD64/uO4/
                  g+oNqjeo7j+86e+mv5vuP/To0aNHj+4/1+ddn3d97j9l55WdV3buP57md5rfae4/neV1ltdZ7j+5
                  5OWSl0vuP2PjjY03Nu4/uOHfhn8b7j9F4BWBVwTuPyjfoXyH8u0/tt3Xdl/b7T/u27dv377tPyba
                  l2hfou0/oNwUSL0D7D+xMgHQUmrsP/u5nGprm+w/m6gAb8XT7D9MrSQghvrsP5ja+qYaIe0/wavD
                  TRBC7T9k0HSRu1vtP0WZC2qAce0/Qy1GTteD7T/82F2UvI/tPwSzau4Zpu0/0LfUhF/A7T/O8LEQ
                  ndrtP/Xr2Wqx6u0/SuBhA9D47T9RuZzQtAPuP9gVCMcMDu4/7qufMe4Q7j/zlb5I6xPuP/mELKed
                  HO4/aTt6dIwo7j+uR+F6FC7uP2qYh2XxNu4/xeMTj0887j+WnAMZ+UHuPzd2qZ+9Ru4//q+G8SNN
                  7j9QPvnkk0/uPy/Ih1+mUu4/oa9Iic1Z7j+SpdM1DmbuP3Gn4ZFBbe4/L+drsOpy7j92fp8jNHbu
                  P8cL05/5eu4/X+YlPet/7j/zN9GlZILuP2lTsnHNg+4/ZbW8MYKJ7j+g7/PeoIbuP6ODVXSwiu4/
                  UNnM60eL7j+fVkoYF5HuP5wP2tbUlu4/EiOs0wWZ7j/GDoIcCp3uP4qztt93n+4/USeXF8Gn7j/0
                  n3bSIqnuP5LSJWnzre4/zmtvMQax7j9+fvdyWbLuP4aYaxXSs+4/9X4F/uG47j9wJrZBlrnuPwRP
                  CCIju+4/yTfzDzG37j8em065M7zuPwgyR1ywve4/HAA7EgfA7j9/D9RkKbruP1+WKARNu+4/J0/5
                  /Ei/7j8we7tvxcbuPycdRaiFwu4/G+XM+cHC7j8tgy7BRcLuPx+Ybbylwu4/HF6HiVy/7j81NPLM
                  zsDuP7B3ZhKWvu4/ly8j3NC+7j9KJdPo6LnuP0AgbGKRte4/TnJ/Ani07j+faAxEw7HuP1ksognl
                  qu4/PiiALAep7j+FhmIqxaDuP2hUmry1lu4/AAAAAAAAl0AAAAAAAMSUQAAAAAAAwJNAAAAAAACY
                  kkAAAAAAANSRQAAAAAAABJFAAAAAAABUkEAAAAAAAJiPQAAAAAAA8I5AAAAAAAAwjkAAAAAAABCO
                  QAAAAAAAKI1AAAAAAAA4jEAAAAAAAIiLQAAAAAAAYItAAAAAAADwikAAAAAAAMCKQAAAAAAAWIpA
                  AAAAAACYikAAAAAAALiKQAAAAAAAgIpAAAAAAAAoikAAAAAAABCKQAAAAAAAuIlAAAAAAADgiUAA
                  AAAAAKiJQAAAAAAAmIlAAAAAAAB4iUAAAAAAAGCJQAAAAAAAgIlAAAAAAAB4iUAAAAAAAACJQAAA
                  AAAA+IhAAAAAAADgiEAAAAAAAMCIQAAAAAAA0IhAAAAAAACgiEAAAAAAAIiIQAAAAAAA2IhAAAAA
                  AADAiEAAAAAAAHiJQAAAAAAAcIlAAAAAAADIiUAAAAAAAJCJQAAAAAAAeIlAAAAAAADgiUAAAAAA
                  APiJQAAAAAAAAIpAAAAAAADQiUAAAAAAAECKQAAAAAAAcIpAAAAAAADQikAAAAAAAGCLQAAAAAAA
                  sItAAAAAAACAi0AAAAAAALiLQAAAAAAA6ItAAAAAAAAIjUAAAAAAAPiMQAAAAAAASI1AAAAAAACQ
                  jUAAAAAAACCPQAAAAAAACJBAAAAAAAA0kEAAAAAAAECQQAAAAAAAEJFAAAAAAACckUAAAAAAAAyS
                  QAAAAAAAeJJAAAAAAAAkk0AAAAAAAFyTQAAAAAAA1JNAAAAAAABglEAAAAAAAPSUQAAAAAAAxJVA
                  AAAAAAC0lkAAAAAAAIyXQAAAAAAASJhAAAAAAAAcmUAAAAAAADyaQAAAAAAAZJtA78znARTSrT+P
                  W0GupUuqPwtyLV3SoKg/ZBZrZB6/pj+QaVR22nWlP1NknXTIOKQ/bf1kJH4soz933ApyLV2iP+E2
                  QKInoKE/2FvEjdANoT/GozdI9pmgP41LmclE0p8/CWKFeHEnnj9ZKbJOOmScP7LLWjlaM5s/j1tB
                  rqVLmj8u4kbohoiZP49fa+f96Zg/dcrNcKCUmD/w3I/mdEuYP+gBFNIduZc/h4gZDP/1lj/WSxyp
                  b5SWP2QYAIFKDpY/h4xDRVeUlT9t96XO+T6VP1NiCFic6ZQ/DH6rCNt7lD9Kh4xDRVeUP8aZTrkZ
                  DpQ/U2YykfSHkz+IlJe3B9GSPxVhe4/iSpI/ZSR+LFPpkT8Nhv96i7iRP1xJAhj8VpE/2FvEjdAN
                  kT8WZaXIOumQPygfxypBrJA/d+LJx7FKkD9LkwrvTTKQP41LmclE0o8/3Q6cZrVwjz+o5GB5+sWO
                  P0drZrPbAo4/EkErxiBYjT/eFvDYZa2MPy3a8nXWS4w/SHO6JYw/iz8/mD4RNa2KP7LPhHKy0Yk/
                  UVaKrJMOiT8dLE+/2GOIP0CgkoPl6Yc/N8UWb45Xhz+z19jkYg6HPwOb24HTrIY/Ul7eHkRLhj8d
                  NKMxiaCFP0Go5vWVJoU/OM1q4T6UhD9cQa6lSxqEP6IpNS5lJoM/FWF7j+JKgj8Eq4NmNCaBP6Mx
                  iaAVY4A/1DMgUl7efj9r36l36Ih9P6nstOuqAnw/P5g+ETWtej8u4kbohoh5P3XKzXCglHg/A5vb
                  gdOsdj/6v19tfBp2PzjNauE+lHQ/DYb/eou4cT+FcB217T9vP3O6JYw/G24/LuJG6IaIaT+q/FzQ
                  C3xmP4iQbX6vMmQ/gDkB5gSYgz+gQIECBQqEP8BHAR8FfIQ/4E6BOwXuhD8AVgFYBWCFPwBWAVgF
                  YIU/AFYBWAVghT8AVgFYBWCFP0BkAZEFRIY/QGQBkQVEhj+geYHmBZqHP6B5geYFmoc/wIABAwYM
                  iD8gloFYBmKJP6CygcoGKos/wLkB5waciz8AyAEgB4CMPwDIASAHgIw/YN2BdQfWjT+g64GuB7qO
                  P8DyAcsHLI8/4PmB5weejz+AAAECBAiQP4AAAQIECJA/MAvBLASzkD8wC8EsBLOQP8AOATsE7JA/
                  UBJBSQQlkT9QEkFJBCWRP3AZwWUEl5E/kCBBggQJkj+QIEGCBAmSP7AnwZ4Ee5I/QCsBrQS0kj9A
                  KwGtBLSSP2AygckEJpM/YDKByQQmkz9gMoHJBCaTPxA9QfQE0ZM/oECBAgUKlD9wUsFJBSeVPwBW
                  AVgFYJU/sGDBggULlj+wYMGCBQuWP0BkAZEFRJY/gHIBygUolz+geYHmBZqXPzB9wfQF05c/wIAB
                  AwYMmD8AjwE8BvCYP7CZwWYGm5k/8KfBnwZ/mj/AuQHnBpybP3DEwREHR5w/cMTBEQdHnD+Qy0Eu
                  B7mcP7DSwUoHK50/MO/BvAfznj/A8gHLByyfP3D9wfUH158/EARBEARBoD+oF6FehHqhP+gloZeE
                  XqI/CC0htITQoj9gMoHJBCajP8BHAR8FfKQ/yFchX4V8pT9AZAGRBUSmP7hw4cKFC6c/iIIhCoYo
                  qD+oiaEmhpqoPyCWgVgGYqk/KKahmIZiqj9otKHRhkarP8jJISeHnKw/gOQBkgdIrj+o+6Huh7qv
                  P7wG8RrEa7A/UBJBSQQlsT+QIEGCBAmyP9AuQbsE7bI/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  yxEAAAAAAADKEQAAAAAAAMkRAAAAAAAAyBEAAAAAAADHEQAAAAAAAMcRAAAAAAAAxxEAAAAAAADH
                  EQAAAAAAAMURAAAAAAAAxREAAAAAAADCEQAAAAAAAMIRAAAAAAAAwREAAAAAAAC+EQAAAAAAALoR
                  AAAAAAAAuREAAAAAAAC3EQAAAAAAALcRAAAAAAAAtBEAAAAAAACyEQAAAAAAALERAAAAAAAAsBEA
                  AAAAAACvEQAAAAAAAK8RAAAAAAAArBEAAAAAAACsEQAAAAAAAKsRAAAAAAAAqhEAAAAAAACqEQAA
                  AAAAAKgRAAAAAAAAphEAAAAAAACmEQAAAAAAAKQRAAAAAAAAoxEAAAAAAACjEQAAAAAAAKERAAAA
                  AAAAoREAAAAAAAChEQAAAAAAAJ4RAAAAAAAAnREAAAAAAACYEQAAAAAAAJcRAAAAAAAAlBEAAAAA
                  AACUEQAAAAAAAJMRAAAAAAAAjxEAAAAAAACNEQAAAAAAAIwRAAAAAAAAixEAAAAAAACHEQAAAAAA
                  AIQRAAAAAAAAgBEAAAAAAAB7EQAAAAAAAHgRAAAAAAAAeBEAAAAAAAB2EQAAAAAAAHQRAAAAAAAA
                  bBEAAAAAAABrEQAAAAAAAGgRAAAAAAAAZREAAAAAAABaEQAAAAAAAFIRAAAAAAAAThEAAAAAAABL
                  EQAAAAAAAD8RAAAAAAAANhEAAAAAAAAvEQAAAAAAACgRAAAAAAAAHhEAAAAAAAAaEQAAAAAAABMR
                  AAAAAAAAChEAAAAAAAACEQAAAAAAAPYQAAAAAAAA5xAAAAAAAADaEAAAAAAAANAQAAAAAAAAwxAA
                  AAAAAACzEAAAAAAAAKMQAAAAAAAA5AQAAAAAAABQBAAAAAAAAAoEAAAAAAAAuwMAAAAAAACFAwAA
                  AAAAAFEDAAAAAAAAJQMAAAAAAAADAwAAAAAAAOQCAAAAAAAAzAIAAAAAAAC5AgAAAAAAAJwCAAAA
                  AAAAeQIAAAAAAABUAgAAAAAAADsCAAAAAAAAKAIAAAAAAAAYAgAAAAAAAAsCAAAAAAAABAIAAAAA
                  AAD+AQAAAAAAAPIBAAAAAAAA4gEAAAAAAADaAQAAAAAAAM8BAAAAAAAAxQEAAAAAAAC+AQAAAAAA
                  ALcBAAAAAAAArgEAAAAAAACrAQAAAAAAAKUBAAAAAAAAmgEAAAAAAACLAQAAAAAAAIABAAAAAAAA
                  eAEAAAAAAAB0AQAAAAAAAGwBAAAAAAAAZgEAAAAAAABjAQAAAAAAAF4BAAAAAAAAVgEAAAAAAABU
                  AQAAAAAAAE4BAAAAAAAASgEAAAAAAABDAQAAAAAAADsBAAAAAAAANAEAAAAAAAAtAQAAAAAAACkB
                  AAAAAAAAHgEAAAAAAAAYAQAAAAAAAA8BAAAAAAAABwEAAAAAAAAAAQAAAAAAAPsAAAAAAAAA9QAA
                  AAAAAADyAAAAAAAAAO4AAAAAAAAA6gAAAAAAAADjAAAAAAAAAN4AAAAAAAAA2AAAAAAAAADTAAAA
                  AAAAAMkAAAAAAAAAwAAAAAAAAAC0AAAAAAAAAKwAAAAAAAAAogAAAAAAAACbAAAAAAAAAJMAAAAA
                  AAAAjAAAAAAAAACGAAAAAAAAAIEAAAAAAAAAdwAAAAAAAAB0AAAAAAAAAGwAAAAAAAAAXQAAAAAA
                  AABSAAAAAAAAAE8AAAAAAAAAQwAAAAAAAAA7AAAAAAAAADUAAAAAAAAALAAAAAAAAAAtAAAAAAAA
                  AC4AAAAAAAAALwAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADIAAAAAAAAA
                  MgAAAAAAAAA1AAAAAAAAADUAAAAAAAAANgAAAAAAAAA5AAAAAAAAAD0AAAAAAAAAPgAAAAAAAABA
                  AAAAAAAAAEAAAAAAAAAAQwAAAAAAAABFAAAAAAAAAEYAAAAAAAAARwAAAAAAAABIAAAAAAAAAEgA
                  AAAAAAAASwAAAAAAAABLAAAAAAAAAEwAAAAAAAAATQAAAAAAAABNAAAAAAAAAE8AAAAAAAAAUQAA
                  AAAAAABRAAAAAAAAAFMAAAAAAAAAVAAAAAAAAABUAAAAAAAAAFYAAAAAAAAAVgAAAAAAAABWAAAA
                  AAAAAFkAAAAAAAAAWgAAAAAAAABfAAAAAAAAAGAAAAAAAAAAYwAAAAAAAABjAAAAAAAAAGQAAAAA
                  AAAAaAAAAAAAAABqAAAAAAAAAGsAAAAAAAAAbAAAAAAAAABwAAAAAAAAAHMAAAAAAAAAdwAAAAAA
                  AAB8AAAAAAAAAH8AAAAAAAAAfwAAAAAAAACBAAAAAAAAAIMAAAAAAAAAiwAAAAAAAACMAAAAAAAA
                  AI8AAAAAAAAAkgAAAAAAAACdAAAAAAAAAKUAAAAAAAAAqQAAAAAAAACsAAAAAAAAALgAAAAAAAAA
                  wQAAAAAAAADIAAAAAAAAAM8AAAAAAAAA2QAAAAAAAADdAAAAAAAAAOQAAAAAAAAA7QAAAAAAAAD1
                  AAAAAAAAAAEBAAAAAAAAEAEAAAAAAAAdAQAAAAAAACcBAAAAAAAANAEAAAAAAABEAQAAAAAAAFQB
                  AAAAAAAAFE8AAAAAAACoTwAAAAAAAO5PAAAAAAAAPVAAAAAAAABzUAAAAAAAAKdQAAAAAAAA01AA
                  AAAAAAD1UAAAAAAAABRRAAAAAAAALFEAAAAAAAA/UQAAAAAAAFxRAAAAAAAAf1EAAAAAAACkUQAA
                  AAAAAL1RAAAAAAAA0FEAAAAAAADgUQAAAAAAAO1RAAAAAAAA9FEAAAAAAAD6UQAAAAAAAAZSAAAA
                  AAAAFlIAAAAAAAAeUgAAAAAAAClSAAAAAAAAM1IAAAAAAAA6UgAAAAAAAEFSAAAAAAAASlIAAAAA
                  AABNUgAAAAAAAFNSAAAAAAAAXlIAAAAAAABtUgAAAAAAAHhSAAAAAAAAgFIAAAAAAACEUgAAAAAA
                  AIxSAAAAAAAAklIAAAAAAACVUgAAAAAAAJpSAAAAAAAAolIAAAAAAACkUgAAAAAAAKpSAAAAAAAA
                  rlIAAAAAAAC1UgAAAAAAAL1SAAAAAAAAxFIAAAAAAADLUgAAAAAAAM9SAAAAAAAA2lIAAAAAAADg
                  UgAAAAAAAOlSAAAAAAAA8VIAAAAAAAD4UgAAAAAAAP1SAAAAAAAAA1MAAAAAAAAGUwAAAAAAAApT
                  AAAAAAAADlMAAAAAAAAVUwAAAAAAABpTAAAAAAAAIFMAAAAAAAAlUwAAAAAAAC9TAAAAAAAAOFMA
                  AAAAAABEUwAAAAAAAExTAAAAAAAAVlMAAAAAAABdUwAAAAAAAGVTAAAAAAAAbFMAAAAAAAByUwAA
                  AAAAAHdTAAAAAAAAgVMAAAAAAACEUwAAAAAAAIxTAAAAAAAAm1MAAAAAAACmUwAAAAAAAKlTAAAA
                  AAAAtVMAAAAAAAC9UwAAAAAAAMNTAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          XI/C9Shc7z8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          Vg4tsp3v7z8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          rBxaZDvf7z8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          MPz/////7z8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          3yr/af7/7z8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          CLv+////7z8=
    randomforest:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          MzMzMzMz4z8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          4XoUrkfh4j8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          exSuR+F65D8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/tsEI07+y0z8t7L0GewbUP1YElNr1bNQ/SoyfRg811T/qUrGt
                  N7zVPzFyPoWsHdY/V+bZAnZx1j8s04kfYa7WP/K5oSjL99Y/zWR/KNUt1z82WfOa41HXP+UFEWoy
                  ddc/3NRIhSeP1z8rzu4JS+fXP0ohiWC9K9g/pXdtbt1j2D9YDNjEdHvYP8ggetfRjtg/R+w3rUWr
                  2D8hsfjEibvYP4ab/l+G0Ng/ASxqEIXf2D+hY37ZvPfYPzpnZ307DNk/rF0xxjck2T822WSTTTbZ
                  P2TFnNARTNk/USBbbUto2T8ihdGJz4DZP9Tzwk/JrNk/mE4FX2j32T+g38YVMUDaP3dMQTKyido/
                  GWOMMcYY2z9mVZb00cHbP0SgKzBletw/WIpxDV5j3T+KDfvJxv7dP38AF38AF98/i6/4iq8Y4D/o
                  A5lL0svgP/UYZoqqueE/aaKJJppo4j+rCF4EgCjjP/eR5JHOweM/7qJFjQiz5D/a2FWPNnblP8e/
                  wNQFeOY/mB5XyDHg5z8omApETQbpPwYENozI8uk/vtomKcys6j/YUF5Dea3rP7UuPCFX6+w/85R5
                  yjxl7j9sC2ZcKALvP5co5sPiP+8/dvwcPs5w7z/1wol5Ap3vP4KuvVPK0O8/QVsZvQjZ7z961Md1
                  W9rvP7cXHYAH7+8/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAA
                  APA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/4/+N/zf+7z/j/43/N/7v
                  P+P/jf83/u8/4/+N/zf+7z/H/xv/b/zvP8f/G/9v/O8/x/8b/2/87z9x/8X9F/fvP//+/fv37+8/
                  xv4Z+2fs7z+N/jX61+jvP43+NfrX6O8/OP7f+H/j7z8b/m34t+HvP+L9ifcn3u8//vz58+fP7z+p
                  /KPyj8rvP3D8v/H/xu8//vv379+/7z/h+4XvF77vP+H7he8Xvu8/b/u97fe27z9T+0vtL7XvP/36
                  9evXr+8/qPqf6n+q7z+L+i3qt6jvP+D5gecHnu8/NfnV5FeT7z9R+EXhF4XvP233td3Xdu8/UfdD
                  3Q917z/79u3bt2/vP8L2CdsnbO8/pvaX2l9q7z+J9iXal2jvP1D2QdkHZe8/UPZB2Qdl7z809s/Y
                  P2PvP/v169evX+8/wvUH1x9c7z9s9bHVx1bvP/r06dOnT+8/T/Q90fdE7z8W9FnQZ0HvPxbzV8xf
                  Me8/+fHjx48f7z8U8FHARwHvPzDuv7j/4u4/EuxJsCfB7j8Q6EGgB4HuP5zib4q/Ke4/tdvTbk+7
                  7T9c1G9Rv0XtP8nKIyuPrOw/GL9h/Ibx6z8vs7vM7jLrP4Cq/6n+p+o/Q6MNjTY06j+UmU9mPpnp
                  P1iTX01+Nek//4v7L+6/6D/dgXUH1h3oP2Z3md1lduc/m29tvrX55j/NYDODzQzmP/xN8TfF3+Q/
                  nDdv3rx54z+nFZ1WdFrhP50BdQbUGeA/xcMVD1c83D95neV1ltfZP9h3Yd+Ffdc/N1LdSHUj1T+V
                  KlWqVKnSP58FfRb0WdA/YN2BdQfWzT/CvQn3JtzLP4iCIQqGKMg/3kp5K+WtxD8EJRGURFDCPxgU
                  YVCEQcE/3vnZsn8f3j+DdtKLBYHePxD1qUVa994/cmSNbDLb3z9AuYuaJDngP/aDVokyb+A/8qaL
                  E0qd4D8okUGT2b3gPwmbsWC45OA/OW8G+mAB4T+gATOgPhThP3QuOT4nJ+E/WxMPND004T9RETlw
                  ymLhP9MnwQ5ahuE/mKYktpih4T8jiV79Fa3hP8/s4CaetuE/DJ8sc0nE4T/U8vYccMzhP9ygiepL
                  1+E/uCMWpOnd4T//EjNuHOrhPxEo+ZXL8+E/N9SVoD3/4T8FnNYmNgjiP8r91G+XEeI/PiXuIDce
                  4j8XckygTijiPxUDv0IqPOI/JWRH02Bh4j98S/7LzYTiP1t7bK6jqOI/WhoDWZvu4j9E7wohMUDj
                  Pztc9giUl+M/d5J7lcEE5D8TLmDCBUzkP4ANREYOyuQ/E42wDwdG5T8yBHYP597lP+RAfRXcouY/
                  M1b3q50s5z/iE86zYsHnP2WwsZ4KMeg/igxeQerc6D/dtSeQj13pP55fKSOEA+o/NYDeMs/h6j/Q
                  ndEMdX7rP1y+5Vu+5es/O+28A9oe7D/sdszW5HPsP8D2XO7Qy+w/Hfyx5TAe7T9c+d/3ofrsP1oW
                  nFJNxew/lnTw352V7D9gyU9KMErsP0nSPyMrIew/khDLwYna6z8QeBKpN3PrP5wS5kPmDOs/TQKN
                  ADu/6j+9HFlK8hvqP+f2UTE4ROk/oUf3CdA26D/2KwYh2YDmP4tQgVw+bOU/8gTgho+X4z8mOJkb
                  r2jiP0RM4e3yLuE/W/FUE8PH3z/m/FnOn+XcP5GED6c8DNo/8nA2pjIy2D/hehSuR+HWP7D+Ew+a
                  UtQ/g6MK/5XN0T9Isls9dAXQPwS8Dfh1ac4/AAAAAAA1xEAAAAAAALvDQAAAAAAAK8NAAAAAAIAh
                  wkAAAAAAgHrBQAAAAAAABsFAAAAAAAClwEAAAAAAAGXAQAAAAACAGsBAAAAAAADHv0AAAAAAAIG/
                  QAAAAAAAN79AAAAAAAALv0AAAAAAAFu+QAAAAAAA2r1AAAAAAACIvUAAAAAAAGW9QAAAAAAAR71A
                  AAAAAAAfvUAAAAAAAAS9QAAAAAAA3bxAAAAAAADPvEAAAAAAAKa8QAAAAAAAi7xAAAAAAABqvEAA
                  AAAAAE28QAAAAAAAO7xAAAAAAAAevEAAAAAAAA+8QAAAAAAA37tAAAAAAABku0AAAAAAAPa6QAAA
                  AAAAhrpAAAAAAACpuUAAAAAAAK+4QAAAAAAAr7dAAAAAAAB1tkAAAAAAALK1QAAAAAAAZ7RAAAAA
                  AAAxs0AAAAAAAMixQAAAAAAAF7BAAAAAAAAGrkAAAAAAALSrQAAAAAAAPKpAAAAAAAAAqEAAAAAA
                  AK6mQAAAAAAA9qRAAAAAAAC+okAAAAAAAPKhQAAAAAAANqJAAAAAAABmo0AAAAAAAHKkQAAAAAAA
                  GqZAAAAAAABuqEAAAAAAAO6rQAAAAAAAuK5AAAAAAACIsEAAAAAAACWyQAAAAAAAJbNAAAAAAABr
                  tEAAAAAAADG2QAAAAAAA/rdAAAAAAABVuUAAAAAAAO67QAAAAAAAO79AAAAAAACUwUAAAAAAgI7E
                  QAAAAACAUMZAAAAAAAAZyUAAAAAAAMfKQAAAAACAbcxAAAAAAAAUzkAAAAAAANHPQAAAAAAAuNBA
                  AAAAAMA40UAAAAAAgJHRQAAAAADAN9JAAAAAAADU0kAAAAAAQD7TQAAAAADAbdNAB8k+E8rJ3j9X
                  DYy+0A/eP8pE0h9ONN0/BrsrS5Wf2z9KDAGLM53aP9ET0Iic69k/1kPINr9X2T8JWjEGwerYP+j/
                  frXxadg/ApexSHsO2D8UUdOqgdHXP/7pmMUamdc/BTD4rSJs1z8zGP7rLeLWP0sYfzY8eNY/c7ol
                  jD8b1j++znqJI/XVP8eri7qm1tU/VfckhOuo1T8oqGWrh5DVP6t/PPvNctU/jDvx5ONY1T+8OV5d
                  1DXVP9Ih41DRFdU/ECvEizvx1D/x5nh1UdfUPy/wWbC7stQ/Nja5mMOF1D8iZjD811vUP2lMImjF
                  GNQ/uA8lBTa30z90wnn+71fTP5xkIFTz+tI/dEVZKbJO0j+NxiSCVozRP26EboiYwdA/G5cyk4mk
                  zz9zOdt9qXPOPxfybYLZa8w/pcJ7kwyEyj8Y/ust4kbIP4eMQ0VXlMU/XML4s+HBwz89AyLl5e3B
                  P0sSwOC3isA/WScdMg4VvT+hFWMQrA66Py9r5WjNbLY/ZSR+LFPpsT8k+0woJxutP81q4T6UFKg/
                  f7NcTSxRpD8bGH2hH0yfP1NiCFic6ZQ/iJBtfq8yhD8UUdOqgdF3P7XngMnDh3E/LuJG6IaIaT8E
                  q4NmNCZhP/P0iz2GAVA/3h5ESxbqST8dLE+/2GNIP5lGZaddVzU/AAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAMgBIAeALD8AyAEgB4AsPwDIASAHgCw/AMgBIAeALD8AyAEgB4A8PwDIASAH
                  gDw/AMgBIAeAPD8AHQF0BNBRP4AAAQIECGA/gDkB5gSYYz+AcgHKBShnP4ByAcoFKGc/AMgBIAeA
                  bD+A5AGSB0huP8AOATsE7HA/wIABAwYMeD+AqwGuBrh6PwDIASAHgHw/gAABAgQIgD+gB4EeBHqA
                  P6AHgR4EeoA/ICSBkARCgj9AKwGtBLSCP6BAgQIFCoQ/AFYBWAVghT8gXYF0BdKFP+CHgR8Gfog/
                  oLKBygYqiz+g64GuB7qOP1ASQUkEJZE/4BWBVwRekT+QIEGCBAmSP7AnwZ4Ee5I/QCsBrQS0kj/Q
                  LkG7BO2SP/A1wdcEX5M/8DXB1wRfkz+AOQHmBJiTP6BAgQIFCpQ/wEcBHwV8lD9wUsFJBSeVP7Bg
                  wYIFC5Y/EHZB2AVhlz8wfcH0BdOXP0CdAXUG1Jk/4MCBAwcOnD9w/cH1B9efPwAdAXQE0KE/2D5h
                  +4Ttoz/4fuH7he+nP0DWAVkHZK0/WCJhiYQlsj8gXYF0BdK1P7ip4aaGm7o/ngN5DuQ5wD9EMxHN
                  RDTDPwBWAVgFYMU/8nLJyyUvxz+wmcFmBpvJP6CygcoGKss/BNARQEcAzT+K+Cnip4jPPzMRzUQ0
                  E9E/ySAlg5QM0j9mPpn5ZObTPwdkHZB1QNY/yJAhQ4YM2T+x1MVSF0vdP8X8FfNXzN8/HR51eNTh
                  4T9DMQ3FNBTjPxRETxA9QeQ/5FaRW0Vu5T+1atWqVavmPzB9wfQF0+c/qIifIn6K6D+PkD1C9gjp
                  P16fd33e9ek/SK0htYbU6j+/tvva7mvrP/q65+uer+s/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  9hEAAAAAAAD2EQAAAAAAAPYRAAAAAAAA9hEAAAAAAAD1EQAAAAAAAPURAAAAAAAA9REAAAAAAADy
                  EQAAAAAAAO4RAAAAAAAA7BEAAAAAAADqEQAAAAAAAOoRAAAAAAAA5xEAAAAAAADmEQAAAAAAAOQR
                  AAAAAAAA3BEAAAAAAADZEQAAAAAAANcRAAAAAAAA0xEAAAAAAADSEQAAAAAAANIRAAAAAAAAzhEA
                  AAAAAADNEQAAAAAAAMoRAAAAAAAAxxEAAAAAAADGEQAAAAAAAMARAAAAAAAAuhEAAAAAAACyEQAA
                  AAAAAKoRAAAAAAAAqREAAAAAAACmEQAAAAAAAKQRAAAAAAAAoxEAAAAAAACiEQAAAAAAAKARAAAA
                  AAAAoBEAAAAAAACfEQAAAAAAAJ0RAAAAAAAAmxEAAAAAAACYEQAAAAAAAJQRAAAAAAAAjhEAAAAA
                  AACMEQAAAAAAAIMRAAAAAAAAeREAAAAAAABoEQAAAAAAAFcRAAAAAAAARBEAAAAAAAAgEQAAAAAA
                  AO8QAAAAAAAAsRAAAAAAAABvEAAAAAAAABkQAAAAAAAAsA8AAAAAAABFDwAAAAAAAPcOAAAAAAAA
                  tg4AAAAAAABfDgAAAAAAACcOAAAAAAAA5Q0AAAAAAACKDQAAAAAAACwNAAAAAAAA5gwAAAAAAABh
                  DAAAAAAAALgLAAAAAAAA7woAAAAAAAC+CQAAAAAAAAoJAAAAAAAA7QcAAAAAAABBBwAAAAAAAJgG
                  AAAAAAAA7wUAAAAAAAA9BQAAAAAAAJcEAAAAAAAAMAQAAAAAAADpAwAAAAAAAGQDAAAAAAAA5wIA
                  AAAAAACSAgAAAAAAAGwCAAAAAAAAZSgAAAAAAABxJwAAAAAAAFEmAAAAAAAAPiQAAAAAAADrIgAA
                  AAAAAAIiAAAAAAAAQCEAAAAAAACxIAAAAAAAAAggAAAAAAAAkB8AAAAAAABAHwAAAAAAAPYeAAAA
                  AAAAux4AAAAAAAAGHgAAAAAAAHsdAAAAAAAAAR0AAAAAAADPHAAAAAAAAKccAAAAAAAAaxwAAAAA
                  AABLHAAAAAAAACQcAAAAAAAAAhwAAAAAAADUGwAAAAAAAKobAAAAAAAAehsAAAAAAABYGwAAAAAA
                  ACgbAAAAAAAA7RoAAAAAAAC2GgAAAAAAAF4aAAAAAAAA3hkAAAAAAABhGQAAAAAAAOcYAAAAAAAA
                  BRgAAAAAAAAGFwAAAAAAAPwVAAAAAAAAwhQAAAAAAAD6EwAAAAAAAKUSAAAAAAAAZREAAAAAAADt
                  DwAAAAAAACgOAAAAAAAA9gwAAAAAAADDCwAAAAAAANoKAAAAAAAAigkAAAAAAACMCAAAAAAAAFsH
                  AAAAAAAA4AUAAAAAAADGBAAAAAAAAPMDAAAAAAAAVQMAAAAAAACRAgAAAAAAALcBAAAAAAAA1AAA
                  AAAAAAB9AAAAAAAAAFwAAAAAAAAAQwAAAAAAAAAtAAAAAAAAABUAAAAAAAAAEQAAAAAAAAAQAAAA
                  AAAAAAcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAABAAAAAAAA
                  AAEAAAAAAAAAAQAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAAFAAAAAAAAAAkAAAAAAAAA
                  CwAAAAAAAAANAAAAAAAAAA0AAAAAAAAAEAAAAAAAAAARAAAAAAAAABMAAAAAAAAAGwAAAAAAAAAe
                  AAAAAAAAACAAAAAAAAAAJAAAAAAAAAAlAAAAAAAAACUAAAAAAAAAKQAAAAAAAAAqAAAAAAAAAC0A
                  AAAAAAAAMAAAAAAAAAAxAAAAAAAAADcAAAAAAAAAPQAAAAAAAABFAAAAAAAAAE0AAAAAAAAATgAA
                  AAAAAABRAAAAAAAAAFMAAAAAAAAAVAAAAAAAAABVAAAAAAAAAFcAAAAAAAAAVwAAAAAAAABYAAAA
                  AAAAAFoAAAAAAAAAXAAAAAAAAABfAAAAAAAAAGMAAAAAAAAAaQAAAAAAAABrAAAAAAAAAHQAAAAA
                  AAAAfgAAAAAAAACPAAAAAAAAAKAAAAAAAAAAswAAAAAAAADXAAAAAAAAAAgBAAAAAAAARgEAAAAA
                  AACIAQAAAAAAAN4BAAAAAAAARwIAAAAAAACyAgAAAAAAAAADAAAAAAAAQQMAAAAAAACYAwAAAAAA
                  ANADAAAAAAAAEgQAAAAAAABtBAAAAAAAAMsEAAAAAAAAEQUAAAAAAACWBQAAAAAAAD8GAAAAAAAA
                  CAcAAAAAAAA5CAAAAAAAAO0IAAAAAAAACgoAAAAAAAC2CgAAAAAAAF8LAAAAAAAACAwAAAAAAAC6
                  DAAAAAAAAGANAAAAAAAAxw0AAAAAAAAODgAAAAAAAJMOAAAAAAAAEA8AAAAAAABlDwAAAAAAAIsP
                  AAAAAAAAkysAAAAAAACHLAAAAAAAAKctAAAAAAAAui8AAAAAAAANMQAAAAAAAPYxAAAAAAAAuDIA
                  AAAAAABHMwAAAAAAAPAzAAAAAAAAaDQAAAAAAAC4NAAAAAAAAAI1AAAAAAAAPTUAAAAAAADyNQAA
                  AAAAAH02AAAAAAAA9zYAAAAAAAApNwAAAAAAAFE3AAAAAAAAjTcAAAAAAACtNwAAAAAAANQ3AAAA
                  AAAA9jcAAAAAAAAkOAAAAAAAAE44AAAAAAAAfjgAAAAAAACgOAAAAAAAANA4AAAAAAAACzkAAAAA
                  AABCOQAAAAAAAJo5AAAAAAAAGjoAAAAAAACXOgAAAAAAABE7AAAAAAAA8zsAAAAAAADyPAAAAAAA
                  APw9AAAAAAAANj8AAAAAAAD+PwAAAAAAAFNBAAAAAAAAk0IAAAAAAAALRAAAAAAAANBFAAAAAAAA
                  AkcAAAAAAAA1SAAAAAAAAB5JAAAAAAAAbkoAAAAAAABsSwAAAAAAAJ1MAAAAAAAAGE4AAAAAAAAy
                  TwAAAAAAAAVQAAAAAAAAo1AAAAAAAABnUQAAAAAAAEFSAAAAAAAAJFMAAAAAAAB7UwAAAAAAAJxT
                  AAAAAAAAtVMAAAAAAADLUwAAAAAAAONTAAAAAAAA51MAAAAAAADoUwAAAAAAAPFTAAAAAAAA+FMA
                  AAAAAAD4UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAA
                  AAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAA
                  AAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          m75ke3NH7j8=
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          Gzy9SlqJ6z8=
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          8yo/bSFI7T8=
    xgboost:
      threshold_optimization:
        cost_fn: 5.0
        cost_fp: 1.0
        optimal_constrained_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_cost_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          mpmZmZmZuT8=
        optimal_f1_threshold: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          SOF6FK5H0T8=
        threshold_analysis: !!python/object:pandas.core.frame.DataFrame
          _flags:
            allows_duplicate_labels: true
          _metadata: *id003
          _mgr: !!python/object/apply:pandas.core.internals.managers.BlockManager
          - !!python/tuple
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 7
                  - 81
                - *id001
                - false
                - !!binary |
                  mpmZmZmZuT8pXI/C9Si8P7kehetRuL4/pHA9CtejwD/sUbgehevBPzQzMzMzM8M/exSuR+F6xD/D
                  9Shcj8LFPwrXo3A9Csc/UrgehetRyD+amZmZmZnJP+J6FK5H4co/KVyPwvUozD9xPQrXo3DNP7ke
                  hetRuM4/AAAAAAAA0D+kcD0K16PQP0jhehSuR9E/7FG4HoXr0T+QwvUoXI/SPzQzMzMzM9M/16Nw
                  PQrX0z97FK5H4XrUPx+F61G4HtU/wvUoXI/C1T9mZmZmZmbWPwrXo3A9Ctc/rkfhehSu1z9SuB6F
                  61HYP/YoXI/C9dg/mpmZmZmZ2T8+CtejcD3aP+J6FK5H4do/hutRuB6F2z8qXI/C9SjcP87MzMzM
                  zNw/cD0K16Nw3T8UrkfhehTeP7gehetRuN4/XI/C9Shc3z8AAAAAAADgP1K4HoXrUeA/pHA9Ctej
                  4D/2KFyPwvXgP0jhehSuR+E/mpmZmZmZ4T/sUbgehevhPz4K16NwPeI/j8L1KFyP4j/hehSuR+Hi
                  PzMzMzMzM+M/hetRuB6F4z/Xo3A9CtfjPylcj8L1KOQ/exSuR+F65D/NzMzMzMzkPx+F61G4HuU/
                  cT0K16Nw5T/C9Shcj8LlPxSuR+F6FOY/ZmZmZmZm5j+4HoXrUbjmPwrXo3A9Cuc/XI/C9Shc5z+u
                  R+F6FK7nPwAAAAAAAOg/UrgehetR6D+kcD0K16PoP/YoXI/C9eg/SOF6FK5H6T+amZmZmZnpP+tR
                  uB6F6+k/PQrXo3A96j+PwvUoXI/qP+F6FK5H4eo/MzMzMzMz6z+F61G4HoXrP9ejcD0K1+s/KVyP
                  wvUo7D97FK5H4XrsP83MzMzMzOw/bm2fabpy7z9R9nQKEYTvP2ZIF9axjO8/o40KQluV7z+xcs6m
                  kqHvPxcdk5ifqO8/sK+vr6+v7z96h5PRb7HvP7IjV9hJuu8/FvxSul7B7z/tMd6x5cTvP/jA00CM
                  z+8/qF/tAx7T7z/8gyh/G9PvPzB6gPCr1u8/MHqA8KvW7z+05XaVPdrvP8+ugwqd3+8/STGufGTh
                  7z/EoifA+eTvP9gcSLTC5u8/lkr5Qozo7z9t6V1QIuzvP0Wq9Tq87+8/bkICDorx7z9uQgIOivHv
                  PwlG+aZW8+8/60NhOFXz7z+AKPaAVPPvP/yu9z4j9e8/rQxYMfL27z+tDFgx8vbvP7ksIe/A+O8/
                  Y08WhsD47z8CjP8cwPjvPzI7JIuO+u8/kH0dPI767z88G/WdjfrvP31z006N+u8/j484YYz67z+P
                  jzhhjPrvP7mfTGFd/O8/uZ9MYV387z8fVJ3CXPzvP4ThtyNc/O8/2q207lv87z+/cau5W/zvP3WA
                  axpb/O8/dYBrGlv87z/ricFFWvzvPxjlhxBa/O8/hi5I21n87z8rZQKmWfzvP/yHtnBZ/O8/P+jf
                  ZVj87z8/6N9lWPzvP8VXfMVX/O8/PJn5j1f87z9NwOEkV/zvP03A4SRX/O8/iWxdDCv+7z94hYPx
                  Kv7vPxJa46Aq/u8/cSL9hSr+7z9FcidQKv7vPwRYIQwp/u8/fbSahCj+7z/j/05OKP7vP8x0JDMo
                  /u8/eMj2Fyj+7z9a+sX8J/7vPwl4GD4n/u8/rzMx7Cb+7z8AAAAAAADwPwAAAAAAAPA/AAAAAAAA
                  8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/jPsv7r+47z9T+0vtL7Xv
                  P+H6g+sPru8/b/q76e+m7z/9+fPnz5/vP/358+fPn+8//fnz58+f7z/g+YHnB57vP+D5gecHnu8/
                  xPkP5z+c7z+L+Svmr5jvPzX51eRXk+8/NfnV5FeT7z8Z+WPkj5HvP/z48ePHj+8//Pjx48eP7z/g
                  +H/j/43vP+D4f+P/je8/p/ib4m+K7z+K+Cnip4jvP1H4ReEXhe8/GPhh4IeB7z/D9wvfL3zvP6b3
                  md5neu8/pveZ3md67z+m95neZ3rvP233td3Xdu8/NPfR3Edz7z8Y91/cf3HvPxj3X9x/ce8/GPdf
                  3H9x7z8Y91/cf3HvP/v27du3b+8/3/Z72+9t7z/C9gnbJ2zvP1D2QdkHZe8/NPbP2D9j7z/79evX
                  r1/vP971edfnXe8/ifUj1o9Y7z+J9SPWj1jvP4n1I9aPWO8/ifUj1o9Y7z8z9c3UN1PvP970d9Pf
                  Te8/wfQF0xdM7z+l9JPST0rvP0/0PdH3RO8/T/Q90fdE7z/d83XP1z3vP8HzA88PPO8/pPORzkc6
                  7z+I8x/OfzjvP2vzrc23Nu8/3fJzy88t7z/d8nPLzy3vP4fyHcp3KO8/a/Krya8m7z8y8sfIHyPv
                  PzLyx8gfI+8/wPH/xv8b7z+j8Y3GNxrvP07xN8XfFO8/MfHFxBcT7z/48OHDhw/vP6Lvib4n+u4/
                  FO9PvD/x7j/b7mu7r+3uP77u+brn6+4/ou6Huh/q7j+F7hW6V+juP77t97bf2+4/aO2htYfW7j+h
                  7IOyD8ruP2jsn7F/xu4/L+y7sO/C7j/269evX7/uPxLrR6wfse4/Suopqaek7j+75+uer3vuP9fm
                  W5tvbe4/MA4AXZaV7z9woPFmjZzvP+oH4xJYne8//fFAJCOe7z8pQBtBsaDvP8MPOiM3pO8/E8Ra
                  zr2n7z9lxCjjuKfvP4bZbZIirO8/Z85BccSu7z/H4c5fu67vP492eXtVse8/Rl414Rqz7z97JifP
                  M7LvP2PcHkgSs+8/Y9weSBKz7z+w+mnm8LPvP948QvWZtu8/2Qkgk6617z9Mz52hjbbvPxdV5xai
                  te8/LYGbZLa07z80MDhFxrPvP8vEL4KltO8/+/HbEYm17z/78dsRibXvP1qNIg6dtO8/VHH0Oc2y
                  7z96ilg85bHvP0vE8+TIsu8/dM9Vmqyz7z90z1WarLPvPxPBgVGos+8/LNt8OcCy7z/F9G8U2LHv
                  P0dz7Msar+8/kwaGZTKu7z8+K4txYazvP+iI9ON4q+8/ZJa+7L6o7z9klr7svqjvP9N8tMaiqe8/
                  03y0xqKp7z8GP1FZ6KbvP9yTCnYtpO8/Pv1lZUSj7z8KtaNHW6LvP6Cfn5+fn+8/oJ+fn5+f7z88
                  PLcH+pvvPweA34AQm+8/iBbg7Caa7z8o47dLPZnvPy3JZZ1TmO8/ZM4vcMKT7z9kzi9wwpPvPzzs
                  alAEke8/25VtKxqQ7z/zpNK5RY7vP/Ok0rlFju8/qWdJGYCL7z+7W5GXlYrvPzsh9cLVh+8/6UhB
                  DOuG7z+lNBN3FYXvP/xic5wPeu8/KcwXoXV17z+Gm2USnnPvPxzoBjeycu8/sZ9NTsZx7z/onzhY
                  2nDvP5YxPidlau8/41i4259n7z/KFcTkCmLvP/GQOggxYO8/Xm/l9VZe7z9ph7utfFzvP4WdDHIR
                  Ve8/YwIE2ZBO7z+Z590uITnvP/FVxdWlMe8/AAAAAACAcUAAAAAAAIBxQAAAAAAAcHJAAAAAAABg
                  c0AAAAAAADB0QAAAAAAA8HNAAAAAAACwc0AAAAAAAPBzQAAAAAAAoHNAAAAAAACwc0AAAAAAADB0
                  QAAAAAAAwHRAAAAAAACgdEAAAAAAAPB0QAAAAAAAIHVAAAAAAAAgdUAAAAAAAFB1QAAAAAAAIHVA
                  AAAAAACwdUAAAAAAAOB1QAAAAAAAcHZAAAAAAAAAd0AAAAAAANB3QAAAAAAAAHhAAAAAAADwd0AA
                  AAAAAPB3QAAAAAAAgHhAAAAAAAAgeUAAAAAAAHB5QAAAAAAAYHlAAAAAAABQeUAAAAAAAFB5QAAA
                  AAAAkHlAAAAAAADgeUAAAAAAADB6QAAAAAAAYHtAAAAAAACwe0AAAAAAAFB8QAAAAAAAoHxAAAAA
                  AACQfUAAAAAAAJB9QAAAAAAAgH1AAAAAAACAfUAAAAAAAHB+QAAAAAAAYH9AAAAAAACwf0AAAAAA
                  AACAQAAAAAAAeIBAAAAAAAB4gEAAAAAAABiBQAAAAAAAQIFAAAAAAABogUAAAAAAAJCBQAAAAAAA
                  uIFAAAAAAACAgkAAAAAAAICCQAAAAAAA+IJAAAAAAAAgg0AAAAAAAHCDQAAAAAAAcINAAAAAAAAI
                  hEAAAAAAADCEQAAAAAAAqIRAAAAAAADQhEAAAAAAACCFQAAAAAAAAIdAAAAAAADIh0AAAAAAABiI
                  QAAAAAAAQIhAAAAAAABoiEAAAAAAAJCIQAAAAAAAqIlAAAAAAAAgikAAAAAAADCLQAAAAAAAgItA
                  AAAAAADQi0AAAAAAACCMQAAAAAAAYI1AAAAAAAB4jkAAAAAAAAiRQAAAAAAAqJFAJPci7858bj8/
                  mD4RNa1qP81oTCJoxWg/WzlaM5vdZj+IkG1+rzJkP8adePJxrGI/BKuDZjQmYT9UboYDpcRgP8N9
                  KCmwuV0/P5g+ETWtWj99pUmF9yZZPzjNauE+lFQ/dtp1VQEOUz922nVVAQ5TP7XngMnDh1E/teeA
                  ycOHUT/z9Is9hgFQP6AROddTcEs/3h5ESxbqST9bOVozm91GP5lGZaddV0U/11NwGyDRQz9UboYD
                  pcRAP6AROddTcDs/HSxPv9hjOD8dLE+/2GM4P5lGZaddVzU/mUZlp11XNT+ZRmWnXVc1PxVhe4/i
                  SjI/JPci7858Lj8k9yLvznwuPx0sT7/YYyg/HSxPv9hjKD8dLE+/2GMoPxVhe4/iSiI/FWF7j+JK
                  Ij8VYXuP4koiPxVhe4/iSiI/FWF7j+JKIj8VYXuP4koiPx0sT7/YYxg/HSxPv9hjGD8dLE+/2GMY
                  Px0sT7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/
                  HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8dLE+/2GMYPx0sT7/YYxg/HSxPv9hjGD8d
                  LE+/2GMYPx0sT7/YYxg/HSxPv9hjCD8dLE+/2GMIPx0sT7/YYwg/HSxPv9hjCD8dLE+/2GMIPx0s
                  T7/YYwg/HSxPv9hjCD8dLE+/2GMIPx0sT7/YYwg/HSxPv9hjCD8dLE+/2GMIPx0sT7/YYwg/HSxP
                  v9hjCD8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAB0BdATQgT9AKwGtBLSCP8BHAR8FfIQ/QGQBkQVEhj/AgAEDBgyIP8CAAQMG
                  DIg/wIABAwYMiD/gh4EfBn6IP+CHgR8Gfog/AI8BPAbwiD9AnQF1BtSJP6CygcoGKos/oLKBygYq
                  iz/AuQHnBpyLP+DAgQMHDow/4MCBAwcOjD8AyAEgB4CMPwDIASAHgIw/QNYBWQdkjT9g3YF1B9aN
                  P6Drga4Huo4/4PmB5weejz+gB4EeBHqQPzALwSwEs5A/MAvBLASzkD8wC8EsBLOQP1ASQUkEJZE/
                  cBnBZQSXkT8AHQF0BNCRPwAdAXQE0JE/AB0BdATQkT8AHQF0BNCRP5AgQYIECZI/ICSBkARCkj+w
                  J8GeBHuSP/A1wdcEX5M/gDkB5gSYkz+gQIECBQqUPzBEwRAFQ5Q/4E6BOwXulD/gToE7Be6UP+BO
                  gTsF7pQ/4E6BOwXulD+QWUFmBZmVP0BkAZEFRJY/0GdBnwV9lj9ga4GtBbaWPxB2QdgFYZc/EHZB
                  2AVhlz9QhEERBkWYP+CHgR8Gfpg/cIvBLQa3mD8AjwE8BvCYP5CSQUoGKZk/YKSBkQZGmj9gpIGR
                  BkaaPxCvQbwG8Zo/oLKBygYqmz/AuQHnBpybP8C5AecGnJs/AMgBIAeAnD+Qy0EuB7mcP0DWAVkH
                  ZJ0/0NlBZwednT/w4MGDBw+eP9gFYReEXaA/wA4BOwTsoD9QEkFJBCWhPxgUYVCEQaE/4BWBVwRe
                  oT+oF6FehHqhPyAkgZAEQqI/eCnhpYSXoj/wNcHXBF+jP4A5AeYEmKM/ED1B9ATRoz+gQIECBQqk
                  P+BOgTsF7qQ/WFthbYW1pT9QhEERBkWoP5CSQUoGKak/
              - !!python/object/apply:builtins.slice
                - 0
                - 7
                - 1
              - 2
            - !!python/object/apply:pandas._libs.internals._unpickle_block
              - !!python/object/apply:numpy._core.multiarray._reconstruct
                args:
                - *id002
                - !!python/tuple
                  - 0
                - !!binary |
                  Yg==
                state: !!python/tuple
                - 1
                - !!python/tuple
                  - 4
                  - 81
                - *id004
                - false
                - !!binary |
                  zxEAAAAAAADNEQAAAAAAAMkRAAAAAAAAxREAAAAAAADBEQAAAAAAAMERAAAAAAAAwREAAAAAAADA
                  EQAAAAAAAMARAAAAAAAAvxEAAAAAAAC9EQAAAAAAALoRAAAAAAAAuhEAAAAAAAC5EQAAAAAAALgR
                  AAAAAAAAuBEAAAAAAAC3EQAAAAAAALcRAAAAAAAAtREAAAAAAAC0EQAAAAAAALIRAAAAAAAAsBEA
                  AAAAAACtEQAAAAAAAKwRAAAAAAAArBEAAAAAAACsEQAAAAAAAKoRAAAAAAAAqBEAAAAAAACnEQAA
                  AAAAAKcRAAAAAAAApxEAAAAAAACnEQAAAAAAAKYRAAAAAAAApREAAAAAAACkEQAAAAAAAKARAAAA
                  AAAAnxEAAAAAAACdEQAAAAAAAJwRAAAAAAAAmREAAAAAAACZEQAAAAAAAJkRAAAAAAAAmREAAAAA
                  AACWEQAAAAAAAJMRAAAAAAAAkhEAAAAAAACREQAAAAAAAI4RAAAAAAAAjhEAAAAAAACKEQAAAAAA
                  AIkRAAAAAAAAiBEAAAAAAACHEQAAAAAAAIYRAAAAAAAAgREAAAAAAACBEQAAAAAAAH4RAAAAAAAA
                  fREAAAAAAAB7EQAAAAAAAHsRAAAAAAAAdxEAAAAAAAB2EQAAAAAAAHMRAAAAAAAAchEAAAAAAABw
                  EQAAAAAAAGQRAAAAAAAAXxEAAAAAAABdEQAAAAAAAFwRAAAAAAAAWxEAAAAAAABaEQAAAAAAAFMR
                  AAAAAAAAUBEAAAAAAABJEQAAAAAAAEcRAAAAAAAARREAAAAAAABDEQAAAAAAADsRAAAAAAAANBEA
                  AAAAAAAdEQAAAAAAABURAAAAAAAAUAAAAAAAAABGAAAAAAAAAEEAAAAAAAAAPAAAAAAAAAA1AAAA
                  AAAAADEAAAAAAAAALQAAAAAAAAAsAAAAAAAAACcAAAAAAAAAIwAAAAAAAAAhAAAAAAAAABsAAAAA
                  AAAAGQAAAAAAAAAZAAAAAAAAABcAAAAAAAAAFwAAAAAAAAAVAAAAAAAAABIAAAAAAAAAEQAAAAAA
                  AAAPAAAAAAAAAA4AAAAAAAAADQAAAAAAAAALAAAAAAAAAAkAAAAAAAAACAAAAAAAAAAIAAAAAAAA
                  AAcAAAAAAAAABwAAAAAAAAAHAAAAAAAAAAYAAAAAAAAABQAAAAAAAAAFAAAAAAAAAAQAAAAAAAAA
                  BAAAAAAAAAAEAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAADAAAAAAAAAAMAAAAAAAAAAwAAAAAAAAAD
                  AAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIA
                  AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAgAA
                  AAAAAAACAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAQAAAAAAAAABAAAA
                  AAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAA
                  AAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
                  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKAAAAAAAAAAqAAAAAAAA
                  AC4AAAAAAAAAMgAAAAAAAAA2AAAAAAAAADYAAAAAAAAANgAAAAAAAAA3AAAAAAAAADcAAAAAAAAA
                  OAAAAAAAAAA6AAAAAAAAAD0AAAAAAAAAPQAAAAAAAAA+AAAAAAAAAD8AAAAAAAAAPwAAAAAAAABA
                  AAAAAAAAAEAAAAAAAAAAQgAAAAAAAABDAAAAAAAAAEUAAAAAAAAARwAAAAAAAABKAAAAAAAAAEsA
                  AAAAAAAASwAAAAAAAABLAAAAAAAAAE0AAAAAAAAATwAAAAAAAABQAAAAAAAAAFAAAAAAAAAAUAAA
                  AAAAAABQAAAAAAAAAFEAAAAAAAAAUgAAAAAAAABTAAAAAAAAAFcAAAAAAAAAWAAAAAAAAABaAAAA
                  AAAAAFsAAAAAAAAAXgAAAAAAAABeAAAAAAAAAF4AAAAAAAAAXgAAAAAAAABhAAAAAAAAAGQAAAAA
                  AAAAZQAAAAAAAABmAAAAAAAAAGkAAAAAAAAAaQAAAAAAAABtAAAAAAAAAG4AAAAAAAAAbwAAAAAA
                  AABwAAAAAAAAAHEAAAAAAAAAdgAAAAAAAAB2AAAAAAAAAHkAAAAAAAAAegAAAAAAAAB8AAAAAAAA
                  AHwAAAAAAAAAgAAAAAAAAACBAAAAAAAAAIQAAAAAAAAAhQAAAAAAAACHAAAAAAAAAJMAAAAAAAAA
                  mAAAAAAAAACaAAAAAAAAAJsAAAAAAAAAnAAAAAAAAACdAAAAAAAAAKQAAAAAAAAApwAAAAAAAACu
                  AAAAAAAAALAAAAAAAAAAsgAAAAAAAAC0AAAAAAAAALwAAAAAAAAAwwAAAAAAAADaAAAAAAAAAOIA
                  AAAAAAAAqFMAAAAAAACyUwAAAAAAALdTAAAAAAAAvFMAAAAAAADDUwAAAAAAAMdTAAAAAAAAy1MA
                  AAAAAADMUwAAAAAAANFTAAAAAAAA1VMAAAAAAADXUwAAAAAAAN1TAAAAAAAA31MAAAAAAADfUwAA
                  AAAAAOFTAAAAAAAA4VMAAAAAAADjUwAAAAAAAOZTAAAAAAAA51MAAAAAAADpUwAAAAAAAOpTAAAA
                  AAAA61MAAAAAAADtUwAAAAAAAO9TAAAAAAAA8FMAAAAAAADwUwAAAAAAAPFTAAAAAAAA8VMAAAAA
                  AADxUwAAAAAAAPJTAAAAAAAA81MAAAAAAADzUwAAAAAAAPRTAAAAAAAA9FMAAAAAAAD0UwAAAAAA
                  APVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPVTAAAAAAAA9VMAAAAAAAD1UwAAAAAAAPZTAAAAAAAA
                  9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2
                  UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZT
                  AAAAAAAA9lMAAAAAAAD2UwAAAAAAAPZTAAAAAAAA91MAAAAAAAD3UwAAAAAAAPdTAAAAAAAA91MA
                  AAAAAAD3UwAAAAAAAPdTAAAAAAAA91MAAAAAAAD3UwAAAAAAAPdTAAAAAAAA91MAAAAAAAD3UwAA
                  AAAAAPdTAAAAAAAA91MAAAAAAAD4UwAAAAAAAPhTAAAAAAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAA
                  AAAA+FMAAAAAAAD4UwAAAAAAAPhTAAAAAAAA
              - !!python/object/apply:builtins.slice
                - 7
                - 11
                - 1
              - 2
          - - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id005
              - data: !!python/object/apply:numpy._core.multiarray._reconstruct
                  args:
                  - *id002
                  - !!python/tuple
                    - 0
                  - !!binary |
                    Yg==
                  state: !!python/tuple
                  - 1
                  - !!python/tuple
                    - 11
                  - *id006
                  - false
                  - - threshold
                    - precision
                    - recall
                    - f1
                    - total_cost
                    - fpr
                    - fnr
                    - tp
                    - fp
                    - fn
                    - tn
                name: null
            - !!python/object/apply:pandas.core.indexes.base._new_Index
              - *id007
              - name: null
                start: 0
                step: 1
                stop: 81
          _typ: dataframe
          attrs: {}
      top_k_metrics:
        precision_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        precision_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAA8D8=
        threshold_at_100: !!python/object/apply:numpy._core.multiarray.scalar
        - &id008 !!python/object/apply:numpy.dtype
          args:
          - f4
          - false
          - true
          state: !!python/tuple
          - 3
          - <
          - null
          - null
          - null
          - -1
          - -1
          - 0
        - !!binary |
          b/N/Pw==
        threshold_at_1000: !!python/object/apply:numpy._core.multiarray.scalar
        - *id008
        - !!binary |
          tsh/Pw==
        threshold_at_500: !!python/object/apply:numpy._core.multiarray.scalar
        - *id008
        - !!binary |
          2+1/Pw==
  calibration_metrics:
    catboost:
      brier_score: 0.0026822949008168447
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            FsAHQw8qVj8WX/EVX/HFPwAAAAAAAOA/FDuxEzux4z9ddNFFF13kPxvKayivoew/AAAAAAAA8D+f
            9Emf9EnvP5yPwfkYnO8/X5LtOB/+7z8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            eqja9r+Mdz8iq0b8tAnBP4HwF0mSis8/zA23Kwnm1T88khdrDxvdP/SxS1ik3+E/JAIBxx1j5T+G
            lFN5tBPoP7TzGYY2des/Fx8Pzqih7z8=
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        Oxv4zdJwfj8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        nU92Njzaxj8=
    lightgbm:
      brier_score: 0.0032456201542030005
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            Sj4alsshLz8CjkX4x+meP0HRVoPnCMQ/juM4juM4zj9aWlpaWlraPxQ7sRM7seM/ep7neZ7n6T/s
            xE7sxE7sPwAAAAAAAPA/AAAAAAAA8D8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            bvaHPnzDjD/JvCctgFrDP9EXjfAp8s8/FyoS8+cI1j9BnEAQrFTcP0U8215cnuE/NDIBpS/J5D93
            pQmavfHnP0+i3hATd+s/AsnnqsnS7z8=
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        mXnqL2Rajj8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        9IsycHtotj8=
    logisticregression:
      brier_score: 0.012785495726670053
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            n9cZYD3EYT9BTK4gJleQP19CewntJbQ/HMdxHMdxvD9VVVVVVVXFP8xhDnOYw8w/xB1xR9wR1z/m
            T1towJLePwrXo3A9CuM/0JE3UU+b7z8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            m6fOYbD7gz9hGL+iUyrBP183pCOtXs4/p/QF06oe1j+9U5wiFGHcP9s7IWp4v+E/ftY1B3S85D+F
            /3PCHPDnP00ZmNjFVOs/WK1D8eDH7z8=
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        P2ThW9uQkj8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        W1PXkeHuyD8=
    randomforest:
      brier_score: 0.09094428832503428
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            u9DGwn5/Fz/o39aZmuN0P/kdiN+B+J0/e2yIjPbYsD94qqulwy17PzanNd5Yv6o/fN4NOG0Z3D+Y
            dGoe5K7vPwAAAAAAAPA/AAAAAAAA8D8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            AwR5OW7urD9lg7QD/dHBPwBR6VNJyM4/V6SEgRoV1z9VgRwJ7FDdP24EgmGOi+E/QuFCuLlk5D+u
            53EQskjoP56EK+hn6uo/KQIy7cik7T8=
      calibration_quality: POOR
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        9seqT/DJyD8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NiCBUomEzT8=
    xgboost:
      brier_score: 0.003168786209333383
      calibration_curve:
        fraction_of_positives: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            KXLy0luLXj+SG7mRG7nRP3d3d3d3d9c/mpmZmZmZ4T8AAAAAAADsPx4eHh4eHu4/DeU1lNdQ7j8A
            AAAAAADwP7D4iq/4iu8/AAAAAAAA8D8=
        mean_predicted_value: !!python/object/apply:numpy._core.multiarray._reconstruct
          args:
          - *id002
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 10
          - *id001
          - false
          - !!binary |
            OaNNTwhHcz9P7MR+ha3BP3d3dxf3es8/AAAAyAyO1T8AAACiVe7cP5eWltYGuuE/oryGsnME5T8j
            LPc0D+jnP7D4ir8onOs/uMy7OC7P7z8=
      calibration_quality: EXCELLENT
      ece: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        +jakM8DHdT8=
      mae_calibration: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        VEG+IF0QyT8=
  leakage_tests:
    adversarial:
      auc_adversarial: 0.99912689814852
      distribution_shift: true
      shift_risk: HIGH
    catboost_null_importance:
      auc_reference: 0.9998702887774135
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gD4am9GfeD8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          iIyproTktT8=
        BUSINESS_INCOHERENCE_CLASSIFICATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          IGyO6GaTlT8=
        BUSINESS_IS_GROUPES_ELECTROGENES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        BUSINESS_IS_MACHINES_ELECTRIQUES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ILhGPw2gmT8=
        BUSINESS_IS_TELEPHONES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIBoIF4f/T4=
        BUSINESS_RATIO_LIQUIDATION_CAF: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AICN8aKFBz8=
        BUSINESS_TAUX_DROITS_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABAkPqn3r4=
        BUSINESS_TAUX_DROITS_TRES_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACAam7upr4=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          APrvMjsOXz8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ALRbI5VvSD8=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACEoPpE8j4=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ALT2uKBCez8=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIAhbr/q8D4=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gLmHp+00cz8=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gGRObhoIcD8=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAC0BhBxyD4=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAgUaRbT4=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAXaemkT4=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gJA/LlV4dz8=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIAhBDov+j4=
      null_importance_risk: HIGH
      suspicious_features:
      - POIDS_NET_KG
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - TAUX_DROITS_PERCENT
      - BUSINESS_TAUX_DROITS_ELEVE
      - BUSINESS_TAUX_DROITS_TRES_ELEVE
      - BUSINESS_RATIO_LIQUIDATION_CAF
      - BUSINESS_IS_TELEPHONES
      - BUSINESS_DETOURNEMENT_REGIME
      - BUSINESS_VALEUR_ELEVEE
      - BUSINESS_IS_GROUPES_ELECTROGENES
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    catboost_permutation:
      auc_normal: 0.9997984323382867
      auc_permuted: 0.4990956614814
      leakage_detected: false
      leakage_risk: LOW
    lightgbm_null_importance:
      auc_reference: 0.9998894899977373
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gNplGsUrdj8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          6N8P6gD6tj8=
        BUSINESS_INCOHERENCE_CLASSIFICATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          oIrr9eUFnD8=
        BUSINESS_IS_GROUPES_ELECTROGENES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoLw=
        BUSINESS_IS_MACHINES_ELECTRIQUES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ANWjCDZxkz8=
        BUSINESS_IS_TELEPHONES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AADqxc08+j4=
        BUSINESS_RATIO_LIQUIDATION_CAF: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AO9S6NgJbz8=
        BUSINESS_TAUX_DROITS_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAcEdOSj4=
        BUSINESS_TAUX_DROITS_TRES_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AADgP8Pblj4=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAB/fILLWT8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AODa7nApNz8=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABcRr1zwT4=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AClv0Oz2fD8=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABxDx8YAz8=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          wKNg06SEiz8=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          APq+C1cPaD8=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAA4xj9Ruj4=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABAOzbKpz4=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AACgkrXT2D4=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAoLw=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          QP1lXp1oij8=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIAn3KNz9j4=
      null_importance_risk: HIGH
      suspicious_features:
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - BUSINESS_TAUX_DROITS_ELEVE
      - BUSINESS_TAUX_DROITS_TRES_ELEVE
      - BUSINESS_RATIO_LIQUIDATION_CAF
      - BUSINESS_IS_TELEPHONES
      - BUSINESS_DETOURNEMENT_REGIME
      - BUSINESS_VALEUR_ELEVEE
      - BUSINESS_IS_GROUPES_ELECTROGENES
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    lightgbm_permutation:
      auc_normal: 0.9997354696288807
      auc_permuted: 0.5037636618306612
      leakage_detected: false
      leakage_risk: LOW
    logisticregression_permutation:
      auc_normal: 0.9964901918371438
      auc_permuted: 0.5009151012019974
      leakage_detected: false
      leakage_risk: LOW
    randomforest_permutation:
      auc_normal: 0.9877973069904923
      auc_permuted: 0.48724806115923164
      leakage_detected: false
      leakage_risk: LOW
    xgboost_null_importance:
      auc_reference: 0.9996380658383262
      feature_importances:
        BUSINESS_DETOURNEMENT_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gG/szdzmdj8=
        BUSINESS_FAUSSE_DECLARATION_ESPECE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          qCIcDgTgsz8=
        BUSINESS_INCOHERENCE_CLASSIFICATION: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AObH59Ybhj8=
        BUSINESS_IS_GROUPES_ELECTROGENES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAA1Amz07D4=
        BUSINESS_IS_MACHINES_ELECTRIQUES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          QD3hSG31jT8=
        BUSINESS_IS_TELEPHONES: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AFDYIUidJD8=
        BUSINESS_RATIO_LIQUIDATION_CAF: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ADBJkSpwTz8=
        BUSINESS_TAUX_DROITS_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAtA43nL4=
        BUSINESS_TAUX_DROITS_TRES_ELEVE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        BUSINESS_VALEUR_ELEVEE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AA5pkvegVj8=
        CODE_PAYS_ORIGINE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AGAF0V0zPD8=
        CODE_PAYS_PROVENANCE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAB8J7rp6j4=
        CODE_SH_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          APYPQwkqez8=
        NOMBRE_COLIS: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AED7V7h3Iz8=
        POIDS_NET_KG: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          gHI8ggC/fz8=
        QUANTITE_COMPLEMENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          ABoWstxkZD8=
        REGIME_COMPLET: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AABJxyT4+z4=
        REGIME_DOUANIER: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAALVeanD4=
        REGIME_FISCAL: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAkdqld5D4=
        STATUT_BAE: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AAAAAAAAAAA=
        TAUX_DROITS_PERCENT: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          QDHZ5iHPjD8=
        TYPE_REGIME: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          AIDgZyZ3Dz8=
      null_importance_risk: HIGH
      suspicious_features:
      - POIDS_NET_KG
      - NOMBRE_COLIS
      - QUANTITE_COMPLEMENT
      - BUSINESS_TAUX_DROITS_ELEVE
      - BUSINESS_TAUX_DROITS_TRES_ELEVE
      - BUSINESS_RATIO_LIQUIDATION_CAF
      - BUSINESS_IS_TELEPHONES
      - BUSINESS_DETOURNEMENT_REGIME
      - BUSINESS_VALEUR_ELEVEE
      - BUSINESS_IS_GROUPES_ELECTROGENES
      - CODE_SH_COMPLET
      - CODE_PAYS_ORIGINE
      - CODE_PAYS_PROVENANCE
      - REGIME_COMPLET
      - STATUT_BAE
      - TYPE_REGIME
      - REGIME_DOUANIER
      - REGIME_FISCAL
    xgboost_permutation:
      auc_normal: 0.9992922524135721
      auc_permuted: 0.5039006331613812
      leakage_detected: false
      leakage_risk: LOW
  monitoring_config:
    alert_channels:
    - email
    - dashboard
    alert_thresholds:
      distribution_shift_threshold: 0.05
      ks_threshold: 0.05
      missing_rate_threshold: 0.1
      psi_threshold: 0.2
    features_to_monitor: []
    monitoring_frequency: daily
    reference_stats: {}
  subgroup_analysis:
    subgroup_results:
      CODE_SH_COMPLET:
        8501100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F61D8=
            n_samples: 450
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F61D8=
            n_samples: 450
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F61D8=
            n_samples: 450
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F61D8=
            n_samples: 450
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F61D8=
            n_samples: 450
            precision: 1.0
            recall: 1.0
        8501200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRB4j8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRB4j8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRB4j8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRB4j8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GqRBGqRB4j8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
        8501310000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IQ3SIA3S4D8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IQ3SIA3S4D8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IQ3SIA3S4D8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IQ3SIA3S4D8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IQ3SIA3S4D8=
            n_samples: 156
            precision: 1.0
            recall: 1.0
        8501320000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              a8/J9FXC2D8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              a8/J9FXC2D8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              a8/J9FXC2D8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              a8/J9FXC2D8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              a8/J9FXC2D8=
            n_samples: 137
            precision: 1.0
            recall: 1.0
        8501400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVV1T8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVV1T8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVV1T8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVV1T8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVV1T8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
        8501510000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5brjxZbg2z8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5brjxZbg2z8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5brjxZbg2z8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5brjxZbg2z8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5brjxZbg2z8=
            n_samples: 326
            precision: 1.0
            recall: 1.0
        8501520000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ovdMovdM4j8=
            n_samples: 918
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ovdMovdM4j8=
            n_samples: 918
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ovdMovdM4j8=
            n_samples: 918
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9980988593155894
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ovdMovdM4j8=
            n_samples: 918
            precision: 0.9962049335863378
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ovdMovdM4j8=
            n_samples: 918
            precision: 1.0
            recall: 1.0
        8501530000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SOUGOKcO5z8=
            n_samples: 297
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SOUGOKcO5z8=
            n_samples: 297
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SOUGOKcO5z8=
            n_samples: 297
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9976689976689976
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SOUGOKcO5z8=
            n_samples: 297
            precision: 0.9953488372093023
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              SOUGOKcO5z8=
            n_samples: 297
            precision: 1.0
            recall: 1.0
        8501610000:
          catboost:
            auc: 1.0
            f1: 0.9961685823754789
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              czQuR+Ny5D8=
            n_samples: 205
            precision: 1.0
            recall: 0.9923664122137404
          lightgbm:
            auc: 1.0
            f1: 0.9961685823754789
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              czQuR+Ny5D8=
            n_samples: 205
            precision: 1.0
            recall: 0.9923664122137404
          logisticregression:
            auc: 0.9997936868165876
            f1: 0.9961685823754789
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              czQuR+Ny5D8=
            n_samples: 205
            precision: 1.0
            recall: 0.9923664122137404
          randomforest:
            auc: 0.9997936868165875
            f1: 0.9961685823754789
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              czQuR+Ny5D8=
            n_samples: 205
            precision: 1.0
            recall: 0.9923664122137404
          xgboost:
            auc: 1.0
            f1: 0.9961685823754789
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              czQuR+Ny5D8=
            n_samples: 205
            precision: 1.0
            recall: 0.9923664122137404
        8501620000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5DiO4ziO4z8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5DiO4ziO4z8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5DiO4ziO4z8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5DiO4ziO4z8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              5DiO4ziO4z8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
        8502119000:
          catboost:
            auc: 0.9999968322753133
            f1: 0.9988726042841037
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Y+q4Q2fF4T8=
            n_samples: 1599
            precision: 1.0
            recall: 0.9977477477477478
          lightgbm:
            auc: 0.99999524841297
            f1: 0.9983098591549295
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Y+q4Q2fF4T8=
            n_samples: 1599
            precision: 0.9988726042841037
            recall: 0.9977477477477478
          logisticregression:
            auc: 0.9998922973606519
            f1: 0.9960652051714446
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Y+q4Q2fF4T8=
            n_samples: 1599
            precision: 0.9943883277216611
            recall: 0.9977477477477478
          randomforest:
            auc: 0.9999413970932959
            f1: 0.9949466591802358
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Y+q4Q2fF4T8=
            n_samples: 1599
            precision: 0.9921612541993281
            recall: 0.9977477477477478
          xgboost:
            auc: 0.9999920806882832
            f1: 0.9988726042841037
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Y+q4Q2fF4T8=
            n_samples: 1599
            precision: 1.0
            recall: 0.9977477477477478
        8502129000:
          catboost:
            auc: 1.0
            f1: 0.9990574929311969
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JRAXak4C4T8=
            n_samples: 999
            precision: 1.0
            recall: 0.9981167608286252
          lightgbm:
            auc: 0.9999959759846765
            f1: 0.9990574929311969
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JRAXak4C4T8=
            n_samples: 999
            precision: 1.0
            recall: 0.9981167608286252
          logisticregression:
            auc: 0.9995493102837736
            f1: 0.9981167608286252
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JRAXak4C4T8=
            n_samples: 999
            precision: 0.9981167608286252
            recall: 0.9981167608286252
          randomforest:
            auc: 0.9999517118161185
            f1: 0.9953051643192489
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JRAXak4C4T8=
            n_samples: 999
            precision: 0.9925093632958801
            recall: 0.9981167608286252
          xgboost:
            auc: 0.9999839039387062
            f1: 0.9990574929311969
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JRAXak4C4T8=
            n_samples: 999
            precision: 1.0
            recall: 0.9981167608286252
        8502139000:
          catboost:
            auc: 1.0
            f1: 0.9989806320081549
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xhuuOULa5T8=
            n_samples: 719
            precision: 1.0
            recall: 0.9979633401221996
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xhuuOULa5T8=
            n_samples: 719
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9998749419373281
            f1: 0.9979633401221996
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xhuuOULa5T8=
            n_samples: 719
            precision: 0.9979633401221996
            recall: 0.9979633401221996
          randomforest:
            auc: 0.999937470968664
            f1: 0.9929078014184397
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xhuuOULa5T8=
            n_samples: 719
            precision: 0.9879032258064516
            recall: 0.9979633401221996
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xhuuOULa5T8=
            n_samples: 719
            precision: 1.0
            recall: 1.0
        8502209000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JO6IO+KO6D8=
            n_samples: 688
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JO6IO+KO6D8=
            n_samples: 688
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JO6IO+KO6D8=
            n_samples: 688
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9990539262062441
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JO6IO+KO6D8=
            n_samples: 688
            precision: 0.998109640831758
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JO6IO+KO6D8=
            n_samples: 688
            precision: 1.0
            recall: 1.0
        8502399000:
          catboost:
            auc: 1.0
            f1: 0.994535519125683
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wpDKsU7C4D8=
            n_samples: 527
            precision: 1.0
            recall: 0.9891304347826086
          lightgbm:
            auc: 1.0
            f1: 0.994535519125683
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wpDKsU7C4D8=
            n_samples: 527
            precision: 1.0
            recall: 0.9891304347826086
          logisticregression:
            auc: 0.996362376580634
            f1: 0.9963636363636363
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wpDKsU7C4D8=
            n_samples: 527
            precision: 1.0
            recall: 0.9927536231884058
          randomforest:
            auc: 0.9997546047693284
            f1: 0.9927272727272727
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wpDKsU7C4D8=
            n_samples: 527
            precision: 0.9963503649635036
            recall: 0.9891304347826086
          xgboost:
            auc: 0.9999999999999999
            f1: 0.994535519125683
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wpDKsU7C4D8=
            n_samples: 527
            precision: 1.0
            recall: 0.9891304347826086
        8503000090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAxj8=
            n_samples: 448
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAxj8=
            n_samples: 448
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAxj8=
            n_samples: 448
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8868274582560297
            f1: 0.4631578947368421
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAxj8=
            n_samples: 448
            precision: 0.3173076923076923
            recall: 0.8571428571428571
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAxj8=
            n_samples: 448
            precision: 1.0
            recall: 1.0
        8504100000:
          catboost:
            auc: 1.0
            f1: 0.9873417721518988
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              x9TA3jE10D8=
            n_samples: 154
            precision: 0.975
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              x9TA3jE10D8=
            n_samples: 154
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9587513935340023
            f1: 0.72
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              x9TA3jE10D8=
            n_samples: 154
            precision: 0.5901639344262295
            recall: 0.9230769230769231
          randomforest:
            auc: 0.9839464882943143
            f1: 0.8478260869565217
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              x9TA3jE10D8=
            n_samples: 154
            precision: 0.7358490566037735
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9873417721518988
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              x9TA3jE10D8=
            n_samples: 154
            precision: 0.975
            recall: 1.0
        8504210000:
          catboost:
            auc: 0.9999508067689885
            f1: 0.9961977186311787
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ntiJndiJ3T8=
            n_samples: 286
            precision: 1.0
            recall: 0.9924242424242424
          lightgbm:
            auc: 1.0
            f1: 0.9924812030075187
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ntiJndiJ3T8=
            n_samples: 286
            precision: 0.9850746268656716
            recall: 1.0
          logisticregression:
            auc: 0.9602518693427784
            f1: 0.8770764119601329
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ntiJndiJ3T8=
            n_samples: 286
            precision: 0.7810650887573964
            recall: 1.0
          randomforest:
            auc: 0.9951298701298701
            f1: 0.9496402877697842
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ntiJndiJ3T8=
            n_samples: 286
            precision: 0.9041095890410958
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ntiJndiJ3T8=
            n_samples: 286
            precision: 1.0
            recall: 1.0
        8504220000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YuXNvx1W3j8=
            n_samples: 327
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YuXNvx1W3j8=
            n_samples: 327
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9863465866466616
            f1: 0.950920245398773
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YuXNvx1W3j8=
            n_samples: 327
            precision: 0.9064327485380117
            recall: 1.0
          randomforest:
            auc: 0.9998499624906226
            f1: 0.9657320872274143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YuXNvx1W3j8=
            n_samples: 327
            precision: 0.9337349397590361
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YuXNvx1W3j8=
            n_samples: 327
            precision: 1.0
            recall: 1.0
        8504230000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              juM4juM45D8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              juM4juM45D8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9545925772340866
            f1: 0.882051282051282
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              juM4juM45D8=
            n_samples: 144
            precision: 0.8269230769230769
            recall: 0.945054945054945
          randomforest:
            auc: 0.9848641924113622
            f1: 0.9456521739130435
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              juM4juM45D8=
            n_samples: 144
            precision: 0.9354838709677419
            recall: 0.9560439560439561
          xgboost:
            auc: 1.0
            f1: 0.994475138121547
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              juM4juM45D8=
            n_samples: 144
            precision: 1.0
            recall: 0.989010989010989
        8504310000:
          catboost:
            auc: 0.9999502652519894
            f1: 0.9965635738831615
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              i9DnRLmK0D8=
            n_samples: 561
            precision: 0.9931506849315068
            recall: 1.0
          lightgbm:
            auc: 0.9999834217506631
            f1: 0.9965635738831615
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              i9DnRLmK0D8=
            n_samples: 561
            precision: 0.9931506849315068
            recall: 1.0
          logisticregression:
            auc: 0.920291777188329
            f1: 0.7286821705426356
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              i9DnRLmK0D8=
            n_samples: 561
            precision: 0.5826446280991735
            recall: 0.9724137931034482
          randomforest:
            auc: 0.9790948275862068
            f1: 0.8173913043478261
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              i9DnRLmK0D8=
            n_samples: 561
            precision: 0.705
            recall: 0.9724137931034482
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              i9DnRLmK0D8=
            n_samples: 561
            precision: 1.0
            recall: 1.0
        8504320000:
          catboost:
            auc: 1.0
            f1: 0.98
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              q6qqqqqq1j8=
            n_samples: 144
            precision: 1.0
            recall: 0.9607843137254902
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              q6qqqqqq1j8=
            n_samples: 144
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9386464263124604
            f1: 0.8429752066115702
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              q6qqqqqq1j8=
            n_samples: 144
            precision: 0.7285714285714285
            recall: 1.0
          randomforest:
            auc: 0.9789162977018764
            f1: 0.8909090909090909
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              q6qqqqqq1j8=
            n_samples: 144
            precision: 0.8305084745762712
            recall: 0.9607843137254902
          xgboost:
            auc: 1.0
            f1: 0.98
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              q6qqqqqq1j8=
            n_samples: 144
            precision: 1.0
            recall: 0.9607843137254902
        8504330000:
          catboost:
            auc: 1.0
            f1: 0.9974025974025974
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bhCcyoRt4D8=
            n_samples: 374
            precision: 0.9948186528497409
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bhCcyoRt4D8=
            n_samples: 374
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9453697344322345
            f1: 0.8909512761020881
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bhCcyoRt4D8=
            n_samples: 374
            precision: 0.803347280334728
            recall: 1.0
          randomforest:
            auc: 0.9986263736263736
            f1: 0.9164677804295943
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bhCcyoRt4D8=
            n_samples: 374
            precision: 0.8458149779735683
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9974025974025974
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bhCcyoRt4D8=
            n_samples: 374
            precision: 0.9948186528497409
            recall: 1.0
        8504340000:
          catboost:
            auc: 1.0
            f1: 0.991869918699187
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              zP58OA2h4j8=
            n_samples: 213
            precision: 1.0
            recall: 0.9838709677419355
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              zP58OA2h4j8=
            n_samples: 213
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9896701703515767
            f1: 0.9461538461538461
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              zP58OA2h4j8=
            n_samples: 213
            precision: 0.9044117647058824
            recall: 0.9919354838709677
          randomforest:
            auc: 0.9982783617252629
            f1: 0.9721115537848606
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              zP58OA2h4j8=
            n_samples: 213
            precision: 0.9606299212598425
            recall: 0.9838709677419355
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              zP58OA2h4j8=
            n_samples: 213
            precision: 1.0
            recall: 1.0
        8504401010:
          catboost:
            auc: 1.0
            f1: 0.9990592662276576
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9iCSwx5E4j8=
            n_samples: 932
            precision: 1.0
            recall: 0.9981203007518797
          lightgbm:
            auc: 0.9999812030075188
            f1: 0.9962546816479401
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9iCSwx5E4j8=
            n_samples: 932
            precision: 0.9925373134328358
            recall: 1.0
          logisticregression:
            auc: 0.9928947368421053
            f1: 0.9628286491387126
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9iCSwx5E4j8=
            n_samples: 932
            precision: 0.9299474605954466
            recall: 0.9981203007518797
          randomforest:
            auc: 0.9996099624060151
            f1: 0.9842154131847726
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9iCSwx5E4j8=
            n_samples: 932
            precision: 0.9724770642201835
            recall: 0.9962406015037594
          xgboost:
            auc: 0.9999953007518797
            f1: 0.9990610328638497
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9iCSwx5E4j8=
            n_samples: 932
            precision: 0.99812382739212
            recall: 1.0
        8504401090:
          catboost:
            auc: 0.9993604093380237
            f1: 0.9975186104218362
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lfxkN8+x3D8=
            n_samples: 2246
            precision: 0.9970238095238095
            recall: 0.9980139026812314
          lightgbm:
            auc: 0.9992958892273858
            f1: 0.9995032290114257
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lfxkN8+x3D8=
            n_samples: 2246
            precision: 1.0
            recall: 0.9990069513406157
          logisticregression:
            auc: 0.9918945108213448
            f1: 0.9507109004739337
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lfxkN8+x3D8=
            n_samples: 2246
            precision: 0.9093381686310064
            recall: 0.9960278053624627
          randomforest:
            auc: 0.9987368485171997
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lfxkN8+x3D8=
            n_samples: 2246
            precision: 0.933768656716418
            recall: 0.9940417080436942
          xgboost:
            auc: 0.9992321706088052
            f1: 0.9995032290114257
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lfxkN8+x3D8=
            n_samples: 2246
            precision: 1.0
            recall: 0.9990069513406157
        8504402000:
          catboost:
            auc: 0.9998865308384999
            f1: 0.9907773386034255
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              btu2bds2yz8=
            n_samples: 1792
            precision: 0.9947089947089947
            recall: 0.9868766404199475
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              btu2bds2yz8=
            n_samples: 1792
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9835804542858789
            f1: 0.8584905660377359
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              btu2bds2yz8=
            n_samples: 1792
            precision: 0.7794432548179872
            recall: 0.9553805774278216
          randomforest:
            auc: 0.9942595765182081
            f1: 0.8711217183770883
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              btu2bds2yz8=
            n_samples: 1792
            precision: 0.7986870897155361
            recall: 0.958005249343832
          xgboost:
            auc: 0.9999758180475491
            f1: 0.9934296977660972
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              btu2bds2yz8=
            n_samples: 1792
            precision: 0.9947368421052631
            recall: 0.9921259842519685
        8504409010:
          catboost:
            auc: 0.9999511547028253
            f1: 0.9984051036682615
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm3z8=
            n_samples: 640
            precision: 1.0
            recall: 0.9968152866242038
          lightgbm:
            auc: 1.0
            f1: 0.9984101748807631
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm3z8=
            n_samples: 640
            precision: 0.9968253968253968
            recall: 1.0
          logisticregression:
            auc: 0.987563987339299
            f1: 0.9452887537993921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm3z8=
            n_samples: 640
            precision: 0.9040697674418605
            recall: 0.9904458598726115
          randomforest:
            auc: 0.9990328631159392
            f1: 0.963076923076923
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm3z8=
            n_samples: 640
            precision: 0.9315476190476191
            recall: 0.9968152866242038
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm3z8=
            n_samples: 640
            precision: 1.0
            recall: 1.0
        8504409090:
          catboost:
            auc: 0.9999195571321946
            f1: 0.9943799175721244
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3ciN3MiN1D8=
            n_samples: 4160
            precision: 0.9954988747186797
            recall: 0.9932634730538922
          lightgbm:
            auc: 0.9999843620125187
            f1: 0.997003745318352
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3ciN3MiN1D8=
            n_samples: 4160
            precision: 0.9977511244377811
            recall: 0.9962574850299402
          logisticregression:
            auc: 0.9709271259181355
            f1: 0.8268928683631794
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3ciN3MiN1D8=
            n_samples: 4160
            precision: 0.7125067677314564
            recall: 0.9850299401197605
          randomforest:
            auc: 0.9927702138216484
            f1: 0.8706666666666667
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3ciN3MiN1D8=
            n_samples: 4160
            precision: 0.7848557692307693
            recall: 0.9775449101796407
          xgboost:
            auc: 0.9999793260504487
            f1: 0.9962490622655664
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3ciN3MiN1D8=
            n_samples: 4160
            precision: 0.9984962406015038
            recall: 0.9940119760479041
        8504509000:
          catboost:
            auc: 0.9999999999999999
            f1: 0.9969604863221885
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JvvqpMoY1j8=
            n_samples: 475
            precision: 0.9939393939393939
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9969604863221885
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JvvqpMoY1j8=
            n_samples: 475
            precision: 0.9939393939393939
            recall: 1.0
          logisticregression:
            auc: 0.9566112461767704
            f1: 0.8256410256410256
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JvvqpMoY1j8=
            n_samples: 475
            precision: 0.7123893805309734
            recall: 0.9817073170731707
          randomforest:
            auc: 0.9938632264136146
            f1: 0.907563025210084
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JvvqpMoY1j8=
            n_samples: 475
            precision: 0.8393782383419689
            recall: 0.9878048780487805
          xgboost:
            auc: 1.0
            f1: 0.9969604863221885
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              JvvqpMoY1j8=
            n_samples: 475
            precision: 0.9939393939393939
            recall: 1.0
        8504900000:
          catboost:
            auc: 0.9999438314125377
            f1: 0.9963898916967509
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BJpCoCkE0j8=
            n_samples: 984
            precision: 0.9963898916967509
            recall: 0.9963898916967509
          lightgbm:
            auc: 1.0
            f1: 0.9963768115942029
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BJpCoCkE0j8=
            n_samples: 984
            precision: 1.0
            recall: 0.9927797833935018
          logisticregression:
            auc: 0.9687038843131347
            f1: 0.8125
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BJpCoCkE0j8=
            n_samples: 984
            precision: 0.6911392405063291
            recall: 0.9855595667870036
          randomforest:
            auc: 0.9907321830687453
            f1: 0.853125
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BJpCoCkE0j8=
            n_samples: 984
            precision: 0.7520661157024794
            recall: 0.9855595667870036
          xgboost:
            auc: 0.9999744688238807
            f1: 0.9963898916967509
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BJpCoCkE0j8=
            n_samples: 984
            precision: 0.9963898916967509
            recall: 0.9963898916967509
        8505209000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 202
            precision: 0.0
            recall: 0.0
        8505900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wFoBawWslT8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wFoBawWslT8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wFoBawWslT8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9581081081081081
            f1: 0.16326530612244897
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wFoBawWslT8=
            n_samples: 189
            precision: 0.08888888888888889
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wFoBawWslT8=
            n_samples: 189
            precision: 1.0
            recall: 1.0
        8506109000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
        8506500000:
          catboost:
            auc: 1.0
            f1: 0.8979591836734694
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4u2qe4dEtT8=
            n_samples: 325
            precision: 1.0
            recall: 0.8148148148148148
          lightgbm:
            auc: 1.0
            f1: 0.8979591836734694
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4u2qe4dEtT8=
            n_samples: 325
            precision: 1.0
            recall: 0.8148148148148148
          logisticregression:
            auc: 1.0
            f1: 0.9818181818181818
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4u2qe4dEtT8=
            n_samples: 325
            precision: 0.9642857142857143
            recall: 1.0
          randomforest:
            auc: 0.9993785732040765
            f1: 0.3724137931034483
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4u2qe4dEtT8=
            n_samples: 325
            precision: 0.2288135593220339
            recall: 1.0
          xgboost:
            auc: 0.9995028585632614
            f1: 0.8979591836734694
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              4u2qe4dEtT8=
            n_samples: 325
            precision: 1.0
            recall: 0.8148148148148148
        8506800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NyYNIG5Maj8=
            n_samples: 623
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NyYNIG5Maj8=
            n_samples: 623
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.998389694041868
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NyYNIG5Maj8=
            n_samples: 623
            precision: 0.6666666666666666
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.032
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NyYNIG5Maj8=
            n_samples: 623
            precision: 0.016260162601626018
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NyYNIG5Maj8=
            n_samples: 623
            precision: 1.0
            recall: 1.0
        8506900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAA4D8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAA4D8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAA4D8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.8676470588235294
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAA4D8=
            n_samples: 118
            precision: 0.7662337662337663
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAA4D8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
        8507100000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 309
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 309
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 309
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 309
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 309
            precision: 0.0
            recall: 0.0
        8507100010:
          catboost:
            auc: 0.9967637540453075
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IjXBeCv7nD8=
            n_samples: 106
            precision: 1.0
            recall: 0.6666666666666666
          lightgbm:
            auc: 1.0
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IjXBeCv7nD8=
            n_samples: 106
            precision: 1.0
            recall: 0.6666666666666666
          logisticregression:
            auc: 0.8284789644012945
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IjXBeCv7nD8=
            n_samples: 106
            precision: 1.0
            recall: 0.6666666666666666
          randomforest:
            auc: 0.8252427184466019
            f1: 0.07407407407407407
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IjXBeCv7nD8=
            n_samples: 106
            precision: 0.0392156862745098
            recall: 0.6666666666666666
          xgboost:
            auc: 1.0
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IjXBeCv7nD8=
            n_samples: 106
            precision: 1.0
            recall: 0.6666666666666666
        8507100090:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /8CODS+FeT8=
            n_samples: 1284
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /8CODS+FeT8=
            n_samples: 1284
            precision: 0.8888888888888888
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /8CODS+FeT8=
            n_samples: 1284
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.046242774566473986
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /8CODS+FeT8=
            n_samples: 1284
            precision: 0.023668639053254437
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /8CODS+FeT8=
            n_samples: 1284
            precision: 1.0
            recall: 1.0
        8507200010:
          catboost:
            auc: 1.0
            f1: 0.9642857142857143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rw42ceOTvT8=
            n_samples: 502
            precision: 1.0
            recall: 0.9310344827586207
          lightgbm:
            auc: 0.9997281764523144
            f1: 0.9557522123893806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rw42ceOTvT8=
            n_samples: 502
            precision: 0.9818181818181818
            recall: 0.9310344827586207
          logisticregression:
            auc: 0.9432665424044735
            f1: 0.7692307692307693
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rw42ceOTvT8=
            n_samples: 502
            precision: 0.8695652173913043
            recall: 0.6896551724137931
          randomforest:
            auc: 0.9202392047219634
            f1: 0.26146788990825687
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rw42ceOTvT8=
            n_samples: 502
            precision: 0.15079365079365079
            recall: 0.9827586206896551
          xgboost:
            auc: 0.9999611680646163
            f1: 0.7789473684210526
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Rw42ceOTvT8=
            n_samples: 502
            precision: 1.0
            recall: 0.6379310344827587
        8507200090:
          catboost:
            auc: 0.9997262598888615
            f1: 0.9210526315789473
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WDC0/AqGpj8=
            n_samples: 932
            precision: 1.0
            recall: 0.8536585365853658
          lightgbm:
            auc: 0.999835755933317
            f1: 0.95
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WDC0/AqGpj8=
            n_samples: 932
            precision: 0.9743589743589743
            recall: 0.926829268292683
          logisticregression:
            auc: 0.9473324026169555
            f1: 0.9367088607594937
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WDC0/AqGpj8=
            n_samples: 932
            precision: 0.9736842105263158
            recall: 0.9024390243902439
          randomforest:
            auc: 0.9648791437409324
            f1: 0.18009478672985782
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WDC0/AqGpj8=
            n_samples: 932
            precision: 0.09973753280839895
            recall: 0.926829268292683
          xgboost:
            auc: 0.9999452519777723
            f1: 0.9210526315789473
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WDC0/AqGpj8=
            n_samples: 932
            precision: 1.0
            recall: 0.8536585365853658
        8507600010:
          catboost:
            auc: 0.9961656441717792
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0pz8=
            n_samples: 171
            precision: 1.0
            recall: 0.75
          lightgbm:
            auc: 0.9992331288343559
            f1: 0.9333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0pz8=
            n_samples: 171
            precision: 1.0
            recall: 0.875
          logisticregression:
            auc: 0.9248466257668712
            f1: 0.9333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0pz8=
            n_samples: 171
            precision: 1.0
            recall: 0.875
          randomforest:
            auc: 0.74079754601227
            f1: 0.1206896551724138
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0pz8=
            n_samples: 171
            precision: 0.06481481481481481
            recall: 0.875
          xgboost:
            auc: 0.9992331288343558
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              QH8B/QX0pz8=
            n_samples: 171
            precision: 1.0
            recall: 0.75
        8507600090:
          catboost:
            auc: 0.9987498263647729
            f1: 0.921875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mrMGxdMexz8=
            n_samples: 382
            precision: 1.0
            recall: 0.855072463768116
          lightgbm:
            auc: 0.9984257072741585
            f1: 0.921875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mrMGxdMexz8=
            n_samples: 382
            precision: 1.0
            recall: 0.855072463768116
          logisticregression:
            auc: 0.9564291336759735
            f1: 0.921875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mrMGxdMexz8=
            n_samples: 382
            precision: 1.0
            recall: 0.855072463768116
          randomforest:
            auc: 0.9361022364217252
            f1: 0.436241610738255
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mrMGxdMexz8=
            n_samples: 382
            precision: 0.2838427947598253
            recall: 0.9420289855072463
          xgboost:
            auc: 0.9984488586377738
            f1: 0.921875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mrMGxdMexz8=
            n_samples: 382
            precision: 1.0
            recall: 0.855072463768116
        8507800000:
          catboost:
            auc: 0.9998078078078079
            f1: 0.9451476793248945
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              s/NxjWElrj8=
            n_samples: 2123
            precision: 1.0
            recall: 0.896
          lightgbm:
            auc: 0.9995375375375375
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              s/NxjWElrj8=
            n_samples: 2123
            precision: 0.9448818897637795
            recall: 0.96
          logisticregression:
            auc: 0.9519079079079078
            f1: 0.7867647058823529
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              s/NxjWElrj8=
            n_samples: 2123
            precision: 0.7278911564625851
            recall: 0.856
          randomforest:
            auc: 0.9139219219219219
            f1: 0.18945634266886327
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              s/NxjWElrj8=
            n_samples: 2123
            precision: 0.10560146923783287
            recall: 0.92
          xgboost:
            auc: 0.9996776776776777
            f1: 0.9035087719298246
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              s/NxjWElrj8=
            n_samples: 2123
            precision: 1.0
            recall: 0.824
        8507900000:
          catboost:
            auc: 0.9996505939902166
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OMa3Je+qxT8=
            n_samples: 319
            precision: 0.9629629629629629
            recall: 0.9629629629629629
          lightgbm:
            auc: 0.9992313067784766
            f1: 0.9908256880733946
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OMa3Je+qxT8=
            n_samples: 319
            precision: 0.9818181818181818
            recall: 1.0
          logisticregression:
            auc: 0.911320754716981
            f1: 0.5810055865921788
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OMa3Je+qxT8=
            n_samples: 319
            precision: 0.416
            recall: 0.9629629629629629
          randomforest:
            auc: 0.8874912648497554
            f1: 0.48826291079812206
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OMa3Je+qxT8=
            n_samples: 319
            precision: 0.3270440251572327
            recall: 0.9629629629629629
          xgboost:
            auc: 0.9996505939902166
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OMa3Je+qxT8=
            n_samples: 319
            precision: 0.9629629629629629
            recall: 0.9629629629629629
        8508190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EhiBERiBgT8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EhiBERiBgT8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EhiBERiBgT8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.06451612903225806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EhiBERiBgT8=
            n_samples: 117
            precision: 0.03333333333333333
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EhiBERiBgT8=
            n_samples: 117
            precision: 1.0
            recall: 1.0
        8508600000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 234
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 234
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 234
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 234
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 234
            precision: 0.0
            recall: 0.0
        8509400000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 583
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 583
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 583
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 583
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 583
            precision: 0.0
            recall: 0.0
        8509800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UpLndQ3uZz8=
            n_samples: 1027
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UpLndQ3uZz8=
            n_samples: 1027
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UpLndQ3uZz8=
            n_samples: 1027
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.009302325581395349
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UpLndQ3uZz8=
            n_samples: 1027
            precision: 0.004672897196261682
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UpLndQ3uZz8=
            n_samples: 1027
            precision: 1.0
            recall: 1.0
        8511100000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 191
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 191
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 191
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 191
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 191
            precision: 0.0
            recall: 0.0
        8511400000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tD7jR0zBjD8=
            n_samples: 641
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tD7jR0zBjD8=
            n_samples: 641
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tD7jR0zBjD8=
            n_samples: 641
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9992967651195499
            f1: 0.1276595744680851
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tD7jR0zBjD8=
            n_samples: 641
            precision: 0.06818181818181818
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tD7jR0zBjD8=
            n_samples: 641
            precision: 1.0
            recall: 1.0
        8511500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9AV9QV/Qdz8=
            n_samples: 344
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9AV9QV/Qdz8=
            n_samples: 344
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9AV9QV/Qdz8=
            n_samples: 344
            precision: 0.5
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.04
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9AV9QV/Qdz8=
            n_samples: 344
            precision: 0.02040816326530612
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9AV9QV/Qdz8=
            n_samples: 344
            precision: 1.0
            recall: 1.0
        8511800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.07142857142857142
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 0.037037037037037035
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
        8512200000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 1270
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 1270
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 1270
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 1270
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 1270
            precision: 0.0
            recall: 0.0
        8512300000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 110
            precision: 0.0
            recall: 0.0
        8512400000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 141
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 141
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 141
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 141
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 141
            precision: 0.0
            recall: 0.0
        8512900000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 145
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 145
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 145
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 145
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 145
            precision: 0.0
            recall: 0.0
        8513100010:
          catboost:
            auc: 1.0
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LsUKh4kylz8=
            n_samples: 309
            precision: 1.0
            recall: 0.8571428571428571
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LsUKh4kylz8=
            n_samples: 309
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9044465468306527
            f1: 0.8333333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LsUKh4kylz8=
            n_samples: 309
            precision: 1.0
            recall: 0.7142857142857143
          randomforest:
            auc: 0.8088930936613056
            f1: 0.047619047619047616
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LsUKh4kylz8=
            n_samples: 309
            precision: 0.024630541871921183
            recall: 0.7142857142857143
          xgboost:
            auc: 1.0
            f1: 0.8333333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LsUKh4kylz8=
            n_samples: 309
            precision: 1.0
            recall: 0.7142857142857143
        8513100090:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 2071
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 2071
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 2071
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 2071
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 2071
            precision: 0.0
            recall: 0.0
        8513900000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 276
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 276
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 276
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 276
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 276
            precision: 0.0
            recall: 0.0
        8514900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCunT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCunT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCunT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.1111111111111111
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCunT8=
            n_samples: 138
            precision: 0.058823529411764705
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCunT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
        8515210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bkT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bkT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bkT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.21052631578947367
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bkT8=
            n_samples: 118
            precision: 0.11764705882352941
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DSd1Xx5bkT8=
            n_samples: 118
            precision: 1.0
            recall: 1.0
        8515290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O80KDl9IcT8=
            n_samples: 237
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O80KDl9IcT8=
            n_samples: 237
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O80KDl9IcT8=
            n_samples: 237
            precision: 0.5
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.027777777777777776
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O80KDl9IcT8=
            n_samples: 237
            precision: 0.014084507042253521
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              O80KDl9IcT8=
            n_samples: 237
            precision: 1.0
            recall: 1.0
        8515310000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGhD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGhD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGhD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.04878048780487805
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGhD8=
            n_samples: 101
            precision: 0.025
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              +9liZfhGhD8=
            n_samples: 101
            precision: 1.0
            recall: 1.0
        8515390000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ez6IZf3Eiz8=
            n_samples: 295
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ez6IZf3Eiz8=
            n_samples: 295
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ez6IZf3Eiz8=
            n_samples: 295
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.997422680412371
            f1: 0.06557377049180328
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ez6IZf3Eiz8=
            n_samples: 295
            precision: 0.03389830508474576
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ez6IZf3Eiz8=
            n_samples: 295
            precision: 1.0
            recall: 1.0
        8515800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YbmnEZZ7ij8=
            n_samples: 232
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YbmnEZZ7ij8=
            n_samples: 232
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YbmnEZZ7ij8=
            n_samples: 232
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.0821917808219178
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YbmnEZZ7ij8=
            n_samples: 232
            precision: 0.04285714285714286
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YbmnEZZ7ij8=
            n_samples: 232
            precision: 1.0
            recall: 1.0
        8515900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 380
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 380
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 380
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9870766488413547
            f1: 0.125
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 380
            precision: 0.06666666666666667
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 380
            precision: 1.0
            recall: 1.0
        8516100090:
          catboost:
            auc: 0.9999335106382978
            f1: 0.9928571428571429
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              k0Bnmdwbxz8=
            n_samples: 781
            precision: 1.0
            recall: 0.9858156028368794
          lightgbm:
            auc: 0.9999667553191489
            f1: 0.99644128113879
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              k0Bnmdwbxz8=
            n_samples: 781
            precision: 1.0
            recall: 0.9929078014184397
          logisticregression:
            auc: 0.9994015957446809
            f1: 0.972027972027972
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              k0Bnmdwbxz8=
            n_samples: 781
            precision: 0.9586206896551724
            recall: 0.9858156028368794
          randomforest:
            auc: 0.9998891843971631
            f1: 0.9856115107913669
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              k0Bnmdwbxz8=
            n_samples: 781
            precision: 1.0
            recall: 0.9716312056737588
          xgboost:
            auc: 1.0
            f1: 0.989247311827957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              k0Bnmdwbxz8=
            n_samples: 781
            precision: 1.0
            recall: 0.9787234042553191
        8516290000:
          catboost:
            auc: 1.0
            f1: 0.96875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fN3rXve6xz8=
            n_samples: 178
            precision: 1.0
            recall: 0.9393939393939394
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fN3rXve6xz8=
            n_samples: 178
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9954022988505746
            f1: 0.927536231884058
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fN3rXve6xz8=
            n_samples: 178
            precision: 0.8888888888888888
            recall: 0.9696969696969697
          randomforest:
            auc: 0.9972831765935214
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fN3rXve6xz8=
            n_samples: 178
            precision: 0.9375
            recall: 0.9090909090909091
          xgboost:
            auc: 1.0
            f1: 0.9846153846153847
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fN3rXve6xz8=
            n_samples: 178
            precision: 1.0
            recall: 0.9696969696969697
        8516400000:
          catboost:
            auc: 0.999671412924425
            f1: 0.9622641509433962
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jGR4bvMwwj8=
            n_samples: 387
            precision: 1.0
            recall: 0.9272727272727272
          lightgbm:
            auc: 0.9998904709748083
            f1: 0.9724770642201835
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jGR4bvMwwj8=
            n_samples: 387
            precision: 0.9814814814814815
            recall: 0.9636363636363636
          logisticregression:
            auc: 0.9956736035049287
            f1: 0.9357798165137615
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jGR4bvMwwj8=
            n_samples: 387
            precision: 0.9444444444444444
            recall: 0.9272727272727272
          randomforest:
            auc: 0.9995618838992333
            f1: 0.9629629629629629
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jGR4bvMwwj8=
            n_samples: 387
            precision: 0.9811320754716981
            recall: 0.9454545454545454
          xgboost:
            auc: 0.9999452354874042
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jGR4bvMwwj8=
            n_samples: 387
            precision: 1.0
            recall: 0.9090909090909091
        8516500000:
          catboost:
            auc: 0.9999822528892297
            f1: 0.9914893617021276
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rmgGsjTUxD8=
            n_samples: 1438
            precision: 0.9872881355932204
            recall: 0.9957264957264957
          lightgbm:
            auc: 0.9999999999999999
            f1: 0.997867803837953
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rmgGsjTUxD8=
            n_samples: 1438
            precision: 0.9957446808510638
            recall: 1.0
          logisticregression:
            auc: 0.9971001221001221
            f1: 0.9708333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rmgGsjTUxD8=
            n_samples: 1438
            precision: 0.9471544715447154
            recall: 0.9957264957264957
          randomforest:
            auc: 0.9998473748473748
            f1: 0.9892933618843683
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rmgGsjTUxD8=
            n_samples: 1438
            precision: 0.9914163090128756
            recall: 0.9871794871794872
          xgboost:
            auc: 0.999996450577846
            f1: 0.9978586723768736
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rmgGsjTUxD8=
            n_samples: 1438
            precision: 1.0
            recall: 0.9957264957264957
        8516609000:
          catboost:
            auc: 0.9999195003469814
            f1: 0.9981785063752276
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              h00guEw1xj8=
            n_samples: 1585
            precision: 1.0
            recall: 0.9963636363636363
          lightgbm:
            auc: 0.9999306037473975
            f1: 0.9927536231884058
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              h00guEw1xj8=
            n_samples: 1585
            precision: 0.9891696750902527
            recall: 0.9963636363636363
          logisticregression:
            auc: 0.9928771686328939
            f1: 0.9462738301559792
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              h00guEw1xj8=
            n_samples: 1585
            precision: 0.9039735099337748
            recall: 0.9927272727272727
          randomforest:
            auc: 0.9995863983344899
            f1: 0.9856115107913669
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              h00guEw1xj8=
            n_samples: 1585
            precision: 0.9750889679715302
            recall: 0.9963636363636363
          xgboost:
            auc: 0.9999555863983344
            f1: 0.9981785063752276
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              h00guEw1xj8=
            n_samples: 1585
            precision: 1.0
            recall: 0.9963636363636363
        8516710000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Yid2Yid2wj8=
            n_samples: 312
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Yid2Yid2wj8=
            n_samples: 312
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.995089471493966
            f1: 0.8936170212765957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Yid2Yid2wj8=
            n_samples: 312
            precision: 0.8571428571428571
            recall: 0.9333333333333333
          randomforest:
            auc: 0.996421140241365
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Yid2Yid2wj8=
            n_samples: 312
            precision: 0.9130434782608695
            recall: 0.9333333333333333
          xgboost:
            auc: 1.0
            f1: 0.9772727272727273
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Yid2Yid2wj8=
            n_samples: 312
            precision: 1.0
            recall: 0.9555555555555556
        8516791000:
          catboost:
            auc: 0.9999999999999999
            f1: 0.9935760171306209
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wETFYQFM1D8=
            n_samples: 741
            precision: 1.0
            recall: 0.9872340425531915
          lightgbm:
            auc: 0.9999747708350853
            f1: 0.997867803837953
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wETFYQFM1D8=
            n_samples: 741
            precision: 1.0
            recall: 0.9957446808510638
          logisticregression:
            auc: 0.9991001597847111
            f1: 0.9893390191897654
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wETFYQFM1D8=
            n_samples: 741
            precision: 0.9914529914529915
            recall: 0.9872340425531915
          randomforest:
            auc: 0.9999159027836177
            f1: 0.9957264957264957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wETFYQFM1D8=
            n_samples: 741
            precision: 1.0
            recall: 0.9914893617021276
          xgboost:
            auc: 1.0
            f1: 0.9957264957264957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wETFYQFM1D8=
            n_samples: 741
            precision: 1.0
            recall: 0.9914893617021276
        8516792000:
          catboost:
            auc: 1.0
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              whAaDKHBsD8=
            n_samples: 275
            precision: 1.0
            recall: 0.8888888888888888
          lightgbm:
            auc: 1.0
            f1: 0.875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              whAaDKHBsD8=
            n_samples: 275
            precision: 1.0
            recall: 0.7777777777777778
          logisticregression:
            auc: 0.991785559878945
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              whAaDKHBsD8=
            n_samples: 275
            precision: 1.0
            recall: 0.8888888888888888
          randomforest:
            auc: 0.9997838305231301
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              whAaDKHBsD8=
            n_samples: 275
            precision: 1.0
            recall: 0.8888888888888888
          xgboost:
            auc: 1.0
            f1: 0.2
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              whAaDKHBsD8=
            n_samples: 275
            precision: 1.0
            recall: 0.1111111111111111
        8516799000:
          catboost:
            auc: 0.9999637313216306
            f1: 0.9833333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hetRuB6Fwz8=
            n_samples: 800
            precision: 1.0
            recall: 0.9672131147540983
          lightgbm:
            auc: 1.0
            f1: 0.9959183673469387
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hetRuB6Fwz8=
            n_samples: 800
            precision: 0.991869918699187
            recall: 1.0
          logisticregression:
            auc: 0.9963005948063253
            f1: 0.944
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hetRuB6Fwz8=
            n_samples: 800
            precision: 0.921875
            recall: 0.9672131147540983
          randomforest:
            auc: 0.9990932830407661
            f1: 0.9583333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hetRuB6Fwz8=
            n_samples: 800
            precision: 0.9745762711864406
            recall: 0.9426229508196722
          xgboost:
            auc: 0.9999637313216307
            f1: 0.9833333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              hetRuB6Fwz8=
            n_samples: 800
            precision: 1.0
            recall: 0.9672131147540983
        8516800000:
          catboost:
            auc: 1.0
            f1: 0.9841269841269841
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7suT8=
            n_samples: 316
            precision: 1.0
            recall: 0.96875
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7suT8=
            n_samples: 316
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9642385563380281
            f1: 0.6582278481012658
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7suT8=
            n_samples: 316
            precision: 0.5531914893617021
            recall: 0.8125
          randomforest:
            auc: 0.981294014084507
            f1: 0.725
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7suT8=
            n_samples: 316
            precision: 0.6041666666666666
            recall: 0.90625
          xgboost:
            auc: 1.0
            f1: 0.9333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              2TMQlY7suT8=
            n_samples: 316
            precision: 1.0
            recall: 0.875
        8516900000:
          catboost:
            auc: 0.9995399125833909
            f1: 0.9302325581395349
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ln0OqQnGuz8=
            n_samples: 212
            precision: 1.0
            recall: 0.8695652173913043
          lightgbm:
            auc: 0.9949390384172994
            f1: 0.8
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ln0OqQnGuz8=
            n_samples: 212
            precision: 0.9411764705882353
            recall: 0.6956521739130435
          logisticregression:
            auc: 0.9804462847941109
            f1: 0.7307692307692307
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ln0OqQnGuz8=
            n_samples: 212
            precision: 0.6551724137931034
            recall: 0.8260869565217391
          randomforest:
            auc: 0.990798251667817
            f1: 0.8333333333333334
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ln0OqQnGuz8=
            n_samples: 212
            precision: 0.8
            recall: 0.8695652173913043
          xgboost:
            auc: 0.9949390384172992
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ln0OqQnGuz8=
            n_samples: 212
            precision: 0.0
            recall: 0.0
        8517110000:
          catboost:
            auc: 0.9999948511996704
            f1: 0.9969788519637462
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9L4Ec9Qr1z8=
            n_samples: 917
            precision: 1.0
            recall: 0.9939759036144579
          lightgbm:
            auc: 1.0
            f1: 0.9984917043740573
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9L4Ec9Qr1z8=
            n_samples: 917
            precision: 1.0
            recall: 0.9969879518072289
          logisticregression:
            auc: 0.9992740191535372
            f1: 0.9969788519637462
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9L4Ec9Qr1z8=
            n_samples: 917
            precision: 1.0
            recall: 0.9939759036144579
          randomforest:
            auc: 0.9996318607764392
            f1: 0.8167281672816729
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9L4Ec9Qr1z8=
            n_samples: 917
            precision: 0.6902286902286903
            recall: 1.0
          xgboost:
            auc: 0.9999974255998352
            f1: 0.9969788519637462
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              9L4Ec9Qr1z8=
            n_samples: 917
            precision: 1.0
            recall: 0.9939759036144579
        8517120000:
          catboost:
            auc: 0.9999857722024781
            f1: 0.9991163475699558
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              K6bHduPg5T8=
            n_samples: 2485
            precision: 1.0
            recall: 0.998234255444379
          lightgbm:
            auc: 0.9999940093484119
            f1: 0.9991163475699558
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              K6bHduPg5T8=
            n_samples: 2485
            precision: 1.0
            recall: 0.998234255444379
          logisticregression:
            auc: 0.9998697033279568
            f1: 0.9991163475699558
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              K6bHduPg5T8=
            n_samples: 2485
            precision: 1.0
            recall: 0.998234255444379
          randomforest:
            auc: 0.9996780024771345
            f1: 0.9399391087738721
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              K6bHduPg5T8=
            n_samples: 2485
            precision: 0.8871473354231975
            recall: 0.9994114184814596
          xgboost:
            auc: 0.9999925116855147
            f1: 0.9991163475699558
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              K6bHduPg5T8=
            n_samples: 2485
            precision: 1.0
            recall: 0.998234255444379
        8517130000:
          catboost:
            auc: 0.9999379414285292
            f1: 0.9904200442151806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LbY7F4+X4j8=
            n_samples: 1179
            precision: 1.0
            recall: 0.981021897810219
          lightgbm:
            auc: 0.9999867017346848
            f1: 0.9985401459854014
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LbY7F4+X4j8=
            n_samples: 1179
            precision: 0.9985401459854014
            recall: 0.9985401459854014
          logisticregression:
            auc: 0.9962557995212624
            f1: 0.9868035190615836
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LbY7F4+X4j8=
            n_samples: 1179
            precision: 0.9911634756995582
            recall: 0.9824817518248176
          randomforest:
            auc: 0.9994798900676733
            f1: 0.8960104643557881
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LbY7F4+X4j8=
            n_samples: 1179
            precision: 0.8116113744075829
            recall: 1.0
          xgboost:
            auc: 0.9999763586394397
            f1: 0.9904200442151806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LbY7F4+X4j8=
            n_samples: 1179
            precision: 1.0
            recall: 0.981021897810219
        8517140000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              m0YhnuO87z8=
            n_samples: 1831
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              m0YhnuO87z8=
            n_samples: 1831
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              m0YhnuO87z8=
            n_samples: 1831
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.9997247453894853
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              m0YhnuO87z8=
            n_samples: 1831
            precision: 0.9994496422674739
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              m0YhnuO87z8=
            n_samples: 1831
            precision: 1.0
            recall: 1.0
        8517180000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IfnzKosz6j8=
            n_samples: 447
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IfnzKosz6j8=
            n_samples: 447
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IfnzKosz6j8=
            n_samples: 447
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9989880590973488
            f1: 0.9851551956815114
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IfnzKosz6j8=
            n_samples: 447
            precision: 0.9733333333333334
            recall: 0.9972677595628415
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IfnzKosz6j8=
            n_samples: 447
            precision: 1.0
            recall: 1.0
        8517620000:
          catboost:
            auc: 0.9995650547121137
            f1: 0.989329268292683
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              P/RZWYX52D8=
            n_samples: 3398
            precision: 1.0
            recall: 0.9788838612368024
          lightgbm:
            auc: 0.9997210162651339
            f1: 0.9912713472485768
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              P/RZWYX52D8=
            n_samples: 3398
            precision: 0.9977081741787625
            recall: 0.9849170437405732
          logisticregression:
            auc: 0.9978267294443766
            f1: 0.9731974329935825
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              P/RZWYX52D8=
            n_samples: 3398
            precision: 0.9743008314436886
            recall: 0.9720965309200603
          randomforest:
            auc: 0.9986178203089967
            f1: 0.976054732041049
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              P/RZWYX52D8=
            n_samples: 3398
            precision: 0.9839080459770115
            recall: 0.9683257918552036
          xgboost:
            auc: 0.9997106430929961
            f1: 0.9908675799086758
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              P/RZWYX52D8=
            n_samples: 3398
            precision: 1.0
            recall: 0.9819004524886877
        8517690000:
          catboost:
            auc: 0.9999905430198031
            f1: 0.9975520195838433
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              S9dW4Jh12j8=
            n_samples: 1981
            precision: 1.0
            recall: 0.9951159951159951
          lightgbm:
            auc: 0.9999915937953804
            f1: 0.9987775061124694
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              S9dW4Jh12j8=
            n_samples: 1981
            precision: 1.0
            recall: 0.9975579975579976
          logisticregression:
            auc: 0.999001763201419
            f1: 0.9957238851557727
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              S9dW4Jh12j8=
            n_samples: 1981
            precision: 0.9963325183374083
            recall: 0.9951159951159951
          randomforest:
            auc: 0.9998970239934095
            f1: 0.9957238851557727
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              S9dW4Jh12j8=
            n_samples: 1981
            precision: 0.9963325183374083
            recall: 0.9951159951159951
          xgboost:
            auc: 0.9999936953465353
            f1: 0.998165137614679
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              S9dW4Jh12j8=
            n_samples: 1981
            precision: 1.0
            recall: 0.9963369963369964
        8517700000:
          catboost:
            auc: 0.9999928613240817
            f1: 0.9992069785884219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BFX5c4PI4j8=
            n_samples: 1075
            precision: 1.0
            recall: 0.9984152139461173
          lightgbm:
            auc: 0.9999928613240816
            f1: 0.9992069785884219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BFX5c4PI4j8=
            n_samples: 1075
            precision: 1.0
            recall: 0.9984152139461173
          logisticregression:
            auc: 0.999982153310204
            f1: 0.9984152139461173
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BFX5c4PI4j8=
            n_samples: 1075
            precision: 0.9984152139461173
            recall: 0.9984152139461173
          randomforest:
            auc: 0.9998679344955097
            f1: 0.997624703087886
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BFX5c4PI4j8=
            n_samples: 1075
            precision: 0.9968354430379747
            recall: 0.9984152139461173
          xgboost:
            auc: 0.9999928613240817
            f1: 0.9992069785884219
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              BFX5c4PI4j8=
            n_samples: 1075
            precision: 1.0
            recall: 0.9984152139461173
        8517710000:
          catboost:
            auc: 0.9999147485080989
            f1: 0.9939246658566221
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bJ5KIX3N4z8=
            n_samples: 669
            precision: 1.0
            recall: 0.9879227053140096
          lightgbm:
            auc: 0.999962110448044
            f1: 0.9951690821256038
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bJ5KIX3N4z8=
            n_samples: 669
            precision: 0.9951690821256038
            recall: 0.9951690821256038
          logisticregression:
            auc: 0.9973856209150328
            f1: 0.991556091676719
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bJ5KIX3N4z8=
            n_samples: 669
            precision: 0.9903614457831326
            recall: 0.9927536231884058
          randomforest:
            auc: 0.9983612768779009
            f1: 0.9915151515151515
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bJ5KIX3N4z8=
            n_samples: 669
            precision: 0.9951338199513382
            recall: 0.9879227053140096
          xgboost:
            auc: 0.999962110448044
            f1: 0.9951456310679612
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bJ5KIX3N4z8=
            n_samples: 669
            precision: 1.0
            recall: 0.9903381642512077
        8517790000:
          catboost:
            auc: 1.0
            f1: 0.9994921279837481
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzUJqHo+5D8=
            n_samples: 1557
            precision: 1.0
            recall: 0.9989847715736041
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzUJqHo+5D8=
            n_samples: 1557
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9995083596606439
            f1: 0.9994921279837481
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzUJqHo+5D8=
            n_samples: 1557
            precision: 1.0
            recall: 0.9989847715736041
          randomforest:
            auc: 0.9999911256256433
            f1: 0.9984763839512443
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzUJqHo+5D8=
            n_samples: 1557
            precision: 0.9989837398373984
            recall: 0.9979695431472081
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzUJqHo+5D8=
            n_samples: 1557
            precision: 1.0
            recall: 1.0
        8518210000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 250
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 250
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 250
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 250
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 250
            precision: 0.0
            recall: 0.0
        8518220000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 101
            precision: 0.0
            recall: 0.0
        8518290000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 671
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 671
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 671
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 671
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 671
            precision: 0.0
            recall: 0.0
        8518300000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 744
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 744
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 744
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 744
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 744
            precision: 0.0
            recall: 0.0
        8521909000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jYn0QOXslj8=
            n_samples: 134
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jYn0QOXslj8=
            n_samples: 134
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jYn0QOXslj8=
            n_samples: 134
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.13333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jYn0QOXslj8=
            n_samples: 134
            precision: 0.07142857142857142
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jYn0QOXslj8=
            n_samples: 134
            precision: 1.0
            recall: 1.0
        8523211000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              T+GTlAWIkz8=
            n_samples: 367
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              T+GTlAWIkz8=
            n_samples: 367
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              T+GTlAWIkz8=
            n_samples: 367
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              T+GTlAWIkz8=
            n_samples: 367
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              T+GTlAWIkz8=
            n_samples: 367
            precision: 1.0
            recall: 1.0
        8523490000:
          catboost:
            auc: 1.0
            f1: 0.98989898989899
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WZZlWZZl2T8=
            n_samples: 126
            precision: 1.0
            recall: 0.98
          lightgbm:
            auc: 1.0
            f1: 0.98989898989899
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WZZlWZZl2T8=
            n_samples: 126
            precision: 1.0
            recall: 0.98
          logisticregression:
            auc: 0.9997368421052631
            f1: 0.98989898989899
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WZZlWZZl2T8=
            n_samples: 126
            precision: 1.0
            recall: 0.98
          randomforest:
            auc: 0.9992105263157894
            f1: 0.98989898989899
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WZZlWZZl2T8=
            n_samples: 126
            precision: 1.0
            recall: 0.98
          xgboost:
            auc: 1.0
            f1: 0.98989898989899
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WZZlWZZl2T8=
            n_samples: 126
            precision: 1.0
            recall: 0.98
        8523510000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dNFFF1100T8=
            n_samples: 176
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dNFFF1100T8=
            n_samples: 176
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dNFFF1100T8=
            n_samples: 176
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dNFFF1100T8=
            n_samples: 176
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dNFFF1100T8=
            n_samples: 176
            precision: 1.0
            recall: 1.0
        8523520000:
          catboost:
            auc: 0.9995983992835159
            f1: 0.9810606060606061
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EvN2hsS81T8=
            n_samples: 792
            precision: 1.0
            recall: 0.9628252788104089
          lightgbm:
            auc: 0.9991577046919757
            f1: 0.9761467889908257
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EvN2hsS81T8=
            n_samples: 792
            precision: 0.9637681159420289
            recall: 0.9888475836431226
          logisticregression:
            auc: 0.978619204333023
            f1: 0.8819188191881919
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EvN2hsS81T8=
            n_samples: 792
            precision: 0.8754578754578755
            recall: 0.8884758364312267
          randomforest:
            auc: 0.984860008387413
            f1: 0.6683168316831684
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EvN2hsS81T8=
            n_samples: 792
            precision: 1.0
            recall: 0.5018587360594795
          xgboost:
            auc: 0.9999004883180393
            f1: 0.9849056603773585
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EvN2hsS81T8=
            n_samples: 792
            precision: 1.0
            recall: 0.9702602230483272
        8523590000:
          catboost:
            auc: 0.9992875463094899
            f1: 0.9702127659574468
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FikDT7fm1z8=
            n_samples: 324
            precision: 1.0
            recall: 0.9421487603305785
          lightgbm:
            auc: 0.9995928836054228
            f1: 0.9790794979079498
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FikDT7fm1z8=
            n_samples: 324
            precision: 0.9915254237288136
            recall: 0.9669421487603306
          logisticregression:
            auc: 0.9945039286732077
            f1: 0.9707112970711297
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FikDT7fm1z8=
            n_samples: 324
            precision: 0.9830508474576272
            recall: 0.9586776859504132
          randomforest:
            auc: 0.9969466270406708
            f1: 0.9658119658119658
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FikDT7fm1z8=
            n_samples: 324
            precision: 1.0
            recall: 0.9338842975206612
          xgboost:
            auc: 0.9995928836054228
            f1: 0.9702127659574468
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              FikDT7fm1z8=
            n_samples: 324
            precision: 1.0
            recall: 0.9421487603305785
        8523800000:
          catboost:
            auc: 0.9998242762377543
            f1: 0.97
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              M/B0a35Y1D8=
            n_samples: 324
            precision: 1.0
            recall: 0.941747572815534
          lightgbm:
            auc: 0.9997803452971927
            f1: 0.9801980198019802
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              M/B0a35Y1D8=
            n_samples: 324
            precision: 1.0
            recall: 0.9611650485436893
          logisticregression:
            auc: 0.9938496683213986
            f1: 0.9603960396039604
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              M/B0a35Y1D8=
            n_samples: 324
            precision: 0.9797979797979798
            recall: 0.941747572815534
          randomforest:
            auc: 0.9975398673285595
            f1: 0.97
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              M/B0a35Y1D8=
            n_samples: 324
            precision: 1.0
            recall: 0.941747572815534
          xgboost:
            auc: 0.9997803452971928
            f1: 0.97
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              M/B0a35Y1D8=
            n_samples: 324
            precision: 1.0
            recall: 0.941747572815534
        8525500000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TgJxoeYk0D8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TgJxoeYk0D8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TgJxoeYk0D8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.6153846153846154
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TgJxoeYk0D8=
            n_samples: 111
            precision: 0.4444444444444444
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TgJxoeYk0D8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
        8525600000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z7fwqzGRnj8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z7fwqzGRnj8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z7fwqzGRnj8=
            n_samples: 201
            precision: 0.8571428571428571
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.11764705882352941
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z7fwqzGRnj8=
            n_samples: 201
            precision: 0.0625
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Z7fwqzGRnj8=
            n_samples: 201
            precision: 1.0
            recall: 1.0
        8525800000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Q9Md7LfamD8=
            n_samples: 206
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Q9Md7LfamD8=
            n_samples: 206
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Q9Md7LfamD8=
            n_samples: 206
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9970149253731344
            f1: 0.17857142857142858
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Q9Md7LfamD8=
            n_samples: 206
            precision: 0.09803921568627451
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Q9Md7LfamD8=
            n_samples: 206
            precision: 1.0
            recall: 1.0
        8525890000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KfEzSvyMkj8=
            n_samples: 276
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KfEzSvyMkj8=
            n_samples: 276
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KfEzSvyMkj8=
            n_samples: 276
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9350553505535054
            f1: 0.12658227848101267
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KfEzSvyMkj8=
            n_samples: 276
            precision: 0.06756756756756757
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              KfEzSvyMkj8=
            n_samples: 276
            precision: 1.0
            recall: 1.0
        8526100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCurT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCurT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCurT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9971153846153846
            f1: 0.17391304347826086
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCurT8=
            n_samples: 138
            precision: 0.09523809523809523
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              24G5dmCurT8=
            n_samples: 138
            precision: 1.0
            recall: 1.0
        8526910000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jXl1K8DNqT8=
            n_samples: 377
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jXl1K8DNqT8=
            n_samples: 377
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jXl1K8DNqT8=
            n_samples: 377
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.2638888888888889
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jXl1K8DNqT8=
            n_samples: 377
            precision: 0.152
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              jXl1K8DNqT8=
            n_samples: 377
            precision: 1.0
            recall: 1.0
        8526920000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AqHkTtHCeT8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AqHkTtHCeT8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AqHkTtHCeT8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8544303797468354
            f1: 0.03636363636363636
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AqHkTtHCeT8=
            n_samples: 159
            precision: 0.018518518518518517
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AqHkTtHCeT8=
            n_samples: 159
            precision: 1.0
            recall: 1.0
        8527990000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oCyBTfvJcj8=
            n_samples: 436
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oCyBTfvJcj8=
            n_samples: 436
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9723502304147466
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oCyBTfvJcj8=
            n_samples: 436
            precision: 1.0
            recall: 0.5
          randomforest:
            auc: 0.9435483870967742
            f1: 0.038461538461538464
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oCyBTfvJcj8=
            n_samples: 436
            precision: 0.02
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              oCyBTfvJcj8=
            n_samples: 436
            precision: 1.0
            recall: 0.5
        8528520000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UmCVvm65eD8=
            n_samples: 497
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UmCVvm65eD8=
            n_samples: 497
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UmCVvm65eD8=
            n_samples: 497
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9784075573549258
            f1: 0.057692307692307696
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UmCVvm65eD8=
            n_samples: 497
            precision: 0.0297029702970297
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              UmCVvm65eD8=
            n_samples: 497
            precision: 1.0
            recall: 1.0
        8528590000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YAVYAVaAZT8=
            n_samples: 381
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YAVYAVaAZT8=
            n_samples: 381
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YAVYAVaAZT8=
            n_samples: 381
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.019230769230769232
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YAVYAVaAZT8=
            n_samples: 381
            precision: 0.009708737864077669
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              YAVYAVaAZT8=
            n_samples: 381
            precision: 1.0
            recall: 1.0
        8528690000:
          catboost:
            auc: 0.9999999999999999
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xnWqkdnVcz8=
            n_samples: 413
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xnWqkdnVcz8=
            n_samples: 413
            precision: 1.0
            recall: 0.5
          logisticregression:
            auc: 0.951338199513382
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xnWqkdnVcz8=
            n_samples: 413
            precision: 1.0
            recall: 0.5
          randomforest:
            auc: 0.8819951338199514
            f1: 0.04
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xnWqkdnVcz8=
            n_samples: 413
            precision: 0.020833333333333332
            recall: 0.5
          xgboost:
            auc: 1.0
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              xnWqkdnVcz8=
            n_samples: 413
            precision: 0.0
            recall: 0.0
        8528719000:
          catboost:
            auc: 0.9960475181063416
            f1: 0.8070175438596491
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreqD8=
            n_samples: 700
            precision: 1.0
            recall: 0.6764705882352942
          lightgbm:
            auc: 0.9978139904610492
            f1: 0.8421052631578947
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreqD8=
            n_samples: 700
            precision: 0.7619047619047619
            recall: 0.9411764705882353
          logisticregression:
            auc: 0.9622416534181241
            f1: 0.5679012345679012
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreqD8=
            n_samples: 700
            precision: 0.48936170212765956
            recall: 0.6764705882352942
          randomforest:
            auc: 0.9635665076841547
            f1: 0.37398373983739835
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreqD8=
            n_samples: 700
            precision: 0.25842696629213485
            recall: 0.6764705882352942
          xgboost:
            auc: 0.9993817346758523
            f1: 0.9206349206349206
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              TPR3slreqD8=
            n_samples: 700
            precision: 1.0
            recall: 0.8529411764705882
        8528721000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 449
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 449
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 449
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 449
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 449
            precision: 0.0
            recall: 0.0
        8528729000:
          catboost:
            auc: 0.9999976573664113
            f1: 0.9491525423728814
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sBhF+6ZmYj8=
            n_samples: 13801
            precision: 1.0
            recall: 0.9032258064516129
          lightgbm:
            auc: 0.9999906294656453
            f1: 0.9836065573770492
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sBhF+6ZmYj8=
            n_samples: 13801
            precision: 1.0
            recall: 0.967741935483871
          logisticregression:
            auc: 0.9883008878581301
            f1: 0.9491525423728814
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sBhF+6ZmYj8=
            n_samples: 13801
            precision: 1.0
            recall: 0.9032258064516129
          randomforest:
            auc: 0.988279804155832
            f1: 0.07920792079207921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sBhF+6ZmYj8=
            n_samples: 13801
            precision: 0.04142011834319527
            recall: 0.9032258064516129
          xgboost:
            auc: 0.9999976573664113
            f1: 0.9666666666666667
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sBhF+6ZmYj8=
            n_samples: 13801
            precision: 1.0
            recall: 0.9354838709677419
        8528739000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WBZ4ZYFXZj8=
            n_samples: 1100
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WBZ4ZYFXZj8=
            n_samples: 1100
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WBZ4ZYFXZj8=
            n_samples: 1100
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.025423728813559324
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WBZ4ZYFXZj8=
            n_samples: 1100
            precision: 0.012875536480686695
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WBZ4ZYFXZj8=
            n_samples: 1100
            precision: 1.0
            recall: 1.0
        8529100000:
          catboost:
            auc: 0.9890370276759762
            f1: 0.8285714285714286
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aPYvEK+UuD8=
            n_samples: 427
            precision: 1.0
            recall: 0.7073170731707317
          lightgbm:
            auc: 0.9952925565525085
            f1: 0.8125
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aPYvEK+UuD8=
            n_samples: 427
            precision: 0.7090909090909091
            recall: 0.9512195121951219
          logisticregression:
            auc: 0.9326424870466321
            f1: 0.6739130434782609
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aPYvEK+UuD8=
            n_samples: 427
            precision: 0.6078431372549019
            recall: 0.7560975609756098
          randomforest:
            auc: 0.85242638695817
            f1: 0.2815533980582524
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aPYvEK+UuD8=
            n_samples: 427
            precision: 0.17575757575757575
            recall: 0.7073170731707317
          xgboost:
            auc: 0.9959560217363831
            f1: 0.9367088607594937
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aPYvEK+UuD8=
            n_samples: 427
            precision: 0.9736842105263158
            recall: 0.9024390243902439
        8529900000:
          catboost:
            auc: 1.0
            f1: 0.9777777777777777
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0gOVs1v4pT8=
            n_samples: 536
            precision: 1.0
            recall: 0.9565217391304348
          lightgbm:
            auc: 1.0
            f1: 0.9777777777777777
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0gOVs1v4pT8=
            n_samples: 536
            precision: 1.0
            recall: 0.9565217391304348
          logisticregression:
            auc: 1.0
            f1: 0.9777777777777777
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0gOVs1v4pT8=
            n_samples: 536
            precision: 1.0
            recall: 0.9565217391304348
          randomforest:
            auc: 0.9961013645224172
            f1: 0.24083769633507854
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0gOVs1v4pT8=
            n_samples: 536
            precision: 0.13690476190476192
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9777777777777777
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              0gOVs1v4pT8=
            n_samples: 536
            precision: 1.0
            recall: 0.9565217391304348
        8530800000:
          catboost:
            auc: 0.9988636363636364
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fAfxHcR3sD8=
            n_samples: 171
            precision: 1.0
            recall: 0.9090909090909091
          lightgbm:
            auc: 0.9988636363636363
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fAfxHcR3sD8=
            n_samples: 171
            precision: 1.0
            recall: 0.9090909090909091
          logisticregression:
            auc: 0.95625
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fAfxHcR3sD8=
            n_samples: 171
            precision: 1.0
            recall: 0.9090909090909091
          randomforest:
            auc: 0.952840909090909
            f1: 0.22727272727272727
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fAfxHcR3sD8=
            n_samples: 171
            precision: 0.12987012987012986
            recall: 0.9090909090909091
          xgboost:
            auc: 0.9994318181818181
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fAfxHcR3sD8=
            n_samples: 171
            precision: 1.0
            recall: 0.9090909090909091
        8531100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VdgB9E2FfT8=
            n_samples: 555
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VdgB9E2FfT8=
            n_samples: 555
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VdgB9E2FfT8=
            n_samples: 555
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.058823529411764705
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VdgB9E2FfT8=
            n_samples: 555
            precision: 0.030303030303030304
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VdgB9E2FfT8=
            n_samples: 555
            precision: 1.0
            recall: 1.0
        8531200000:
          catboost:
            auc: 0.8255675029868578
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VxCTK4jJhT8=
            n_samples: 282
            precision: 1.0
            recall: 0.3333333333333333
          lightgbm:
            auc: 0.8333333333333334
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VxCTK4jJhT8=
            n_samples: 282
            precision: 1.0
            recall: 0.3333333333333333
          logisticregression:
            auc: 0.7968936678614098
            f1: 0.2857142857142857
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VxCTK4jJhT8=
            n_samples: 282
            precision: 0.25
            recall: 0.3333333333333333
          randomforest:
            auc: 0.6236559139784946
            f1: 0.014705882352941176
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VxCTK4jJhT8=
            n_samples: 282
            precision: 0.007518796992481203
            recall: 0.3333333333333333
          xgboost:
            auc: 0.8363201911589009
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VxCTK4jJhT8=
            n_samples: 282
            precision: 1.0
            recall: 0.3333333333333333
        8531800000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tmrO05XeeT8=
            n_samples: 475
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tmrO05XeeT8=
            n_samples: 475
            precision: 0.75
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tmrO05XeeT8=
            n_samples: 475
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.0392156862745098
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tmrO05XeeT8=
            n_samples: 475
            precision: 0.02
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              tmrO05XeeT8=
            n_samples: 475
            precision: 1.0
            recall: 1.0
        8531900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhiD8=
            n_samples: 168
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhiD8=
            n_samples: 168
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhiD8=
            n_samples: 168
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9909638554216867
            f1: 0.05063291139240506
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhiD8=
            n_samples: 168
            precision: 0.025974025974025976
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhiD8=
            n_samples: 168
            precision: 1.0
            recall: 1.0
        8532290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2gj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2gj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2gj8=
            n_samples: 108
            precision: 0.5
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.10526315789473684
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2gj8=
            n_samples: 108
            precision: 0.05555555555555555
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              aC+hvYT2gj8=
            n_samples: 108
            precision: 1.0
            recall: 1.0
        8535290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykuuT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykuuT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykuuT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.38095238095238093
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykuuT8=
            n_samples: 122
            precision: 0.23529411764705882
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykuuT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
        8535300000:
          catboost:
            auc: 0.9979797979797981
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mjRp0qRJsz8=
            n_samples: 146
            precision: 1.0
            recall: 0.9090909090909091
          lightgbm:
            auc: 0.9986531986531987
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mjRp0qRJsz8=
            n_samples: 146
            precision: 1.0
            recall: 0.9090909090909091
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mjRp0qRJsz8=
            n_samples: 146
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9474747474747475
            f1: 0.25
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mjRp0qRJsz8=
            n_samples: 146
            precision: 0.14285714285714285
            recall: 1.0
          xgboost:
            auc: 0.997979797979798
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mjRp0qRJsz8=
            n_samples: 146
            precision: 1.0
            recall: 0.9090909090909091
        8535400000:
          catboost:
            auc: 0.9993827160493828
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 189
            precision: 1.0
            recall: 0.8888888888888888
          lightgbm:
            auc: 0.9999999999999999
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 189
            precision: 1.0
            recall: 0.8888888888888888
          logisticregression:
            auc: 0.9993827160493828
            f1: 0.9473684210526315
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 189
            precision: 0.9
            recall: 1.0
          randomforest:
            auc: 0.9814814814814814
            f1: 0.2571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 189
            precision: 0.14754098360655737
            recall: 1.0
          xgboost:
            auc: 0.9987654320987654
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GIZhGIZhqD8=
            n_samples: 189
            precision: 1.0
            recall: 0.8888888888888888
        8535900000:
          catboost:
            auc: 0.9991120521840101
            f1: 0.989247311827957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wGudLk31sT8=
            n_samples: 670
            precision: 1.0
            recall: 0.9787234042553191
          lightgbm:
            auc: 0.9999316963218471
            f1: 0.989247311827957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wGudLk31sT8=
            n_samples: 670
            precision: 1.0
            recall: 0.9787234042553191
          logisticregression:
            auc: 0.9999658481609235
            f1: 0.989247311827957
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wGudLk31sT8=
            n_samples: 670
            precision: 1.0
            recall: 0.9787234042553191
          randomforest:
            auc: 0.9857586831050852
            f1: 0.30718954248366015
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wGudLk31sT8=
            n_samples: 670
            precision: 0.18146718146718147
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9782608695652174
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              wGudLk31sT8=
            n_samples: 670
            precision: 1.0
            recall: 0.9574468085106383
        8536100000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EQ7hEA7hkD8=
            n_samples: 182
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EQ7hEA7hkD8=
            n_samples: 182
            precision: 0.75
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EQ7hEA7hkD8=
            n_samples: 182
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9981378026070764
            f1: 0.1875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EQ7hEA7hkD8=
            n_samples: 182
            precision: 0.10344827586206896
            recall: 1.0
          xgboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EQ7hEA7hkD8=
            n_samples: 182
            precision: 1.0
            recall: 1.0
        8536200000:
          catboost:
            auc: 1.0
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fy2YYalbhT8=
            n_samples: 863
            precision: 1.0
            recall: 0.8888888888888888
          lightgbm:
            auc: 1.0
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fy2YYalbhT8=
            n_samples: 863
            precision: 1.0
            recall: 0.8888888888888888
          logisticregression:
            auc: 0.9998698933125163
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fy2YYalbhT8=
            n_samples: 863
            precision: 1.0
            recall: 0.8888888888888888
          randomforest:
            auc: 0.9920634920634921
            f1: 0.09230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fy2YYalbhT8=
            n_samples: 863
            precision: 0.04838709677419355
            recall: 1.0
          xgboost:
            auc: 0.9998698933125163
            f1: 0.9411764705882353
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fy2YYalbhT8=
            n_samples: 863
            precision: 1.0
            recall: 0.8888888888888888
        8536300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRpPEzPqiD8=
            n_samples: 411
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRpPEzPqiD8=
            n_samples: 411
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRpPEzPqiD8=
            n_samples: 411
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9970443349753693
            f1: 0.0970873786407767
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRpPEzPqiD8=
            n_samples: 411
            precision: 0.05102040816326531
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              yRpPEzPqiD8=
            n_samples: 411
            precision: 1.0
            recall: 1.0
        8536410000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERgT8=
            n_samples: 120
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERgT8=
            n_samples: 120
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERgT8=
            n_samples: 120
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.13333333333333333
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERgT8=
            n_samples: 120
            precision: 0.07142857142857142
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ERERERERgT8=
            n_samples: 120
            precision: 1.0
            recall: 1.0
        8536490000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PNEbwRO9oT8=
            n_samples: 635
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PNEbwRO9oT8=
            n_samples: 635
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PNEbwRO9oT8=
            n_samples: 635
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.33587786259541985
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PNEbwRO9oT8=
            n_samples: 635
            precision: 0.2018348623853211
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PNEbwRO9oT8=
            n_samples: 635
            precision: 1.0
            recall: 1.0
        8536500000:
          catboost:
            auc: 0.9999587900766505
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gGgfcK1SdD8=
            n_samples: 2217
            precision: 1.0
            recall: 0.9090909090909091
          lightgbm:
            auc: 1.0
            f1: 0.9
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gGgfcK1SdD8=
            n_samples: 2217
            precision: 1.0
            recall: 0.8181818181818182
          logisticregression:
            auc: 0.9999175801533009
            f1: 0.9
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gGgfcK1SdD8=
            n_samples: 2217
            precision: 1.0
            recall: 0.8181818181818182
          randomforest:
            auc: 0.9909750267864502
            f1: 0.09482758620689655
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gGgfcK1SdD8=
            n_samples: 2217
            precision: 0.049773755656108594
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              gGgfcK1SdD8=
            n_samples: 2217
            precision: 1.0
            recall: 0.8181818181818182
        8536610000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
        8536690000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OGRbkFLFhD8=
            n_samples: 986
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OGRbkFLFhD8=
            n_samples: 986
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.9523809523809523
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OGRbkFLFhD8=
            n_samples: 986
            precision: 0.9090909090909091
            recall: 1.0
          randomforest:
            auc: 0.9990778688524591
            f1: 0.10989010989010989
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OGRbkFLFhD8=
            n_samples: 986
            precision: 0.05813953488372093
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              OGRbkFLFhD8=
            n_samples: 986
            precision: 1.0
            recall: 1.0
        8536700000:
          catboost:
            auc: 1.0
            f1: 0.9166666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XPAE1uRmpT8=
            n_samples: 311
            precision: 1.0
            recall: 0.8461538461538461
          lightgbm:
            auc: 1.0
            f1: 0.9166666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XPAE1uRmpT8=
            n_samples: 311
            precision: 1.0
            recall: 0.8461538461538461
          logisticregression:
            auc: 0.9744450180691792
            f1: 0.9166666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XPAE1uRmpT8=
            n_samples: 311
            precision: 1.0
            recall: 0.8461538461538461
          randomforest:
            auc: 0.9535363964894167
            f1: 0.3142857142857143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XPAE1uRmpT8=
            n_samples: 311
            precision: 0.19298245614035087
            recall: 0.8461538461538461
          xgboost:
            auc: 1.0
            f1: 0.9166666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              XPAE1uRmpT8=
            n_samples: 311
            precision: 1.0
            recall: 0.8461538461538461
        8536900000:
          catboost:
            auc: 0.9999017744446572
            f1: 0.9365079365079365
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Hjzi78Wlj8=
            n_samples: 3106
            precision: 1.0
            recall: 0.8805970149253731
          lightgbm:
            auc: 0.9999852661666985
            f1: 0.9848484848484849
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Hjzi78Wlj8=
            n_samples: 3106
            precision: 1.0
            recall: 0.9701492537313433
          logisticregression:
            auc: 0.9744760894441907
            f1: 0.9076923076923077
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Hjzi78Wlj8=
            n_samples: 3106
            precision: 0.9365079365079365
            recall: 0.8805970149253731
          randomforest:
            auc: 0.9477489158354329
            f1: 0.18238021638330756
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Hjzi78Wlj8=
            n_samples: 3106
            precision: 0.10172413793103448
            recall: 0.8805970149253731
          xgboost:
            auc: 1.0
            f1: 0.9612403100775194
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              1Hjzi78Wlj8=
            n_samples: 3106
            precision: 1.0
            recall: 0.9253731343283582
        8537100000:
          catboost:
            auc: 0.9996256535820196
            f1: 0.9932432432432432
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C4bNSxFMqj8=
            n_samples: 2901
            precision: 1.0
            recall: 0.9865771812080537
          lightgbm:
            auc: 0.999917082878102
            f1: 0.9865771812080537
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C4bNSxFMqj8=
            n_samples: 2901
            precision: 0.9865771812080537
            recall: 0.9865771812080537
          logisticregression:
            auc: 0.9945494186046511
            f1: 0.9798657718120806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C4bNSxFMqj8=
            n_samples: 2901
            precision: 0.9798657718120806
            recall: 0.9798657718120806
          randomforest:
            auc: 0.9908181481192446
            f1: 0.3675
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C4bNSxFMqj8=
            n_samples: 2901
            precision: 0.22580645161290322
            recall: 0.9865771812080537
          xgboost:
            auc: 0.999943909005775
            f1: 0.9898305084745763
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              C4bNSxFMqj8=
            n_samples: 2901
            precision: 1.0
            recall: 0.9798657718120806
        8537200000:
          catboost:
            auc: 0.9915376418228548
            f1: 0.9658536585365853
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dLBmifEpwz8=
            n_samples: 708
            precision: 1.0
            recall: 0.9339622641509434
          lightgbm:
            auc: 0.9930107189870244
            f1: 0.9758454106280193
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dLBmifEpwz8=
            n_samples: 708
            precision: 1.0
            recall: 0.9528301886792453
          logisticregression:
            auc: 0.9876825675421551
            f1: 0.957345971563981
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dLBmifEpwz8=
            n_samples: 708
            precision: 0.9619047619047619
            recall: 0.9528301886792453
          randomforest:
            auc: 0.9752867799160032
            f1: 0.4315352697095436
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dLBmifEpwz8=
            n_samples: 708
            precision: 0.2765957446808511
            recall: 0.9811320754716981
          xgboost:
            auc: 0.9952438412837711
            f1: 0.9658536585365853
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dLBmifEpwz8=
            n_samples: 708
            precision: 1.0
            recall: 0.9339622641509434
        8538100000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PAikrZc3kz8=
            n_samples: 373
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PAikrZc3kz8=
            n_samples: 373
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PAikrZc3kz8=
            n_samples: 373
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.12173913043478261
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PAikrZc3kz8=
            n_samples: 373
            precision: 0.06481481481481481
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              PAikrZc3kz8=
            n_samples: 373
            precision: 1.0
            recall: 1.0
        8538900000:
          catboost:
            auc: 1.0
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ctrg35YzjT8=
            n_samples: 1052
            precision: 1.0
            recall: 0.9333333333333333
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ctrg35YzjT8=
            n_samples: 1052
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9998071359691417
            f1: 0.9090909090909091
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ctrg35YzjT8=
            n_samples: 1052
            precision: 0.8333333333333334
            recall: 1.0
          randomforest:
            auc: 0.9943426550948248
            f1: 0.1477832512315271
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ctrg35YzjT8=
            n_samples: 1052
            precision: 0.0797872340425532
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Ctrg35YzjT8=
            n_samples: 1052
            precision: 1.0
            recall: 0.9333333333333333
        8539100000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /Iq+lfbugT8=
            n_samples: 571
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /Iq+lfbugT8=
            n_samples: 571
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /Iq+lfbugT8=
            n_samples: 571
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9985865724381625
            f1: 0.24390243902439024
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /Iq+lfbugT8=
            n_samples: 571
            precision: 0.1388888888888889
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              /Iq+lfbugT8=
            n_samples: 571
            precision: 1.0
            recall: 1.0
        8539210000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVdT8=
            n_samples: 192
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVdT8=
            n_samples: 192
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.6666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVdT8=
            n_samples: 192
            precision: 0.5
            recall: 1.0
          randomforest:
            auc: 0.9947643979057592
            f1: 0.16666666666666666
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVdT8=
            n_samples: 192
            precision: 0.09090909090909091
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              VVVVVVVVdT8=
            n_samples: 192
            precision: 1.0
            recall: 1.0
        8539290000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rzuBtoKRXz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rzuBtoKRXz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rzuBtoKRXz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.006688963210702341
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rzuBtoKRXz8=
            n_samples: 519
            precision: 0.003355704697986577
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              rzuBtoKRXz8=
            n_samples: 519
            precision: 1.0
            recall: 1.0
        8539311000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NSeBuFBzgj8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NSeBuFBzgj8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NSeBuFBzgj8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.09090909090909091
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NSeBuFBzgj8=
            n_samples: 111
            precision: 0.047619047619047616
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              NSeBuFBzgj8=
            n_samples: 111
            precision: 1.0
            recall: 1.0
        8539319000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 299
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 299
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 299
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 299
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 299
            precision: 0.0
            recall: 0.0
        8539390000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 505
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 505
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 505
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 505
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 505
            precision: 0.0
            recall: 0.0
        8539490000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 633
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 633
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 633
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 633
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 633
            precision: 0.0
            recall: 0.0
        8539500000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 148
            precision: 0.0
            recall: 0.0
        8539520000:
          catboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 344
            precision: 0.0
            recall: 0.0
          lightgbm:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 344
            precision: 0.0
            recall: 0.0
          logisticregression:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 344
            precision: 0.0
            recall: 0.0
          randomforest:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 344
            precision: 0.0
            recall: 0.0
          xgboost:
            auc: .nan
            f1: 0.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              AAAAAAAAAAA=
            n_samples: 344
            precision: 0.0
            recall: 0.0
        8539900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.18181818181818182
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 133
            precision: 0.1
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              kHFBxgUZlz8=
            n_samples: 133
            precision: 1.0
            recall: 1.0
        8541401010:
          catboost:
            auc: 0.99934761335218
            f1: 0.9375
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3cl8SHcynz8=
            n_samples: 558
            precision: 1.0
            recall: 0.8823529411764706
          lightgbm:
            auc: 0.999728172230075
            f1: 0.9375
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3cl8SHcynz8=
            n_samples: 558
            precision: 1.0
            recall: 0.8823529411764706
          logisticregression:
            auc: 0.9605306078068935
            f1: 0.9375
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3cl8SHcynz8=
            n_samples: 558
            precision: 1.0
            recall: 0.8823529411764706
          randomforest:
            auc: 0.9120365336522779
            f1: 0.08695652173913043
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3cl8SHcynz8=
            n_samples: 558
            precision: 0.04573170731707317
            recall: 0.8823529411764706
          xgboost:
            auc: 0.9991301511362402
            f1: 0.9375
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              3cl8SHcynz8=
            n_samples: 558
            precision: 1.0
            recall: 0.8823529411764706
        8541409000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cxrceZE6nj8=
            n_samples: 542
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cxrceZE6nj8=
            n_samples: 542
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cxrceZE6nj8=
            n_samples: 542
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.0955223880597015
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cxrceZE6nj8=
            n_samples: 542
            precision: 0.050156739811912224
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              cxrceZE6nj8=
            n_samples: 542
            precision: 1.0
            recall: 1.0
        8541410000:
          catboost:
            auc: 0.999832252292552
            f1: 0.8852459016393442
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fMZFFIsHoD8=
            n_samples: 1086
            precision: 1.0
            recall: 0.7941176470588235
          lightgbm:
            auc: 0.9995806307313799
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fMZFFIsHoD8=
            n_samples: 1086
            precision: 0.9310344827586207
            recall: 0.7941176470588235
          logisticregression:
            auc: 0.9793949899351377
            f1: 0.8571428571428571
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fMZFFIsHoD8=
            n_samples: 1086
            precision: 0.9310344827586207
            recall: 0.7941176470588235
          randomforest:
            auc: 0.9713151420263924
            f1: 0.08571428571428572
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fMZFFIsHoD8=
            n_samples: 1086
            precision: 0.04483695652173913
            recall: 0.9705882352941176
          xgboost:
            auc: 1.0
            f1: 0.8852459016393442
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              fMZFFIsHoD8=
            n_samples: 1086
            precision: 1.0
            recall: 0.7941176470588235
        8541420000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykumT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykumT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykumT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.8683473389355741
            f1: 0.10344827586206896
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykumT8=
            n_samples: 122
            precision: 0.05454545454545454
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              WEeb9ykumT8=
            n_samples: 122
            precision: 1.0
            recall: 1.0
        8541430000:
          catboost:
            auc: 0.9995979975333441
            f1: 0.960352422907489
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Do/+XKrmoT8=
            n_samples: 3375
            precision: 1.0
            recall: 0.923728813559322
          lightgbm:
            auc: 0.9997176875881413
            f1: 0.9527896995708155
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Do/+XKrmoT8=
            n_samples: 3375
            precision: 0.9652173913043478
            recall: 0.940677966101695
          logisticregression:
            auc: 0.9820308800341377
            f1: 0.8898305084745762
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Do/+XKrmoT8=
            n_samples: 3375
            precision: 0.8898305084745762
            recall: 0.8898305084745762
          randomforest:
            auc: 0.9783959451091002
            f1: 0.3313953488372093
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Do/+XKrmoT8=
            n_samples: 3375
            precision: 0.2
            recall: 0.9661016949152542
          xgboost:
            auc: 0.9995615701253622
            f1: 0.9694323144104804
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Do/+XKrmoT8=
            n_samples: 3375
            precision: 1.0
            recall: 0.940677966101695
        8541490000:
          catboost:
            auc: 1.0
            f1: 0.9841269841269841
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bq8lh7jKtD8=
            n_samples: 394
            precision: 1.0
            recall: 0.96875
          lightgbm:
            auc: 0.9999136740331491
            f1: 0.96875
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bq8lh7jKtD8=
            n_samples: 394
            precision: 0.96875
            recall: 0.96875
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bq8lh7jKtD8=
            n_samples: 394
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.998446132596685
            f1: 0.28699551569506726
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bq8lh7jKtD8=
            n_samples: 394
            precision: 0.16753926701570682
            recall: 1.0
          xgboost:
            auc: 0.9999136740331492
            f1: 0.9841269841269841
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              bq8lh7jKtD8=
            n_samples: 394
            precision: 1.0
            recall: 0.96875
        8541900000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dOUByTpXfj8=
            n_samples: 810
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dOUByTpXfj8=
            n_samples: 810
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dOUByTpXfj8=
            n_samples: 810
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.03183023872679045
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dOUByTpXfj8=
            n_samples: 810
            precision: 0.016172506738544475
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              dOUByTpXfj8=
            n_samples: 810
            precision: 1.0
            recall: 1.0
        8542310000:
          catboost:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7byLR8DE0D8=
            n_samples: 229
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7byLR8DE0D8=
            n_samples: 229
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7byLR8DE0D8=
            n_samples: 229
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7byLR8DE0D8=
            n_samples: 229
            precision: 1.0
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              7byLR8DE0D8=
            n_samples: 229
            precision: 1.0
            recall: 1.0
        8542390000:
          catboost:
            auc: 1.0
            f1: 0.9920634920634921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm2T8=
            n_samples: 320
            precision: 1.0
            recall: 0.984251968503937
          lightgbm:
            auc: 1.0
            f1: 0.9920634920634921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm2T8=
            n_samples: 320
            precision: 1.0
            recall: 0.984251968503937
          logisticregression:
            auc: 0.9990208477826281
            f1: 0.9921259842519685
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm2T8=
            n_samples: 320
            precision: 0.9921259842519685
            recall: 0.9921259842519685
          randomforest:
            auc: 0.9996328179184857
            f1: 0.9920634920634921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm2T8=
            n_samples: 320
            precision: 1.0
            recall: 0.984251968503937
          xgboost:
            auc: 0.9999999999999999
            f1: 0.9920634920634921
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              ZmZmZmZm2T8=
            n_samples: 320
            precision: 1.0
            recall: 0.984251968503937
        8543200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F6hD8=
            n_samples: 100
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F6hD8=
            n_samples: 100
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F6hD8=
            n_samples: 100
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.06451612903225806
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F6hD8=
            n_samples: 100
            precision: 0.03333333333333333
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              exSuR+F6hD8=
            n_samples: 100
            precision: 1.0
            recall: 1.0
        8543700000:
          catboost:
            auc: 0.9998959958398336
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DKIbNHivhz8=
            n_samples: 1297
            precision: 1.0
            recall: 0.9333333333333333
          lightgbm:
            auc: 0.9998439937597505
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DKIbNHivhz8=
            n_samples: 1297
            precision: 1.0
            recall: 0.9333333333333333
          logisticregression:
            auc: 0.999739989599584
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DKIbNHivhz8=
            n_samples: 1297
            precision: 1.0
            recall: 0.9333333333333333
          randomforest:
            auc: 0.9932397295891835
            f1: 0.06564551422319474
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DKIbNHivhz8=
            n_samples: 1297
            precision: 0.033936651583710405
            recall: 1.0
          xgboost:
            auc: 0.9999479979199168
            f1: 0.9655172413793104
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              DKIbNHivhz8=
            n_samples: 1297
            precision: 1.0
            recall: 0.9333333333333333
        8543900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 190
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 190
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 190
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.058823529411764705
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 190
            precision: 0.030303030303030304
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              sgJhpB0rkD8=
            n_samples: 190
            precision: 1.0
            recall: 1.0
        8544110000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lmVZlmVZtj8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9565217391304348
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lmVZlmVZtj8=
            n_samples: 126
            precision: 0.9166666666666666
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lmVZlmVZtj8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9960474308300395
            f1: 0.3142857142857143
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lmVZlmVZtj8=
            n_samples: 126
            precision: 0.1864406779661017
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              lmVZlmVZtj8=
            n_samples: 126
            precision: 1.0
            recall: 1.0
        8544190000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mxsr9pEmgj8=
            n_samples: 677
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mxsr9pEmgj8=
            n_samples: 677
            precision: 0.8571428571428571
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.9230769230769231
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mxsr9pEmgj8=
            n_samples: 677
            precision: 0.8571428571428571
            recall: 1.0
          randomforest:
            auc: 0.9999999999999999
            f1: 0.15384615384615385
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mxsr9pEmgj8=
            n_samples: 677
            precision: 0.08333333333333333
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              mxsr9pEmgj8=
            n_samples: 677
            precision: 1.0
            recall: 1.0
        8544200000:
          catboost:
            auc: 1.0
            f1: 0.9904761904761905
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U5LNVqWonD8=
            n_samples: 1858
            precision: 0.9811320754716981
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 0.9811320754716981
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U5LNVqWonD8=
            n_samples: 1858
            precision: 0.9629629629629629
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.9719626168224299
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U5LNVqWonD8=
            n_samples: 1858
            precision: 0.9454545454545454
            recall: 1.0
          randomforest:
            auc: 0.9998722208024533
            f1: 0.1897810218978102
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U5LNVqWonD8=
            n_samples: 1858
            precision: 0.10483870967741936
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.9904761904761905
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              U5LNVqWonD8=
            n_samples: 1858
            precision: 0.9811320754716981
            recall: 1.0
        8544300000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LSkHRIZ7hT8=
            n_samples: 286
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LSkHRIZ7hT8=
            n_samples: 286
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 0.6
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LSkHRIZ7hT8=
            n_samples: 286
            precision: 0.42857142857142855
            recall: 1.0
          randomforest:
            auc: 0.9929328621908127
            f1: 0.08571428571428572
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LSkHRIZ7hT8=
            n_samples: 286
            precision: 0.04477611940298507
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              LSkHRIZ7hT8=
            n_samples: 286
            precision: 1.0
            recall: 1.0
        8544420000:
          catboost:
            auc: 1.0
            f1: 0.9787234042553191
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EtC97S9Ckj8=
            n_samples: 1346
            precision: 1.0
            recall: 0.9583333333333334
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EtC97S9Ckj8=
            n_samples: 1346
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 0.9926563287947554
            f1: 0.9361702127659575
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EtC97S9Ckj8=
            n_samples: 1346
            precision: 0.9565217391304348
            recall: 0.9166666666666666
          randomforest:
            auc: 0.9795763993948563
            f1: 0.19913419913419914
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EtC97S9Ckj8=
            n_samples: 1346
            precision: 0.1111111111111111
            recall: 0.9583333333333334
          xgboost:
            auc: 1.0
            f1: 0.9787234042553191
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              EtC97S9Ckj8=
            n_samples: 1346
            precision: 1.0
            recall: 0.9583333333333334
        8544499000:
          catboost:
            auc: 0.9996832620672985
            f1: 0.9891067538126361
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzAbMBswqz8=
            n_samples: 4369
            precision: 1.0
            recall: 0.978448275862069
          lightgbm:
            auc: 0.9999218574179191
            f1: 0.9764453961456103
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzAbMBswqz8=
            n_samples: 4369
            precision: 0.9702127659574468
            recall: 0.9827586206896551
          logisticregression:
            auc: 0.9903509539646421
            f1: 0.9324894514767933
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzAbMBswqz8=
            n_samples: 4369
            precision: 0.9132231404958677
            recall: 0.9525862068965517
          randomforest:
            auc: 0.9797683645486901
            f1: 0.31674842326559216
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzAbMBswqz8=
            n_samples: 4369
            precision: 0.1891213389121339
            recall: 0.9741379310344828
          xgboost:
            auc: 0.9999479049452793
            f1: 0.9891067538126361
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              GzAbMBswqz8=
            n_samples: 4369
            precision: 1.0
            recall: 0.978448275862069
        8544600000:
          catboost:
            auc: 0.9985417125247633
            f1: 0.9783549783549783
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              famg1ZcKyj8=
            n_samples: 580
            precision: 1.0
            recall: 0.9576271186440678
          lightgbm:
            auc: 0.99933964340744
            f1: 0.9785407725321889
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              famg1ZcKyj8=
            n_samples: 580
            precision: 0.991304347826087
            recall: 0.9661016949152542
          logisticregression:
            auc: 0.9986609435761978
            f1: 0.9504132231404959
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              famg1ZcKyj8=
            n_samples: 580
            precision: 0.9274193548387096
            recall: 0.9745762711864406
          randomforest:
            auc: 0.9867470100520948
            f1: 0.5130434782608696
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              famg1ZcKyj8=
            n_samples: 580
            precision: 0.34502923976608185
            recall: 1.0
          xgboost:
            auc: 0.9995781055103089
            f1: 0.9783549783549783
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              famg1ZcKyj8=
            n_samples: 580
            precision: 1.0
            recall: 0.9576271186440678
        8544700000:
          catboost:
            auc: 0.9961818609022557
            f1: 0.9516129032258065
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izPaPWx9uz8=
            n_samples: 596
            precision: 0.9833333333333333
            recall: 0.921875
          lightgbm:
            auc: 0.9981790413533834
            f1: 0.8905109489051095
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izPaPWx9uz8=
            n_samples: 596
            precision: 0.8356164383561644
            recall: 0.953125
          logisticregression:
            auc: 0.9734786184210527
            f1: 0.8636363636363636
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izPaPWx9uz8=
            n_samples: 596
            precision: 0.8382352941176471
            recall: 0.890625
          randomforest:
            auc: 0.9478383458646616
            f1: 0.43902439024390244
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izPaPWx9uz8=
            n_samples: 596
            precision: 0.2967032967032967
            recall: 0.84375
          xgboost:
            auc: 0.9979440789473684
            f1: 0.9606299212598425
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              izPaPWx9uz8=
            n_samples: 596
            precision: 0.9682539682539683
            recall: 0.953125
        8545200000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 0.9649122807017544
            f1: 0.1
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 0.05263157894736842
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              HYGirQbPgT8=
            n_samples: 115
            precision: 1.0
            recall: 1.0
        8546900000:
          catboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IHRU4Iurrz8=
            n_samples: 194
            precision: 1.0
            recall: 1.0
          lightgbm:
            auc: 0.9999999999999999
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IHRU4Iurrz8=
            n_samples: 194
            precision: 1.0
            recall: 1.0
          logisticregression:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IHRU4Iurrz8=
            n_samples: 194
            precision: 1.0
            recall: 1.0
          randomforest:
            auc: 1.0
            f1: 0.24
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IHRU4Iurrz8=
            n_samples: 194
            precision: 0.13636363636363635
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 1.0
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              IHRU4Iurrz8=
            n_samples: 194
            precision: 1.0
            recall: 1.0
        8547900000:
          catboost:
            auc: 0.9854014598540146
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fl/xFV/xlT8=
            n_samples: 140
            precision: 1.0
            recall: 0.3333333333333333
          lightgbm:
            auc: 1.0
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fl/xFV/xlT8=
            n_samples: 140
            precision: 1.0
            recall: 0.3333333333333333
          logisticregression:
            auc: 0.9878345498783455
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fl/xFV/xlT8=
            n_samples: 140
            precision: 1.0
            recall: 0.3333333333333333
          randomforest:
            auc: 0.8394160583941606
            f1: 0.11538461538461539
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fl/xFV/xlT8=
            n_samples: 140
            precision: 0.061224489795918366
            recall: 1.0
          xgboost:
            auc: 1.0
            f1: 0.5
            fraud_rate: !!python/object/apply:numpy._core.multiarray.scalar
            - *id001
            - !!binary |
              Fl/xFV/xlT8=
            n_samples: 140
            precision: 1.0
            recall: 0.3333333333333333
    worst_group_gaps:
      CODE_SH_COMPLET_catboost:
        best_f1: 1.0
        gap: 1.0
        n_groups: 174
        worst_f1: 0.0
      CODE_SH_COMPLET_lightgbm:
        best_f1: 1.0
        gap: 1.0
        n_groups: 174
        worst_f1: 0.0
      CODE_SH_COMPLET_xgboost:
        best_f1: 1.0
        gap: 1.0
        n_groups: 174
        worst_f1: 0.0
  temporal_backtest: null
